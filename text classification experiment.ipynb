{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e74948",
   "metadata": {},
   "source": [
    "text classifiction(\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd6179e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:15:17.605146Z",
     "start_time": "2021-09-07T08:15:09.811113Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data\n",
    "\n",
    "SEED = 2021\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField(dtype=float)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "733ef911",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:16:14.005796Z",
     "start_time": "2021-09-07T08:15:17.617162Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.legacy import datasets\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd787e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:16:14.011046Z",
     "start_time": "2021-09-07T08:16:14.007926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "311973cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:16:14.027406Z",
     "start_time": "2021-09-07T08:16:14.012560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['I', 'like', 'this', 'film', 'a', 'lot', '.', 'It', 'has', 'a', 'wonderful', 'chemistry', 'between', 'the', 'actors', 'and', 'tells', 'a', 'story', 'that', 'is', 'pretty', 'universal', ',', 'the', 'story', 'of', 'the', 'prodigal', 'son', '.', 'The', 'aspect', 'I', 'like', 'the', 'best', 'however', 'was', 'the', 'way', 'that', 'the', 'bath', 'house', 'was', 'more', 'than', 'just', 'a', 'background', 'for', 'the', 'story', '.', 'As', 'the', 'father', 'told', 'the', 'son', 'the', 'story', 'of', 'his', 'wife', \"'s\", 'family', 'in', 'the', 'northern', 'deserts', 'of', 'china', ',', 'the', 'element', 'of', 'water', 'and', 'bathing', 'becomes', 'an', 'almost', 'sacred', 'ritual', '.', 'Water', 'was', 'so', 'scarce', 'that', 'a', 'simple', 'bath', 'had', 'profound', 'depth', 'and', 'meaning.<br', '/><br', '/>Overall', 'the', 'film', 'was', 'very', 'effective', '.', 'There', 'were', 'moments', ',', 'however', ',', 'when', 'it', 'verged', 'on', '\"', 'too', '\"', 'sweet', '...', 'bordering', 'on', 'cloying', 'during', 'the', 'park', 'recital', 'scene', '.', 'But', 'overall', ',', 'I', 'highly', 'recommend', 'this', 'film', '.'], 'label': 'pos'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd4d3bf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:16:14.069047Z",
     "start_time": "2021-09-07T08:16:14.030868Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "train_data, val_data = train_data.split(random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff3e13e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:16:14.082401Z",
     "start_time": "2021-09-07T08:16:14.074808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500\n",
      "7500\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85820991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T04:21:59.386734Z",
     "start_time": "2021-09-07T04:21:58.889517Z"
    }
   },
   "source": [
    "- word2idx & idx2word\n",
    "- idx2one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87694373",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:16:16.395195Z",
     "start_time": "2021-09-07T08:16:14.092076Z"
    }
   },
   "outputs": [],
   "source": [
    "# glove是斯坦福训练的一些高质量词向量\n",
    "# 通常把embedding这层初始化成预训练的一些向量\n",
    "TEXT.build_vocab(train_data, max_size=25000, vectors=\"glove.6B.100d\", unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a300027f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:16:16.443751Z",
     "start_time": "2021-09-07T08:16:16.398110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 203725), (',', 193518), ('.', 166335), ('a', 110155), ('and', 110050), ('of', 101236), ('to', 94443), ('is', 76935), ('in', 61664), ('I', 54376), ('it', 53996), ('that', 49554), ('\"', 44878), (\"'s\", 43639), ('this', 42540), ('-', 37461), ('/><br', 35862), ('was', 35140), ('as', 30678), ('with', 30114)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dd2849d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:16:16.453530Z",
     "start_time": "2021-09-07T08:16:16.446852Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits((train_data, val_data, test_data),\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          device=device,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bcf3ec5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:16:28.439584Z",
     "start_time": "2021-09-07T08:16:16.455218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1303,    2, 1147,  466,    7,  394,   68,    3,  304,    6,    2, 1060,\n",
      "          28,    2,  418, 1216, 3982,    4,  172,  330, 3537,   31,   80,   16,\n",
      "          22,    3,  575, 2256,   10,    2,  100,  517,    4, 4237,    3,    7,\n",
      "         284,    3,   31,  204,   94,  561,   17, 2747,    4], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Perhaps',\n",
       " 'the',\n",
       " 'biggest',\n",
       " 'waste',\n",
       " 'of',\n",
       " 'production',\n",
       " 'time',\n",
       " ',',\n",
       " 'money',\n",
       " 'and',\n",
       " 'the',\n",
       " 'space',\n",
       " 'on',\n",
       " 'the',\n",
       " 'video',\n",
       " 'store',\n",
       " 'shelf',\n",
       " '.',\n",
       " 'If',\n",
       " 'someone',\n",
       " 'suggests',\n",
       " 'you',\n",
       " 'see',\n",
       " 'this',\n",
       " 'movie',\n",
       " ',',\n",
       " 'run',\n",
       " 'screaming',\n",
       " 'in',\n",
       " 'the',\n",
       " 'other',\n",
       " 'direction',\n",
       " '.',\n",
       " 'Unless',\n",
       " ',',\n",
       " 'of',\n",
       " 'course',\n",
       " ',',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'into',\n",
       " 'self',\n",
       " '-',\n",
       " 'abuse',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(val_iter))\n",
    "print(batch.text[:,0])\n",
    "[TEXT.vocab.itos[i] for i in batch.text[:,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600e9786",
   "metadata": {},
   "source": [
    "weight averaging\n",
    "---------\n",
    "做句子里各个词向量的平均，然后做分类\n",
    "- 训练出词向量\n",
    "- 做平均，得到sentence vector\n",
    "- 训练出sentence vector的分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7acb8284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:16:28.448399Z",
     "start_time": "2021-09-07T08:16:28.441264Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WordAVGModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, output_size, pad_idx):\n",
    "        super(WordAVGModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n",
    "        self.linear = nn.Linear(embed_size, output_size)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # text:[seq_length, batch_size]\n",
    "        embedded = self.embed(text) # embedded:[seq_length, batch_size, embed_size]\n",
    "        embedded = embedded.permute(1,0,2)# embedded:[batch_size, seq_length, embed_size]\n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze() # pooled:[bsz, embed_size]\n",
    "        return self.linear(pooled) # ->here we get logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0d727f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:16:28.487286Z",
     "start_time": "2021-09-07T08:16:28.450210Z"
    }
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBED_SIZE = 100\n",
    "OUTPUT_SIZE = 1 # 阈值分类\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model = WordAVGModel(vocab_size=VOCAB_SIZE,\n",
    "                     embed_size=EMBED_SIZE,\n",
    "                     output_size=OUTPUT_SIZE,\n",
    "                     pad_idx=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fb55950",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:16:28.501835Z",
     "start_time": "2021-09-07T08:16:28.497042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordAVGModel(\n",
       "  (embed): Embedding(25002, 100, padding_idx=1)\n",
       "  (linear): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65b7affe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:16:28.511125Z",
     "start_time": "2021-09-07T08:16:28.504602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500301"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cbfe6e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:16:28.526230Z",
     "start_time": "2021-09-07T08:16:28.515906Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "model.embed.weight.data.copy_(pretrained_embedding)\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embed.weight.data[PAD_IDX] = torch.zeros(EMBED_SIZE)\n",
    "model.embed.weight.data[UNK_IDX] = torch.zeros(EMBED_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c426ff77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T04:52:51.812039Z",
     "start_time": "2021-09-07T04:52:51.806935Z"
    }
   },
   "source": [
    "训练模型\n",
    "----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a29cfd99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:16:28.541149Z",
     "start_time": "2021-09-07T08:16:28.527915Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5) # lr decay\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68c76e49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:16:28.546504Z",
     "start_time": "2021-09-07T08:16:28.543022Z"
    }
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded == y).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f1acf72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:16:28.561839Z",
     "start_time": "2021-09-07T08:16:28.548632Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss, epoch_acc, total_len = 0., 0., 0.\n",
    "    model.train()\n",
    "    for it in iterator:\n",
    "        preds = model(it.text).squeeze() # original preds:[bsz, 1]\n",
    "        loss = criterion(preds, it.label)\n",
    "        acc = binary_accuracy(preds, it.label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch.label)\n",
    "        epoch_acc += acc.item() * len(batch.label)\n",
    "        total_len += len(batch.label)\n",
    " \n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb99b75a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:16:28.577572Z",
     "start_time": "2021-09-07T08:16:28.565842Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, optimizer, criterion):\n",
    "    epoch_loss, epoch_acc, total_len = 0., 0., 0.\n",
    "    model.eval()\n",
    "    for it in iterator:\n",
    "        preds = model(it.text).squeeze()\n",
    "        loss = criterion(preds, it.label)\n",
    "        acc = binary_accuracy(preds, it.label)\n",
    "        \n",
    "        epoch_loss += loss.item() * len(batch.label)\n",
    "        epoch_acc += acc.item() * len(batch.label)\n",
    "        total_len += len(batch.label)\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    return epoch_loss / total_len, epoch_acc / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0f28da9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:18:39.150524Z",
     "start_time": "2021-09-07T08:16:28.583255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss 0.6749341328372044 Train Acc 0.6569029121119971\n",
      "Epoch 0 Valid Loss 0.5570298828189971 Valid Acc 0.7408687944107867\n",
      "best model saved to wordavg_model.pth, val_loss= 0.7408687944107867\n",
      "Epoch 1 Train Loss 0.5821926220603701 Train Acc 0.7796585271083895\n",
      "Epoch 1 Valid Loss 0.4138156174300788 Valid Acc 0.813874113559723\n",
      "best model saved to wordavg_model.pth, val_loss= 0.813874113559723\n",
      "Epoch 2 Train Loss 0.46654784509904834 Train Acc 0.8414566466751656\n",
      "Epoch 2 Valid Loss 0.35676279022230456 Valid Acc 0.8549645390916378\n",
      "best model saved to wordavg_model.pth, val_loss= 0.8549645390916378\n",
      "Epoch 3 Train Loss 0.38706389309972644 Train Acc 0.8767547010285763\n",
      "Epoch 3 Valid Loss 0.3557774397370671 Valid Acc 0.8691932624958931\n",
      "best model saved to wordavg_model.pth, val_loss= 0.8691932624958931\n",
      "Epoch 4 Train Loss 0.3324609484885088 Train Acc 0.8932488901741544\n",
      "Epoch 4 Valid Loss 0.36928652644013593 Valid Acc 0.8781028369639782\n",
      "best model saved to wordavg_model.pth, val_loss= 0.8781028369639782\n",
      "Epoch 5 Train Loss 0.292702659514615 Train Acc 0.9066825542850948\n",
      "Epoch 5 Valid Loss 0.38608116316730445 Valid Acc 0.8828900710065314\n",
      "best model saved to wordavg_model.pth, val_loss= 0.8828900710065314\n",
      "Epoch 6 Train Loss 0.26160773153061295 Train Acc 0.9162721991321069\n",
      "Epoch 6 Valid Loss 0.40485492732214357 Valid Acc 0.8876773050490846\n",
      "best model saved to wordavg_model.pth, val_loss= 0.8876773050490846\n",
      "Epoch 7 Train Loss 0.23827411475934465 Train Acc 0.9244091146824782\n",
      "Epoch 7 Valid Loss 0.4216751183638518 Valid Acc 0.8919326241980208\n",
      "best model saved to wordavg_model.pth, val_loss= 0.8919326241980208\n",
      "Epoch 8 Train Loss 0.2164349329137045 Train Acc 0.9308484592211094\n",
      "Epoch 8 Valid Loss 0.44093278423831683 Valid Acc 0.8953900710065315\n",
      "best model saved to wordavg_model.pth, val_loss= 0.8953900710065315\n",
      "Epoch 9 Train Loss 0.19887165456574832 Train Acc 0.9379978454527079\n",
      "Epoch 9 Valid Loss 0.45818206046637533 Valid Acc 0.8971187944107867\n",
      "best model saved to wordavg_model.pth, val_loss= 0.8971187944107867\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "best_valid_acc = 0.\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_iter, optimizer, criterion)\n",
    "    \n",
    "    print(\"Epoch\", epoch, \"Train Loss\", train_loss, \"Train Acc\", train_acc)\n",
    "    print(\"Epoch\", epoch, \"Valid Loss\", val_loss, \"Valid Acc\", val_acc)\n",
    "    \n",
    "    if val_acc > best_valid_acc:\n",
    "        best_valid_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"wordavg_model.pth\")\n",
    "        print(\"best model saved to wordavg_model.pth, val_loss=\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3c192b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:18:39.166369Z",
     "start_time": "2021-09-07T08:18:39.152325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"wordavg_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59d258c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:18:39.944225Z",
     "start_time": "2021-09-07T08:18:39.171047Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def predict_sentiment(sen):\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sen)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1) # ->[seq_length, bsz]\n",
    "    pred = torch.sigmoid(model(tensor))\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9b19269",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:18:39.954878Z",
     "start_time": "2021-09-07T08:18:39.947714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.584218413541176e-12"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"i want to see more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3a69a98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:18:39.966483Z",
     "start_time": "2021-09-07T08:18:39.958009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"the film is terrible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1736164",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T07:47:07.713008Z",
     "start_time": "2021-09-07T07:47:07.696294Z"
    }
   },
   "source": [
    "RNN模型\n",
    "-----\n",
    "用RNN模型encode这个句子，把传到最后的隐状态作为encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7cd75a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:19:51.559925Z",
     "start_time": "2021-09-07T08:19:51.551237Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, output_size, pad_idx, hidden_size, dropout):\n",
    "        super(RNNModel, self).__init__()     \n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=2, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_size*2, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # text:[seq_length, batch_size]\n",
    "        embedded = self.embed(text) # embedded:[seq_length, batch_size, embed_size]\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # hidden:[num_layers*num_directions, batch_size, hidden_size]\n",
    "        # 两层各一个前向一个反向的hidden state，所以取最后两个（第二层的）\n",
    "        hidden = torch.cat([hidden[-1], hidden[-2]], dim=1)\n",
    "        hidden = self.dropout(hidden.squeeze())\n",
    "        return self.linear(hidden) # ->here we get logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de23a277",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:19:52.551648Z",
     "start_time": "2021-09-07T08:19:52.516088Z"
    }
   },
   "outputs": [],
   "source": [
    "model = RNNModel(vocab_size=VOCAB_SIZE,\n",
    "                 embed_size=EMBED_SIZE,\n",
    "                 output_size=OUTPUT_SIZE,\n",
    "                 pad_idx=PAD_IDX,\n",
    "                 hidden_size=100,\n",
    "                 dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37925759",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:19:53.115749Z",
     "start_time": "2021-09-07T08:19:53.105096Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "model.embed.weight.data.copy_(pretrained_embedding)\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embed.weight.data[PAD_IDX] = torch.zeros(EMBED_SIZE)\n",
    "model.embed.weight.data[UNK_IDX] = torch.zeros(EMBED_SIZE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5) # lr decay\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d312c037",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T08:42:18.909441Z",
     "start_time": "2021-09-07T08:19:54.279380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss 0.6664477060409713 Train Acc 0.5916851658707781\n",
      "Epoch 0 Valid Loss 0.6132248257504815 Valid Acc 0.6838209220703612\n",
      "Epoch 1 Train Loss 0.6061158321576084 Train Acc 0.6854759729322611\n",
      "Epoch 1 Valid Loss 0.6833972782938549 Valid Acc 0.5235372340425531\n",
      "Epoch 2 Train Loss 0.4740264137214915 Train Acc 0.7776916297305873\n",
      "Epoch 2 Valid Loss 0.3365465347378528 Valid Acc 0.8560283688788718\n",
      "Epoch 3 Train Loss 0.30914569001010567 Train Acc 0.874461347703742\n",
      "Epoch 3 Valid Loss 0.3086086222090026 Valid Acc 0.8757092199427017\n",
      "Epoch 4 Train Loss 0.2577675924705586 Train Acc 0.899378101394207\n",
      "Epoch 4 Valid Loss 0.2846900516757779 Valid Acc 0.884751773134191\n",
      "Epoch 5 Train Loss 0.2143628503715951 Train Acc 0.9197897623400365\n",
      "Epoch 5 Valid Loss 0.2739277087139929 Valid Acc 0.8968528369639782\n",
      "Epoch 6 Train Loss 0.18410167872320696 Train Acc 0.9315421781766567\n",
      "Epoch 6 Valid Loss 0.28143334175580637 Valid Acc 0.8957890071767441\n",
      "Epoch 7 Train Loss 0.15773891922953404 Train Acc 0.9409604336269595\n",
      "Epoch 7 Valid Loss 0.3266886692169419 Valid Acc 0.884751773134191\n",
      "Epoch 8 Train Loss 0.1362660916159574 Train Acc 0.9516681901279708\n",
      "Epoch 8 Valid Loss 0.41865711130104494 Valid Acc 0.8769060284533399\n",
      "Epoch 9 Train Loss 0.11870894899433293 Train Acc 0.9576994647055701\n",
      "Epoch 9 Valid Loss 0.3210293109150538 Valid Acc 0.9025709220703612\n",
      "best model saved to LSTM_model.pth, val_loss= 0.9025709220703612\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_iter, optimizer, criterion)\n",
    "    \n",
    "    print(\"Epoch\", epoch, \"Train Loss\", train_loss, \"Train Acc\", train_acc)\n",
    "    print(\"Epoch\", epoch, \"Valid Loss\", val_loss, \"Valid Acc\", val_acc)\n",
    "    \n",
    "    if val_acc > best_valid_acc:\n",
    "        best_valid_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"LSTM_model.pth\")\n",
    "        print(\"best model saved to LSTM_model.pth, val_loss=\", val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce9143",
   "metadata": {},
   "source": [
    "CNN模型\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06096d8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T09:35:36.043167Z",
     "start_time": "2021-09-07T09:35:36.031641Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, output_size, pad_idx, num_filters, filter_sizes, dropout):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1, out_channels=num_filters,\n",
    "                      kernel_size=(fsz, embed_size))\n",
    "            for fsz in filter_sizes\n",
    "        ])\n",
    "#         self.conv = nn.Conv2d(in_channels=1, out_channels=num_filters, kernel_size=(filter_size, embed_size))\n",
    "        self.linear = nn.Linear(num_filters*len(filter_sizes), output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        text = text.permute(1, 0) # text:[batch_size, seq_length]\n",
    "        embedded = self.embed(text) # embedded:[batch_size, seq_length, embed_size]\n",
    "        \n",
    "        # [N, C, H, W]\n",
    "        embedded = embedded.unsqueeze(1) # embedded:[batch_size, 1, seq_length, embed_size]\n",
    "#         conved = F.relu(self.conv(embedded)) # conved:[batch_size, num_filters, seq_length-filter_size+1, 1]\n",
    "#         conved = conved.squeeze(3)\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "    \n",
    "        # maxpooling\n",
    "#         pooled = F.max_pool1d(conved, conved.shape[2]) # pooled:[batch_size, num_filters, 1]\n",
    "#         pooled = pooled.squeeze(2)\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        pooled = torch.cat(pooled, dim=1) # pooled:[batch_size, 3*num_filters]\n",
    "        pooled = self.dropout(pooled)\n",
    "        \n",
    "        return self.linear(pooled) # ->here we get logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9428914",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T09:35:36.909039Z",
     "start_time": "2021-09-07T09:35:36.878162Z"
    }
   },
   "outputs": [],
   "source": [
    "model = CNNModel(vocab_size=VOCAB_SIZE,\n",
    "                 embed_size=EMBED_SIZE,\n",
    "                 output_size=OUTPUT_SIZE,\n",
    "                 pad_idx=PAD_IDX,\n",
    "                 num_filters=100,\n",
    "                 filter_sizes=[3, 4, 5],\n",
    "                 dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73508f9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T09:35:37.663464Z",
     "start_time": "2021-09-07T09:35:37.631319Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_embedding = TEXT.vocab.vectors\n",
    "model.embed.weight.data.copy_(pretrained_embedding)\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embed.weight.data[PAD_IDX] = torch.zeros(EMBED_SIZE)\n",
    "model.embed.weight.data[UNK_IDX] = torch.zeros(EMBED_SIZE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5) # lr decay\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d6d5390",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-07T09:41:31.506999Z",
     "start_time": "2021-09-07T09:35:38.832610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train Loss 0.607937832928573 Train Acc 0.6585351920650908\n",
      "Epoch 0 Valid Loss 0.41745231992751686 Valid Acc 0.8156028369639782\n",
      "Epoch 1 Train Loss 0.3730899749583265 Train Acc 0.8323893314959581\n",
      "Epoch 1 Valid Loss 0.3314560703259949 Valid Acc 0.8618351063829788\n",
      "Epoch 2 Train Loss 0.2560321971560451 Train Acc 0.8958442152309244\n",
      "Epoch 2 Valid Loss 0.31487216588940503 Valid Acc 0.8733156029214251\n",
      "Epoch 3 Train Loss 0.17023584130432842 Train Acc 0.9365777618488402\n",
      "Epoch 3 Valid Loss 0.3302223778614733 Valid Acc 0.876374113559723\n",
      "Epoch 4 Train Loss 0.11057369865897392 Train Acc 0.9607518281535649\n",
      "Epoch 4 Valid Loss 0.35572421963634865 Valid Acc 0.8731826241980207\n",
      "Epoch 5 Train Loss 0.07154549534732031 Train Acc 0.9767400104777251\n",
      "Epoch 5 Valid Loss 0.4056132808631031 Valid Acc 0.8713209220703612\n",
      "Epoch 6 Train Loss 0.0500120494589295 Train Acc 0.9833099373515825\n",
      "Epoch 6 Valid Loss 0.45166121195074616 Valid Acc 0.8685283688788719\n",
      "Epoch 7 Train Loss 0.04202591477006022 Train Acc 0.9866805954868658\n",
      "Epoch 7 Valid Loss 0.5001840966917028 Valid Acc 0.8683953901554676\n",
      "Epoch 8 Train Loss 0.027446352567428726 Train Acc 0.9914876599634369\n",
      "Epoch 8 Valid Loss 0.5736985360168128 Valid Acc 0.8674645390916378\n",
      "Epoch 9 Train Loss 0.021363637564982395 Train Acc 0.9929730347349177\n",
      "Epoch 9 Valid Loss 0.5911899283929565 Valid Acc 0.8691932624958931\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 10\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_iter, optimizer, criterion)\n",
    "    \n",
    "    print(\"Epoch\", epoch, \"Train Loss\", train_loss, \"Train Acc\", train_acc)\n",
    "    print(\"Epoch\", epoch, \"Valid Loss\", val_loss, \"Valid Acc\", val_acc)\n",
    "    \n",
    "    if val_acc > best_valid_acc:\n",
    "        best_valid_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"CNN_model.pth\")\n",
    "        print(\"best model saved to CNN_model.pth, val_loss=\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79ab79c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
