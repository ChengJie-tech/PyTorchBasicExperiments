{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e44f07",
   "metadata": {},
   "source": [
    "语言模型小实验（\n",
    "------\n",
    "- 使用torchtext初体验\n",
    "- nn.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c27be4d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T06:33:53.768432Z",
     "start_time": "2021-09-06T06:33:52.678930Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "random.seed(2021)\n",
    "np.random.seed(2021)\n",
    "torch.manual_seed(2021)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(2021)\n",
    "    \n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 100\n",
    "HIDDEN_SIZE = 100\n",
    "MAX_VOCAB_SIZE = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea57760f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T06:34:38.447272Z",
     "start_time": "2021-09-06T06:33:53.771342Z"
    }
   },
   "outputs": [],
   "source": [
    "TEXT = torchtext.legacy.data.Field(lower=True) # Field相当于一个预处理工具\n",
    "train, val, test = torchtext.legacy.datasets.LanguageModelingDataset.splits(path=\".\",\n",
    "                                                 train=\"text8.train.txt\",\n",
    "                                                 validation=\"text8.train.txt\",\n",
    "                                                 test=\"text8.train.txt\",\n",
    "                                                 text_field=TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17a73bcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T06:34:44.504444Z",
     "start_time": "2021-09-06T06:34:38.449085Z"
    }
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e89daea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T06:34:44.513355Z",
     "start_time": "2021-09-06T06:34:44.507247Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c69ca7f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T06:34:44.523166Z",
     "start_time": "2021-09-06T06:34:44.516385Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = torchtext.legacy.data.BPTTIterator.splits((train, val, test),\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  device=device,\n",
    "                                  bptt_len=50,\n",
    "                                  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83aa8fc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T06:34:55.869625Z",
     "start_time": "2021-09-06T06:34:44.525133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.legacy.data.batch.Batch of size 32]\n",
       "\t[.text]:[torch.cuda.LongTensor of size 50x32 (GPU 0)]\n",
       "\t[.target]:[torch.cuda.LongTensor of size 50x32 (GPU 0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(train_iter)\n",
    "batch = next(it)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9073cd46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T06:35:11.083330Z",
     "start_time": "2021-09-06T06:35:11.064397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lycaon in greek mythology was a son of priam and <unk> during the trojan war lycaon was captured by achilles while cutting branches in priam s orchard achilles sold him as a slave to <unk> of lemnos but <unk> of <unk> bought him and took him back to troy only\n",
      "\n",
      "in greek mythology was a son of priam and <unk> during the trojan war lycaon was captured by achilles while cutting branches in priam s orchard achilles sold him as a slave to <unk> of lemnos but <unk> of <unk> bought him and took him back to troy only twelve\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(TEXT.vocab.itos[i] for i in batch.text[:,31].data.cpu()))\n",
    "print()\n",
    "print(\" \".join(TEXT.vocab.itos[i] for i in batch.target[:,31].data.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c764b71f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T06:35:12.821614Z",
     "start_time": "2021-09-06T06:35:12.811589Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size)\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, text, hidden):\n",
    "        # text:[seq_length, batch_size]\n",
    "        emb = self.embed(text)\n",
    "        # emb:[seq_length, batch_size, embed_size]\n",
    "        output, hidden = self.lstm(emb, hidden)\n",
    "        # output:[seq_length, batch_size, hidden_size]\n",
    "        # hidden:[[1, batch_size, hidden_size], [1, batch_size, hidden_size]]\n",
    "        decoded = self.decoder(output.view(-1, output.shape[2]))\n",
    "        # decoded:[(seq_length*batch_size), vocab_size]\n",
    "        decoded = decoded.view(output.size(0), output.size(1), decoded.size(-1))\n",
    "        # decoded:[seq_length, batch_size, vocab_size]\n",
    "        return decoded, hidden\n",
    "    \n",
    "    def init_hidden(self, bsz, requires_grad=True):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros((1, bsz, self.hidden_size), requires_grad=True),\n",
    "               weight.new_zeros((1, bsz, self.hidden_size), requires_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aacac642",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T06:35:14.041617Z",
     "start_time": "2021-09-06T06:35:13.896454Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LSTMModel(vocab_size=len(TEXT.vocab), embed_size=EMBEDDING_SIZE, hidden_size=HIDDEN_SIZE)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaf9f1cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T06:35:14.593098Z",
     "start_time": "2021-09-06T06:35:14.588216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (embed): Embedding(50002, 100)\n",
       "  (lstm): LSTM(100, 100)\n",
       "  (decoder): Linear(in_features=100, out_features=50002, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c56a46",
   "metadata": {},
   "source": [
    "训练模型\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29047be1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T06:35:17.236085Z",
     "start_time": "2021-09-06T06:35:17.228402Z"
    }
   },
   "outputs": [],
   "source": [
    "# 重新打包，把值复制过来，但不要计算图的历史，相当于bp有了新起点\n",
    "def repackage_hidden(h):\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(t) for t in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3eb8d6a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T06:38:37.993649Z",
     "start_time": "2021-09-06T06:38:37.987961Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, val_iter):\n",
    "    model.eval() # model.eval()不启用BN和Dropout\n",
    "    total_loss = 0.\n",
    "    total_count = 0.\n",
    "    \n",
    "    it = iter(val_iter)\n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(BATCH_SIZE, requires_grad=False)\n",
    "        for i, batch in enumerate(it):\n",
    "            data, target = batch.text, batch.target\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output, hidden = model(data, hidden) # bp all the times, use detach to cut off bp in some branches\n",
    "\n",
    "            loss = loss_fn(output.view(-1, len(TEXT.vocab)), target.view(-1))\n",
    "            total_loss += loss.item() * np.multiply(*data.size())\n",
    "            total_count += np.multiply(*data.size())\n",
    "    \n",
    "    loss = total_loss / total_count\n",
    "    model.train()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfc9933a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T06:35:36.676242Z",
     "start_time": "2021-09-06T06:35:36.669557Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5) # lr decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e840c0aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T07:55:00.073830Z",
     "start_time": "2021-09-06T06:38:40.343063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 iteration 0 loss 10.794493675231934\n",
      "best model saved to lm.pth, val_loss= 10.811535831403834\n",
      "epoch 0 iteration 100 loss 7.261686325073242\n",
      "epoch 0 iteration 200 loss 7.008199691772461\n",
      "epoch 0 iteration 300 loss 7.037992000579834\n",
      "epoch 0 iteration 400 loss 7.0093536376953125\n",
      "epoch 0 iteration 500 loss 6.877022743225098\n",
      "epoch 0 iteration 600 loss 6.638118743896484\n",
      "epoch 0 iteration 700 loss 6.646942615509033\n",
      "epoch 0 iteration 800 loss 6.741439342498779\n",
      "epoch 0 iteration 900 loss 6.606591701507568\n",
      "epoch 0 iteration 1000 loss 6.38194465637207\n",
      "best model saved to lm.pth, val_loss= 6.678328329912397\n",
      "epoch 0 iteration 1100 loss 6.816585063934326\n",
      "epoch 0 iteration 1200 loss 6.3791184425354\n",
      "epoch 0 iteration 1300 loss 6.575960159301758\n",
      "epoch 0 iteration 1400 loss 6.609281539916992\n",
      "epoch 0 iteration 1500 loss 6.168779373168945\n",
      "epoch 0 iteration 1600 loss 6.3976898193359375\n",
      "epoch 0 iteration 1700 loss 6.310326099395752\n",
      "epoch 0 iteration 1800 loss 6.509497165679932\n",
      "epoch 0 iteration 1900 loss 6.239916801452637\n",
      "epoch 0 iteration 2000 loss 6.408156871795654\n",
      "best model saved to lm.pth, val_loss= 6.41645098504701\n",
      "epoch 0 iteration 2100 loss 6.388273239135742\n",
      "epoch 0 iteration 2200 loss 6.160463809967041\n",
      "epoch 0 iteration 2300 loss 5.959885120391846\n",
      "epoch 0 iteration 2400 loss 6.261485576629639\n",
      "epoch 0 iteration 2500 loss 6.135818004608154\n",
      "epoch 0 iteration 2600 loss 6.105490684509277\n",
      "epoch 0 iteration 2700 loss 6.484473705291748\n",
      "epoch 0 iteration 2800 loss 6.267394542694092\n",
      "epoch 0 iteration 2900 loss 6.216353893280029\n",
      "epoch 0 iteration 3000 loss 6.230503559112549\n",
      "best model saved to lm.pth, val_loss= 6.250497588092826\n",
      "epoch 0 iteration 3100 loss 6.369373321533203\n",
      "epoch 0 iteration 3200 loss 6.166588306427002\n",
      "epoch 0 iteration 3300 loss 6.191683769226074\n",
      "epoch 0 iteration 3400 loss 5.977115631103516\n",
      "epoch 0 iteration 3500 loss 6.003986835479736\n",
      "epoch 0 iteration 3600 loss 6.13681173324585\n",
      "epoch 0 iteration 3700 loss 6.4309844970703125\n",
      "epoch 0 iteration 3800 loss 6.143067836761475\n",
      "epoch 0 iteration 3900 loss 6.0817484855651855\n",
      "epoch 0 iteration 4000 loss 5.962674617767334\n",
      "best model saved to lm.pth, val_loss= 6.133100359461039\n",
      "epoch 0 iteration 4100 loss 6.082437515258789\n",
      "epoch 0 iteration 4200 loss 5.767376899719238\n",
      "epoch 0 iteration 4300 loss 6.296930313110352\n",
      "epoch 0 iteration 4400 loss 5.879276275634766\n",
      "epoch 0 iteration 4500 loss 5.8183746337890625\n",
      "epoch 0 iteration 4600 loss 5.938350677490234\n",
      "epoch 0 iteration 4700 loss 6.303303241729736\n",
      "epoch 0 iteration 4800 loss 6.2211127281188965\n",
      "epoch 0 iteration 4900 loss 5.920421123504639\n",
      "epoch 0 iteration 5000 loss 6.303144454956055\n",
      "best model saved to lm.pth, val_loss= 6.037745107268048\n",
      "epoch 0 iteration 5100 loss 5.862302303314209\n",
      "epoch 0 iteration 5200 loss 6.104127407073975\n",
      "epoch 0 iteration 5300 loss 5.638767719268799\n",
      "epoch 0 iteration 5400 loss 6.098119735717773\n",
      "epoch 0 iteration 5500 loss 5.849735260009766\n",
      "epoch 0 iteration 5600 loss 5.661230087280273\n",
      "epoch 0 iteration 5700 loss 5.728836536407471\n",
      "epoch 0 iteration 5800 loss 6.104120254516602\n",
      "epoch 0 iteration 5900 loss 6.048471450805664\n",
      "epoch 0 iteration 6000 loss 5.985698699951172\n",
      "best model saved to lm.pth, val_loss= 5.953300480447683\n",
      "epoch 0 iteration 6100 loss 6.048547744750977\n",
      "epoch 0 iteration 6200 loss 6.090269088745117\n",
      "epoch 0 iteration 6300 loss 6.172612190246582\n",
      "epoch 0 iteration 6400 loss 6.208461284637451\n",
      "epoch 0 iteration 6500 loss 5.700615406036377\n",
      "epoch 0 iteration 6600 loss 6.080938339233398\n",
      "epoch 0 iteration 6700 loss 6.073525905609131\n",
      "epoch 0 iteration 6800 loss 5.716063976287842\n",
      "epoch 0 iteration 6900 loss 5.5893049240112305\n",
      "epoch 0 iteration 7000 loss 5.890820503234863\n",
      "best model saved to lm.pth, val_loss= 5.883808501337512\n",
      "epoch 0 iteration 7100 loss 5.851517200469971\n",
      "epoch 0 iteration 7200 loss 5.646295547485352\n",
      "epoch 0 iteration 7300 loss 5.948278903961182\n",
      "epoch 0 iteration 7400 loss 5.871066093444824\n",
      "epoch 0 iteration 7500 loss 5.639268398284912\n",
      "epoch 0 iteration 7600 loss 6.049851894378662\n",
      "epoch 0 iteration 7700 loss 5.842596054077148\n",
      "epoch 0 iteration 7800 loss 5.563564300537109\n",
      "epoch 0 iteration 7900 loss 5.685822010040283\n",
      "epoch 0 iteration 8000 loss 5.721810817718506\n",
      "best model saved to lm.pth, val_loss= 5.819958508158294\n",
      "epoch 0 iteration 8100 loss 5.517093658447266\n",
      "epoch 0 iteration 8200 loss 5.919100284576416\n",
      "epoch 0 iteration 8300 loss 5.641943454742432\n",
      "epoch 0 iteration 8400 loss 5.80085563659668\n",
      "epoch 0 iteration 8500 loss 5.655763149261475\n",
      "epoch 0 iteration 8600 loss 5.741490364074707\n",
      "epoch 0 iteration 8700 loss 5.713468074798584\n",
      "epoch 0 iteration 8800 loss 5.956325531005859\n",
      "epoch 0 iteration 8900 loss 5.95256233215332\n",
      "epoch 0 iteration 9000 loss 5.780999660491943\n",
      "best model saved to lm.pth, val_loss= 5.764166344840618\n",
      "epoch 0 iteration 9100 loss 5.784491539001465\n",
      "epoch 0 iteration 9200 loss 5.627105712890625\n",
      "epoch 0 iteration 9300 loss 5.787027359008789\n",
      "epoch 0 iteration 9400 loss 5.872506141662598\n",
      "epoch 0 iteration 9500 loss 5.572938442230225\n",
      "epoch 1 iteration 0 loss 5.88167667388916\n",
      "best model saved to lm.pth, val_loss= 5.737828884460643\n",
      "epoch 1 iteration 100 loss 5.6559672355651855\n",
      "epoch 1 iteration 200 loss 5.701973915100098\n",
      "epoch 1 iteration 300 loss 5.879283428192139\n",
      "epoch 1 iteration 400 loss 5.83284854888916\n",
      "epoch 1 iteration 500 loss 5.778934478759766\n",
      "epoch 1 iteration 600 loss 5.579894542694092\n",
      "epoch 1 iteration 700 loss 5.692360877990723\n",
      "epoch 1 iteration 800 loss 5.8658318519592285\n",
      "epoch 1 iteration 900 loss 5.598101615905762\n",
      "epoch 1 iteration 1000 loss 5.543018817901611\n",
      "best model saved to lm.pth, val_loss= 5.690958769518726\n",
      "epoch 1 iteration 1100 loss 5.8516740798950195\n",
      "epoch 1 iteration 1200 loss 5.514081954956055\n",
      "epoch 1 iteration 1300 loss 5.787611484527588\n",
      "epoch 1 iteration 1400 loss 5.765444278717041\n",
      "epoch 1 iteration 1500 loss 5.393517017364502\n",
      "epoch 1 iteration 1600 loss 5.68283748626709\n",
      "epoch 1 iteration 1700 loss 5.576241970062256\n",
      "epoch 1 iteration 1800 loss 5.676533222198486\n",
      "epoch 1 iteration 1900 loss 5.578851222991943\n",
      "epoch 1 iteration 2000 loss 5.709968090057373\n",
      "best model saved to lm.pth, val_loss= 5.654113041459538\n",
      "epoch 1 iteration 2100 loss 5.677448272705078\n",
      "epoch 1 iteration 2200 loss 5.482357025146484\n",
      "epoch 1 iteration 2300 loss 5.365728855133057\n",
      "epoch 1 iteration 2400 loss 5.658700466156006\n",
      "epoch 1 iteration 2500 loss 5.48770809173584\n",
      "epoch 1 iteration 2600 loss 5.478297710418701\n",
      "epoch 1 iteration 2700 loss 5.837648391723633\n",
      "epoch 1 iteration 2800 loss 5.664853096008301\n",
      "epoch 1 iteration 2900 loss 5.667675018310547\n",
      "epoch 1 iteration 3000 loss 5.6289191246032715\n",
      "best model saved to lm.pth, val_loss= 5.621586841122162\n",
      "epoch 1 iteration 3100 loss 5.811220645904541\n",
      "epoch 1 iteration 3200 loss 5.703246593475342\n",
      "epoch 1 iteration 3300 loss 5.5288472175598145\n",
      "epoch 1 iteration 3400 loss 5.373923301696777\n",
      "epoch 1 iteration 3500 loss 5.442685127258301\n",
      "epoch 1 iteration 3600 loss 5.6065850257873535\n",
      "epoch 1 iteration 3700 loss 5.931942939758301\n",
      "epoch 1 iteration 3800 loss 5.651822566986084\n",
      "epoch 1 iteration 3900 loss 5.566519260406494\n",
      "epoch 1 iteration 4000 loss 5.430786609649658\n",
      "best model saved to lm.pth, val_loss= 5.591556827281996\n",
      "epoch 1 iteration 4100 loss 5.632429122924805\n",
      "epoch 1 iteration 4200 loss 5.246423244476318\n",
      "epoch 1 iteration 4300 loss 5.810176849365234\n",
      "epoch 1 iteration 4400 loss 5.3535332679748535\n",
      "epoch 1 iteration 4500 loss 5.314251899719238\n",
      "epoch 1 iteration 4600 loss 5.455934524536133\n",
      "epoch 1 iteration 4700 loss 5.838937282562256\n",
      "epoch 1 iteration 4800 loss 5.7397871017456055\n",
      "epoch 1 iteration 4900 loss 5.483304500579834\n",
      "epoch 1 iteration 5000 loss 5.857275009155273\n",
      "best model saved to lm.pth, val_loss= 5.556464413258053\n",
      "epoch 1 iteration 5100 loss 5.432718276977539\n",
      "epoch 1 iteration 5200 loss 5.606180191040039\n",
      "epoch 1 iteration 5300 loss 5.185425281524658\n",
      "epoch 1 iteration 5400 loss 5.602173328399658\n",
      "epoch 1 iteration 5500 loss 5.438795566558838\n",
      "epoch 1 iteration 5600 loss 5.2428879737854\n",
      "epoch 1 iteration 5700 loss 5.3666510581970215\n",
      "epoch 1 iteration 5800 loss 5.659064769744873\n",
      "epoch 1 iteration 5900 loss 5.661217212677002\n",
      "epoch 1 iteration 6000 loss 5.582244873046875\n",
      "best model saved to lm.pth, val_loss= 5.529444135308495\n",
      "epoch 1 iteration 6100 loss 5.653253555297852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 iteration 6200 loss 5.68695068359375\n",
      "epoch 1 iteration 6300 loss 5.757584095001221\n",
      "epoch 1 iteration 6400 loss 5.853007793426514\n",
      "epoch 1 iteration 6500 loss 5.362195014953613\n",
      "epoch 1 iteration 6600 loss 5.741400718688965\n",
      "epoch 1 iteration 6700 loss 5.702823638916016\n",
      "epoch 1 iteration 6800 loss 5.3430328369140625\n",
      "epoch 1 iteration 6900 loss 5.226779937744141\n",
      "epoch 1 iteration 7000 loss 5.514254093170166\n",
      "best model saved to lm.pth, val_loss= 5.5073272079715565\n",
      "epoch 1 iteration 7100 loss 5.506911754608154\n",
      "epoch 1 iteration 7200 loss 5.317791938781738\n",
      "epoch 1 iteration 7300 loss 5.584412097930908\n",
      "epoch 1 iteration 7400 loss 5.51431941986084\n",
      "epoch 1 iteration 7500 loss 5.299510478973389\n",
      "epoch 1 iteration 7600 loss 5.710257530212402\n",
      "epoch 1 iteration 7700 loss 5.506960868835449\n",
      "epoch 1 iteration 7800 loss 5.25291633605957\n",
      "epoch 1 iteration 7900 loss 5.331318378448486\n",
      "epoch 1 iteration 8000 loss 5.390519618988037\n",
      "best model saved to lm.pth, val_loss= 5.479782944949678\n",
      "epoch 1 iteration 8100 loss 5.187412738800049\n",
      "epoch 1 iteration 8200 loss 5.613238334655762\n",
      "epoch 1 iteration 8300 loss 5.335916042327881\n",
      "epoch 1 iteration 8400 loss 5.453365325927734\n",
      "epoch 1 iteration 8500 loss 5.32712459564209\n",
      "epoch 1 iteration 8600 loss 5.4479289054870605\n",
      "epoch 1 iteration 8700 loss 5.408664703369141\n",
      "epoch 1 iteration 8800 loss 5.64047384262085\n",
      "epoch 1 iteration 8900 loss 5.643982410430908\n",
      "epoch 1 iteration 9000 loss 5.4858317375183105\n",
      "best model saved to lm.pth, val_loss= 5.458449539824888\n",
      "epoch 1 iteration 9100 loss 5.519406318664551\n",
      "epoch 1 iteration 9200 loss 5.288561820983887\n",
      "epoch 1 iteration 9300 loss 5.4196391105651855\n",
      "epoch 1 iteration 9400 loss 5.533731460571289\n",
      "epoch 1 iteration 9500 loss 5.2603440284729\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 2\n",
    "GRAD_CLIP = 5.0\n",
    "\n",
    "val_losses = []\n",
    "\n",
    "for epochs in range(NUM_EPOCHS):\n",
    "    model.train() # model.train()能够启用BN和Dropout\n",
    "    it = iter(train_iter)\n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "    for i, batch in enumerate(it):\n",
    "        data, target = batch.text, batch.target\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(data, hidden) # bp all the times, use detach to cut off bp in some branches\n",
    "        \n",
    "        loss = loss_fn(output.view(-1, len(TEXT.vocab)), target.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP) # 梯度裁切\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(\"epoch\", epochs, \"iteration\", i, \"loss\", loss.item())\n",
    "            \n",
    "        if i % 1000 == 0:\n",
    "            val_loss = evaluate(model, val_iter)\n",
    "            if len(val_losses) == 0 or val_loss < min(val_losses):\n",
    "                torch.save(model.state_dict(), \"lm.pth\")\n",
    "                print(\"best model saved to lm.pth, val_loss=\", val_loss)\n",
    "            else:\n",
    "                # lr decay\n",
    "                scheduler.step()\n",
    "            val_losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39d0921e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T09:53:17.115793Z",
     "start_time": "2021-09-06T09:50:29.305500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.44903101332044\n"
     ]
    }
   ],
   "source": [
    "val_loss = evaluate(model, val_iter)\n",
    "print(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46b636f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T09:55:40.983055Z",
     "start_time": "2021-09-06T09:55:40.848080Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"lm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4168e59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T09:55:46.339143Z",
     "start_time": "2021-09-06T09:55:46.209221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = LSTMModel(vocab_size=len(TEXT.vocab), embed_size=EMBEDDING_SIZE, hidden_size=HIDDEN_SIZE)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.to(device)\n",
    "best_model.load_state_dict(torch.load(\"lm.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a277e92",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-06T06:33:03.189Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eef9d6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-09-06T06:33:03.190Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
