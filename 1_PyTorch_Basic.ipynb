{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d080b3f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:31:39.551505Z",
     "start_time": "2021-09-04T04:31:38.675274Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a2a0da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:31:48.701998Z",
     "start_time": "2021-09-04T04:31:48.680534Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.6181e-01, 4.5884e-01, 4.3463e-01],\n",
       "        [7.3483e-01, 5.9233e-01, 7.2242e-01],\n",
       "        [9.0504e-04, 2.2171e-01, 1.9488e-01],\n",
       "        [6.5667e-01, 7.4339e-01, 3.4155e-01],\n",
       "        [9.2792e-01, 6.2905e-01, 1.0593e-01]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "683119c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:34:27.479995Z",
     "start_time": "2021-09-04T04:34:27.474730Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(5,3,dtype=torch.long)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aed93528",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:35:38.821280Z",
     "start_time": "2021-09-04T04:35:38.816890Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(5,3).long()\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfcbaa16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:36:05.454086Z",
     "start_time": "2021-09-04T04:36:05.449057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000, 3.0000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e715fcb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:37:12.463467Z",
     "start_time": "2021-09-04T04:37:12.451880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.new_ones(5,3,dtype=torch.double)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f8a8f05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:37:45.451272Z",
     "start_time": "2021-09-04T04:37:45.445030Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5849,  0.2790, -1.1126],\n",
       "        [ 0.4434,  0.9265,  0.9847],\n",
       "        [ 0.8129,  0.6956, -0.5485],\n",
       "        [ 0.9066, -1.8292,  0.6099],\n",
       "        [-0.2164,  0.6203, -0.8075]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9be3ddd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:38:13.458754Z",
     "start_time": "2021-09-04T04:38:13.454389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3f35892",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:38:18.146704Z",
     "start_time": "2021-09-04T04:38:18.135535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e566b5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:38:41.098906Z",
     "start_time": "2021-09-04T04:38:41.092033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1246, 0.2005, 0.4508],\n",
       "         [0.1894, 0.5073, 0.8071],\n",
       "         [0.3903, 0.8789, 0.4488],\n",
       "         [0.3021, 0.9763, 0.3671],\n",
       "         [0.0458, 0.3474, 0.2602]]),\n",
       " tensor([[-0.5849,  0.2790, -1.1126],\n",
       "         [ 0.4434,  0.9265,  0.9847],\n",
       "         [ 0.8129,  0.6956, -0.5485],\n",
       "         [ 0.9066, -1.8292,  0.6099],\n",
       "         [-0.2164,  0.6203, -0.8075]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "y, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bbad751",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:38:51.048325Z",
     "start_time": "2021-09-04T04:38:51.043155Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4603,  0.4796, -0.6617],\n",
       "        [ 0.6328,  1.4338,  1.7918],\n",
       "        [ 1.2031,  1.5744, -0.0997],\n",
       "        [ 1.2087, -0.8529,  0.9770],\n",
       "        [-0.1705,  0.9678, -0.5473]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18a43c85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:39:52.012322Z",
     "start_time": "2021-09-04T04:39:52.006887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4603,  0.4796, -0.6617],\n",
       "        [ 0.6328,  1.4338,  1.7918],\n",
       "        [ 1.2031,  1.5744, -0.0997],\n",
       "        [ 1.2087, -0.8529,  0.9770],\n",
       "        [-0.1705,  0.9678, -0.5473]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.empty(5,3)\n",
    "result = x + y\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a4f2384",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:40:05.827808Z",
     "start_time": "2021-09-04T04:40:05.822949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4603,  0.4796, -0.6617],\n",
       "        [ 0.6328,  1.4338,  1.7918],\n",
       "        [ 1.2031,  1.5744, -0.0997],\n",
       "        [ 1.2087, -0.8529,  0.9770],\n",
       "        [-0.1705,  0.9678, -0.5473]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4976e366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:41:38.501856Z",
     "start_time": "2021-09-04T04:41:38.497458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9265,  0.9847],\n",
       "        [ 0.6956, -0.5485],\n",
       "        [-1.8292,  0.6099],\n",
       "        [ 0.6203, -0.8075]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1cbe2209",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:43:41.624048Z",
     "start_time": "2021-09-04T04:43:41.612733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1006, -0.2917, -0.1670,  0.5997,  0.8305, -0.1894, -0.2472,  0.4292,\n",
       "          0.5193, -0.0438, -0.3375, -0.1098, -0.2430, -0.0552,  0.0582, -1.5964]),\n",
       " tensor([[-0.1006, -0.2917, -0.1670,  0.5997,  0.8305, -0.1894, -0.2472,  0.4292,\n",
       "           0.5193, -0.0438, -0.3375, -0.1098, -0.2430, -0.0552,  0.0582, -1.5964]]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4,4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1,16)\n",
    "y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e11151f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:44:16.871048Z",
     "start_time": "2021-09-04T04:44:16.866780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2875])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "121d87f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:44:20.712198Z",
     "start_time": "2021-09-04T04:44:20.690615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__array_wrap__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__complex__',\n",
       " '__contains__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__float__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__idiv__',\n",
       " '__ifloordiv__',\n",
       " '__ilshift__',\n",
       " '__imod__',\n",
       " '__imul__',\n",
       " '__index__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__irshift__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__long__',\n",
       " '__lshift__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pos__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rfloordiv__',\n",
       " '__rmul__',\n",
       " '__rpow__',\n",
       " '__rshift__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__torch_function__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_backward_hooks',\n",
       " '_base',\n",
       " '_cdata',\n",
       " '_coalesced_',\n",
       " '_dimI',\n",
       " '_dimV',\n",
       " '_grad',\n",
       " '_grad_fn',\n",
       " '_indices',\n",
       " '_is_view',\n",
       " '_make_subclass',\n",
       " '_nnz',\n",
       " '_reduce_ex_internal',\n",
       " '_to_sparse_csr',\n",
       " '_update_names',\n",
       " '_values',\n",
       " '_version',\n",
       " 'abs',\n",
       " 'abs_',\n",
       " 'absolute',\n",
       " 'absolute_',\n",
       " 'acos',\n",
       " 'acos_',\n",
       " 'acosh',\n",
       " 'acosh_',\n",
       " 'add',\n",
       " 'add_',\n",
       " 'addbmm',\n",
       " 'addbmm_',\n",
       " 'addcdiv',\n",
       " 'addcdiv_',\n",
       " 'addcmul',\n",
       " 'addcmul_',\n",
       " 'addmm',\n",
       " 'addmm_',\n",
       " 'addmv',\n",
       " 'addmv_',\n",
       " 'addr',\n",
       " 'addr_',\n",
       " 'align_as',\n",
       " 'align_to',\n",
       " 'all',\n",
       " 'allclose',\n",
       " 'amax',\n",
       " 'amin',\n",
       " 'angle',\n",
       " 'any',\n",
       " 'apply_',\n",
       " 'arccos',\n",
       " 'arccos_',\n",
       " 'arccosh',\n",
       " 'arccosh_',\n",
       " 'arcsin',\n",
       " 'arcsin_',\n",
       " 'arcsinh',\n",
       " 'arcsinh_',\n",
       " 'arctan',\n",
       " 'arctan_',\n",
       " 'arctanh',\n",
       " 'arctanh_',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'as_strided',\n",
       " 'as_strided_',\n",
       " 'as_subclass',\n",
       " 'asin',\n",
       " 'asin_',\n",
       " 'asinh',\n",
       " 'asinh_',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atan2_',\n",
       " 'atan_',\n",
       " 'atanh',\n",
       " 'atanh_',\n",
       " 'backward',\n",
       " 'baddbmm',\n",
       " 'baddbmm_',\n",
       " 'bernoulli',\n",
       " 'bernoulli_',\n",
       " 'bfloat16',\n",
       " 'bincount',\n",
       " 'bitwise_and',\n",
       " 'bitwise_and_',\n",
       " 'bitwise_not',\n",
       " 'bitwise_not_',\n",
       " 'bitwise_or',\n",
       " 'bitwise_or_',\n",
       " 'bitwise_xor',\n",
       " 'bitwise_xor_',\n",
       " 'bmm',\n",
       " 'bool',\n",
       " 'broadcast_to',\n",
       " 'byte',\n",
       " 'cauchy_',\n",
       " 'cdouble',\n",
       " 'ceil',\n",
       " 'ceil_',\n",
       " 'cfloat',\n",
       " 'char',\n",
       " 'cholesky',\n",
       " 'cholesky_inverse',\n",
       " 'cholesky_solve',\n",
       " 'chunk',\n",
       " 'clamp',\n",
       " 'clamp_',\n",
       " 'clamp_max',\n",
       " 'clamp_max_',\n",
       " 'clamp_min',\n",
       " 'clamp_min_',\n",
       " 'clip',\n",
       " 'clip_',\n",
       " 'clone',\n",
       " 'coalesce',\n",
       " 'col_indices',\n",
       " 'conj',\n",
       " 'contiguous',\n",
       " 'copy_',\n",
       " 'copysign',\n",
       " 'copysign_',\n",
       " 'cos',\n",
       " 'cos_',\n",
       " 'cosh',\n",
       " 'cosh_',\n",
       " 'count_nonzero',\n",
       " 'cpu',\n",
       " 'cross',\n",
       " 'crow_indices',\n",
       " 'cuda',\n",
       " 'cummax',\n",
       " 'cummin',\n",
       " 'cumprod',\n",
       " 'cumprod_',\n",
       " 'cumsum',\n",
       " 'cumsum_',\n",
       " 'data',\n",
       " 'data_ptr',\n",
       " 'deg2rad',\n",
       " 'deg2rad_',\n",
       " 'dense_dim',\n",
       " 'dequantize',\n",
       " 'det',\n",
       " 'detach',\n",
       " 'detach_',\n",
       " 'device',\n",
       " 'diag',\n",
       " 'diag_embed',\n",
       " 'diagflat',\n",
       " 'diagonal',\n",
       " 'diff',\n",
       " 'digamma',\n",
       " 'digamma_',\n",
       " 'dim',\n",
       " 'dist',\n",
       " 'div',\n",
       " 'div_',\n",
       " 'divide',\n",
       " 'divide_',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'dsplit',\n",
       " 'dtype',\n",
       " 'eig',\n",
       " 'element_size',\n",
       " 'eq',\n",
       " 'eq_',\n",
       " 'equal',\n",
       " 'erf',\n",
       " 'erf_',\n",
       " 'erfc',\n",
       " 'erfc_',\n",
       " 'erfinv',\n",
       " 'erfinv_',\n",
       " 'exp',\n",
       " 'exp2',\n",
       " 'exp2_',\n",
       " 'exp_',\n",
       " 'expand',\n",
       " 'expand_as',\n",
       " 'expm1',\n",
       " 'expm1_',\n",
       " 'exponential_',\n",
       " 'fill_',\n",
       " 'fill_diagonal_',\n",
       " 'fix',\n",
       " 'fix_',\n",
       " 'flatten',\n",
       " 'flip',\n",
       " 'fliplr',\n",
       " 'flipud',\n",
       " 'float',\n",
       " 'float_power',\n",
       " 'float_power_',\n",
       " 'floor',\n",
       " 'floor_',\n",
       " 'floor_divide',\n",
       " 'floor_divide_',\n",
       " 'fmax',\n",
       " 'fmin',\n",
       " 'fmod',\n",
       " 'fmod_',\n",
       " 'frac',\n",
       " 'frac_',\n",
       " 'frexp',\n",
       " 'gather',\n",
       " 'gcd',\n",
       " 'gcd_',\n",
       " 'ge',\n",
       " 'ge_',\n",
       " 'geometric_',\n",
       " 'geqrf',\n",
       " 'ger',\n",
       " 'get_device',\n",
       " 'grad',\n",
       " 'grad_fn',\n",
       " 'greater',\n",
       " 'greater_',\n",
       " 'greater_equal',\n",
       " 'greater_equal_',\n",
       " 'gt',\n",
       " 'gt_',\n",
       " 'half',\n",
       " 'hardshrink',\n",
       " 'has_names',\n",
       " 'heaviside',\n",
       " 'heaviside_',\n",
       " 'histc',\n",
       " 'hsplit',\n",
       " 'hypot',\n",
       " 'hypot_',\n",
       " 'i0',\n",
       " 'i0_',\n",
       " 'igamma',\n",
       " 'igamma_',\n",
       " 'igammac',\n",
       " 'igammac_',\n",
       " 'imag',\n",
       " 'index_add',\n",
       " 'index_add_',\n",
       " 'index_copy',\n",
       " 'index_copy_',\n",
       " 'index_fill',\n",
       " 'index_fill_',\n",
       " 'index_put',\n",
       " 'index_put_',\n",
       " 'index_select',\n",
       " 'indices',\n",
       " 'inner',\n",
       " 'int',\n",
       " 'int_repr',\n",
       " 'inverse',\n",
       " 'is_coalesced',\n",
       " 'is_complex',\n",
       " 'is_contiguous',\n",
       " 'is_cuda',\n",
       " 'is_distributed',\n",
       " 'is_floating_point',\n",
       " 'is_leaf',\n",
       " 'is_meta',\n",
       " 'is_mkldnn',\n",
       " 'is_mlc',\n",
       " 'is_nonzero',\n",
       " 'is_pinned',\n",
       " 'is_quantized',\n",
       " 'is_same_size',\n",
       " 'is_set_to',\n",
       " 'is_shared',\n",
       " 'is_signed',\n",
       " 'is_sparse',\n",
       " 'is_sparse_csr',\n",
       " 'is_vulkan',\n",
       " 'is_xpu',\n",
       " 'isclose',\n",
       " 'isfinite',\n",
       " 'isinf',\n",
       " 'isnan',\n",
       " 'isneginf',\n",
       " 'isposinf',\n",
       " 'isreal',\n",
       " 'istft',\n",
       " 'item',\n",
       " 'kron',\n",
       " 'kthvalue',\n",
       " 'layout',\n",
       " 'lcm',\n",
       " 'lcm_',\n",
       " 'ldexp',\n",
       " 'ldexp_',\n",
       " 'le',\n",
       " 'le_',\n",
       " 'lerp',\n",
       " 'lerp_',\n",
       " 'less',\n",
       " 'less_',\n",
       " 'less_equal',\n",
       " 'less_equal_',\n",
       " 'lgamma',\n",
       " 'lgamma_',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log10_',\n",
       " 'log1p',\n",
       " 'log1p_',\n",
       " 'log2',\n",
       " 'log2_',\n",
       " 'log_',\n",
       " 'log_normal_',\n",
       " 'log_softmax',\n",
       " 'logaddexp',\n",
       " 'logaddexp2',\n",
       " 'logcumsumexp',\n",
       " 'logdet',\n",
       " 'logical_and',\n",
       " 'logical_and_',\n",
       " 'logical_not',\n",
       " 'logical_not_',\n",
       " 'logical_or',\n",
       " 'logical_or_',\n",
       " 'logical_xor',\n",
       " 'logical_xor_',\n",
       " 'logit',\n",
       " 'logit_',\n",
       " 'logsumexp',\n",
       " 'long',\n",
       " 'lstsq',\n",
       " 'lt',\n",
       " 'lt_',\n",
       " 'lu',\n",
       " 'lu_solve',\n",
       " 'map2_',\n",
       " 'map_',\n",
       " 'masked_fill',\n",
       " 'masked_fill_',\n",
       " 'masked_scatter',\n",
       " 'masked_scatter_',\n",
       " 'masked_select',\n",
       " 'matmul',\n",
       " 'matrix_exp',\n",
       " 'matrix_power',\n",
       " 'max',\n",
       " 'maximum',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'min',\n",
       " 'minimum',\n",
       " 'mm',\n",
       " 'mode',\n",
       " 'moveaxis',\n",
       " 'movedim',\n",
       " 'msort',\n",
       " 'mul',\n",
       " 'mul_',\n",
       " 'multinomial',\n",
       " 'multiply',\n",
       " 'multiply_',\n",
       " 'mv',\n",
       " 'mvlgamma',\n",
       " 'mvlgamma_',\n",
       " 'name',\n",
       " 'names',\n",
       " 'nan_to_num',\n",
       " 'nan_to_num_',\n",
       " 'nanmedian',\n",
       " 'nanquantile',\n",
       " 'nansum',\n",
       " 'narrow',\n",
       " 'narrow_copy',\n",
       " 'ndim',\n",
       " 'ndimension',\n",
       " 'ne',\n",
       " 'ne_',\n",
       " 'neg',\n",
       " 'neg_',\n",
       " 'negative',\n",
       " 'negative_',\n",
       " 'nelement',\n",
       " 'new',\n",
       " 'new_empty',\n",
       " 'new_empty_strided',\n",
       " 'new_full',\n",
       " 'new_ones',\n",
       " 'new_tensor',\n",
       " 'new_zeros',\n",
       " 'nextafter',\n",
       " 'nextafter_',\n",
       " 'nonzero',\n",
       " 'norm',\n",
       " 'normal_',\n",
       " 'not_equal',\n",
       " 'not_equal_',\n",
       " 'numel',\n",
       " 'numpy',\n",
       " 'orgqr',\n",
       " 'ormqr',\n",
       " 'outer',\n",
       " 'output_nr',\n",
       " 'permute',\n",
       " 'pin_memory',\n",
       " 'pinverse',\n",
       " 'polygamma',\n",
       " 'polygamma_',\n",
       " 'positive',\n",
       " 'pow',\n",
       " 'pow_',\n",
       " 'prelu',\n",
       " 'prod',\n",
       " 'put',\n",
       " 'put_',\n",
       " 'q_per_channel_axis',\n",
       " 'q_per_channel_scales',\n",
       " 'q_per_channel_zero_points',\n",
       " 'q_scale',\n",
       " 'q_zero_point',\n",
       " 'qr',\n",
       " 'qscheme',\n",
       " 'quantile',\n",
       " 'rad2deg',\n",
       " 'rad2deg_',\n",
       " 'random_',\n",
       " 'ravel',\n",
       " 'real',\n",
       " 'reciprocal',\n",
       " 'reciprocal_',\n",
       " 'record_stream',\n",
       " 'refine_names',\n",
       " 'register_hook',\n",
       " 'reinforce',\n",
       " 'relu',\n",
       " 'relu_',\n",
       " 'remainder',\n",
       " 'remainder_',\n",
       " 'rename',\n",
       " 'rename_',\n",
       " 'renorm',\n",
       " 'renorm_',\n",
       " 'repeat',\n",
       " 'repeat_interleave',\n",
       " 'requires_grad',\n",
       " 'requires_grad_',\n",
       " 'reshape',\n",
       " 'reshape_as',\n",
       " 'resize',\n",
       " 'resize_',\n",
       " 'resize_as',\n",
       " 'resize_as_',\n",
       " 'retain_grad',\n",
       " 'roll',\n",
       " 'rot90',\n",
       " 'round',\n",
       " 'round_',\n",
       " 'rsqrt',\n",
       " 'rsqrt_',\n",
       " 'scatter',\n",
       " 'scatter_',\n",
       " 'scatter_add',\n",
       " 'scatter_add_',\n",
       " 'select',\n",
       " 'set_',\n",
       " 'sgn',\n",
       " 'sgn_',\n",
       " 'shape',\n",
       " 'share_memory_',\n",
       " 'short',\n",
       " 'sigmoid',\n",
       " 'sigmoid_',\n",
       " 'sign',\n",
       " 'sign_',\n",
       " 'signbit',\n",
       " 'sin',\n",
       " 'sin_',\n",
       " 'sinc',\n",
       " 'sinc_',\n",
       " 'sinh',\n",
       " 'sinh_',\n",
       " 'size',\n",
       " 'slogdet',\n",
       " 'smm',\n",
       " 'softmax',\n",
       " 'solve',\n",
       " 'sort',\n",
       " 'sparse_dim',\n",
       " 'sparse_mask',\n",
       " 'sparse_resize_',\n",
       " 'sparse_resize_and_clear_',\n",
       " 'split',\n",
       " 'split_with_sizes',\n",
       " 'sqrt',\n",
       " 'sqrt_',\n",
       " 'square',\n",
       " 'square_',\n",
       " 'squeeze',\n",
       " 'squeeze_',\n",
       " 'sspaddmm',\n",
       " 'std',\n",
       " 'stft',\n",
       " 'storage',\n",
       " 'storage_offset',\n",
       " 'storage_type',\n",
       " 'stride',\n",
       " 'sub',\n",
       " 'sub_',\n",
       " 'subtract',\n",
       " 'subtract_',\n",
       " 'sum',\n",
       " 'sum_to_size',\n",
       " 'svd',\n",
       " 'swapaxes',\n",
       " 'swapaxes_',\n",
       " 'swapdims',\n",
       " 'swapdims_',\n",
       " 'symeig',\n",
       " 't',\n",
       " 't_',\n",
       " 'take',\n",
       " 'take_along_dim',\n",
       " 'tan',\n",
       " 'tan_',\n",
       " 'tanh',\n",
       " 'tanh_',\n",
       " 'tensor_split',\n",
       " 'tile',\n",
       " 'to',\n",
       " 'to_dense',\n",
       " 'to_mkldnn',\n",
       " 'to_sparse',\n",
       " 'tolist',\n",
       " 'topk',\n",
       " 'trace',\n",
       " 'transpose',\n",
       " 'transpose_',\n",
       " 'triangular_solve',\n",
       " 'tril',\n",
       " 'tril_',\n",
       " 'triu',\n",
       " 'triu_',\n",
       " 'true_divide',\n",
       " 'true_divide_',\n",
       " 'trunc',\n",
       " 'trunc_',\n",
       " 'type',\n",
       " 'type_as',\n",
       " 'unbind',\n",
       " 'unflatten',\n",
       " 'unfold',\n",
       " 'uniform_',\n",
       " 'unique',\n",
       " 'unique_consecutive',\n",
       " 'unsafe_chunk',\n",
       " 'unsafe_split',\n",
       " 'unsafe_split_with_sizes',\n",
       " 'unsqueeze',\n",
       " 'unsqueeze_',\n",
       " 'values',\n",
       " 'var',\n",
       " 'vdot',\n",
       " 'view',\n",
       " 'view_as',\n",
       " 'vsplit',\n",
       " 'where',\n",
       " 'xlogy',\n",
       " 'xlogy_',\n",
       " 'xpu',\n",
       " 'zero_']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d18f9ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:44:39.741275Z",
     "start_time": "2021-09-04T04:44:39.736080Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2875])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04acd4aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:45:08.929633Z",
     "start_time": "2021-09-04T04:45:08.926879Z"
    }
   },
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1ab12bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:45:19.304593Z",
     "start_time": "2021-09-04T04:45:19.301131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2874811887741089"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3eea00fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:47:23.391157Z",
     "start_time": "2021-09-04T04:47:23.377873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1006, -0.2917],\n",
       "         [-0.1670,  0.5997]],\n",
       "\n",
       "        [[ 0.8305, -0.1894],\n",
       "         [-0.2472,  0.4292]],\n",
       "\n",
       "        [[ 0.5193, -0.0438],\n",
       "         [-0.3375, -0.1098]],\n",
       "\n",
       "        [[-0.2430, -0.0552],\n",
       "         [ 0.0582, -1.5964]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = z.view(4, 2, -1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a9dea689",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:48:43.125844Z",
     "start_time": "2021-09-04T04:48:43.120161Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1006, -0.2917],\n",
       "         [ 0.8305, -0.1894],\n",
       "         [ 0.5193, -0.0438],\n",
       "         [-0.2430, -0.0552]],\n",
       "\n",
       "        [[-0.1670,  0.5997],\n",
       "         [-0.2472,  0.4292],\n",
       "         [-0.3375, -0.1098],\n",
       "         [ 0.0582, -1.5964]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1585744f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:50:00.076651Z",
     "start_time": "2021-09-04T04:50:00.070298Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c590581",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:50:10.083425Z",
     "start_time": "2021-09-04T04:50:10.078891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ae4afb56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:50:33.247487Z",
     "start_time": "2021-09-04T04:50:33.229858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 1., 1., 1.], dtype=float32), tensor([1., 2., 1., 1., 1.]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[1] = 2\n",
    "b, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a07534b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:52:23.320333Z",
     "start_time": "2021-09-04T04:52:23.313367Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "12d61fdc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:52:25.454628Z",
     "start_time": "2021-09-04T04:52:25.450079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0c470542",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:53:40.126742Z",
     "start_time": "2021-09-04T04:53:40.114543Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3., 3., 3., 3., 3.]),\n",
       " tensor([2., 2., 2., 2., 2.], dtype=torch.float64))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a + 1\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6319183",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:54:42.051126Z",
     "start_time": "2021-09-04T04:54:41.716307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "18012ca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:56:20.890774Z",
     "start_time": "2021-09-04T04:56:13.813022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2875], device='cuda:0')\n",
      "tensor([-0.2875], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones_like(x, device=device)\n",
    "    x = x.to(device)\n",
    "    \n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f6e65aef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T04:57:40.717196Z",
     "start_time": "2021-09-04T04:57:40.710916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deeeede",
   "metadata": {},
   "source": [
    "简单两层神经网络，numpy\n",
    "--\n",
    "- $h = W_1X$\n",
    "- $a = max(0, h)$\n",
    "- $y_{hat} = W_2a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ed4fa96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T05:44:44.217640Z",
     "start_time": "2021-09-04T05:44:40.993282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in iteration  0  is  35499471.371750616\n",
      "loss in iteration  1  is  33198700.18387001\n",
      "loss in iteration  2  is  32929090.411952104\n",
      "loss in iteration  3  is  29267463.738303386\n",
      "loss in iteration  4  is  21869523.411157325\n",
      "loss in iteration  5  is  13524639.91176435\n",
      "loss in iteration  6  is  7503978.36112323\n",
      "loss in iteration  7  is  4096737.255422757\n",
      "loss in iteration  8  is  2423080.1270705895\n",
      "loss in iteration  9  is  1605332.5963308932\n",
      "loss in iteration  10  is  1179301.9852768725\n",
      "loss in iteration  11  is  930125.4794113592\n",
      "loss in iteration  12  is  765565.9583946442\n",
      "loss in iteration  13  is  645707.0096919467\n",
      "loss in iteration  14  is  552668.4131059229\n",
      "loss in iteration  15  is  477508.94597301784\n",
      "loss in iteration  16  is  415489.9150746898\n",
      "loss in iteration  17  is  363484.77948322106\n",
      "loss in iteration  18  is  319475.44132258825\n",
      "loss in iteration  19  is  281936.70110649563\n",
      "loss in iteration  20  is  249711.7787279779\n",
      "loss in iteration  21  is  221910.97958961964\n",
      "loss in iteration  22  is  197966.47182651487\n",
      "loss in iteration  23  is  177124.12199611665\n",
      "loss in iteration  24  is  158901.6177666636\n",
      "loss in iteration  25  is  142899.4480750074\n",
      "loss in iteration  26  is  128808.37224754848\n",
      "loss in iteration  27  is  116359.38933899038\n",
      "loss in iteration  28  is  105333.74975105445\n",
      "loss in iteration  29  is  95553.4725640404\n",
      "loss in iteration  30  is  86832.17101082741\n",
      "loss in iteration  31  is  79017.12390931587\n",
      "loss in iteration  32  is  72018.8108091573\n",
      "loss in iteration  33  is  65747.8244608187\n",
      "loss in iteration  34  is  60105.12338507948\n",
      "loss in iteration  35  is  55020.21531192775\n",
      "loss in iteration  36  is  50427.34984879107\n",
      "loss in iteration  37  is  46274.909196187975\n",
      "loss in iteration  38  is  42513.20611884199\n",
      "loss in iteration  39  is  39097.33826906301\n",
      "loss in iteration  40  is  35992.47011415245\n",
      "loss in iteration  41  is  33166.722148024826\n",
      "loss in iteration  42  is  30590.721570368234\n",
      "loss in iteration  43  is  28239.314168945344\n",
      "loss in iteration  44  is  26090.918692930733\n",
      "loss in iteration  45  is  24125.104241574823\n",
      "loss in iteration  46  is  22324.377872071567\n",
      "loss in iteration  47  is  20673.86129504894\n",
      "loss in iteration  48  is  19159.51651661986\n",
      "loss in iteration  49  is  17769.60054407332\n",
      "loss in iteration  50  is  16491.239689798036\n",
      "loss in iteration  51  is  15314.124076111671\n",
      "loss in iteration  52  is  14229.238155915387\n",
      "loss in iteration  53  is  13228.719126983418\n",
      "loss in iteration  54  is  12305.52752623787\n",
      "loss in iteration  55  is  11452.738822334682\n",
      "loss in iteration  56  is  10664.458434908414\n",
      "loss in iteration  57  is  9935.449084005879\n",
      "loss in iteration  58  is  9261.102862578395\n",
      "loss in iteration  59  is  8636.664058343273\n",
      "loss in iteration  60  is  8058.037170132852\n",
      "loss in iteration  61  is  7521.518659116043\n",
      "loss in iteration  62  is  7024.411508936322\n",
      "loss in iteration  63  is  6563.634099446466\n",
      "loss in iteration  64  is  6135.457953665376\n",
      "loss in iteration  65  is  5737.416113954849\n",
      "loss in iteration  66  is  5367.793356998631\n",
      "loss in iteration  67  is  5024.5894629632085\n",
      "loss in iteration  68  is  4704.930788399202\n",
      "loss in iteration  69  is  4407.182358552465\n",
      "loss in iteration  70  is  4129.759411965419\n",
      "loss in iteration  71  is  3871.128637097451\n",
      "loss in iteration  72  is  3630.0508630587306\n",
      "loss in iteration  73  is  3404.9592908013574\n",
      "loss in iteration  74  is  3194.8004961045262\n",
      "loss in iteration  75  is  2998.5309896245567\n",
      "loss in iteration  76  is  2815.4478521439205\n",
      "loss in iteration  77  is  2644.5013734748554\n",
      "loss in iteration  78  is  2484.5647578879466\n",
      "loss in iteration  79  is  2334.9622090500206\n",
      "loss in iteration  80  is  2194.9741432044984\n",
      "loss in iteration  81  is  2063.97266930539\n",
      "loss in iteration  82  is  1941.2568228420348\n",
      "loss in iteration  83  is  1826.3454181489979\n",
      "loss in iteration  84  is  1718.748881191919\n",
      "loss in iteration  85  is  1617.9008216028246\n",
      "loss in iteration  86  is  1523.3782293776358\n",
      "loss in iteration  87  is  1434.673254988295\n",
      "loss in iteration  88  is  1351.445084614774\n",
      "loss in iteration  89  is  1273.3424575364454\n",
      "loss in iteration  90  is  1200.006610432718\n",
      "loss in iteration  91  is  1131.136106020651\n",
      "loss in iteration  92  is  1066.4442642169288\n",
      "loss in iteration  93  is  1005.6984187695165\n",
      "loss in iteration  94  is  948.5914118467726\n",
      "loss in iteration  95  is  894.8933705987342\n",
      "loss in iteration  96  is  844.3896930059967\n",
      "loss in iteration  97  is  796.9044513694109\n",
      "loss in iteration  98  is  752.2173280262157\n",
      "loss in iteration  99  is  710.1769378433626\n",
      "loss in iteration  100  is  670.6055679135052\n",
      "loss in iteration  101  is  633.3494628408096\n",
      "loss in iteration  102  is  598.2675653393447\n",
      "loss in iteration  103  is  565.4124789375473\n",
      "loss in iteration  104  is  534.4636764721488\n",
      "loss in iteration  105  is  505.2941952039215\n",
      "loss in iteration  106  is  477.7772412112558\n",
      "loss in iteration  107  is  451.83871702153294\n",
      "loss in iteration  108  is  427.37642804712\n",
      "loss in iteration  109  is  404.2897823753426\n",
      "loss in iteration  110  is  382.51009755423877\n",
      "loss in iteration  111  is  361.9699671145079\n",
      "loss in iteration  112  is  342.5712221629183\n",
      "loss in iteration  113  is  324.2548908257104\n",
      "loss in iteration  114  is  306.96429365941697\n",
      "loss in iteration  115  is  290.63868063859377\n",
      "loss in iteration  116  is  275.2136658563718\n",
      "loss in iteration  117  is  260.6417590731542\n",
      "loss in iteration  118  is  246.89014604244235\n",
      "loss in iteration  119  is  233.88125768034382\n",
      "loss in iteration  120  is  221.60455089032024\n",
      "loss in iteration  121  is  209.9982089474165\n",
      "loss in iteration  122  is  199.0212949152493\n",
      "loss in iteration  123  is  188.64197036562\n",
      "loss in iteration  124  is  178.82391134438495\n",
      "loss in iteration  125  is  169.5387070616705\n",
      "loss in iteration  126  is  160.75110882123553\n",
      "loss in iteration  127  is  152.4356779015419\n",
      "loss in iteration  128  is  144.5686496739802\n",
      "loss in iteration  129  is  137.12126285143455\n",
      "loss in iteration  130  is  130.0699181413171\n",
      "loss in iteration  131  is  123.39796356487832\n",
      "loss in iteration  132  is  117.07706926315984\n",
      "loss in iteration  133  is  111.0902341211052\n",
      "loss in iteration  134  is  105.42107871916377\n",
      "loss in iteration  135  is  100.05099148912556\n",
      "loss in iteration  136  is  94.96224644750681\n",
      "loss in iteration  137  is  90.1408756898721\n",
      "loss in iteration  138  is  85.57411207506959\n",
      "loss in iteration  139  is  81.24404529272749\n",
      "loss in iteration  140  is  77.1407645862089\n",
      "loss in iteration  141  is  73.25242970592805\n",
      "loss in iteration  142  is  69.56341326383077\n",
      "loss in iteration  143  is  66.06582166842995\n",
      "loss in iteration  144  is  62.752110937629496\n",
      "loss in iteration  145  is  59.60739666069711\n",
      "loss in iteration  146  is  56.6244709019111\n",
      "loss in iteration  147  is  53.795992540039634\n",
      "loss in iteration  148  is  51.11195553691746\n",
      "loss in iteration  149  is  48.56543526101552\n",
      "loss in iteration  150  is  46.14944543526293\n",
      "loss in iteration  151  is  43.8566634695773\n",
      "loss in iteration  152  is  41.68128893273665\n",
      "loss in iteration  153  is  39.61742755822991\n",
      "loss in iteration  154  is  37.65733534971705\n",
      "loss in iteration  155  is  35.79711107812036\n",
      "loss in iteration  156  is  34.0311895345644\n",
      "loss in iteration  157  is  32.35506047600327\n",
      "loss in iteration  158  is  30.76303395368568\n",
      "loss in iteration  159  is  29.251370931888232\n",
      "loss in iteration  160  is  27.815478735444067\n",
      "loss in iteration  161  is  26.451897063271122\n",
      "loss in iteration  162  is  25.15667398661786\n",
      "loss in iteration  163  is  23.926424211880892\n",
      "loss in iteration  164  is  22.757870917821023\n",
      "loss in iteration  165  is  21.647384588060312\n",
      "loss in iteration  166  is  20.592386862573385\n",
      "loss in iteration  167  is  19.590201164954273\n",
      "loss in iteration  168  is  18.637592125470828\n",
      "loss in iteration  169  is  17.73283501180564\n",
      "loss in iteration  170  is  16.873119979279508\n",
      "loss in iteration  171  is  16.055267919530255\n",
      "loss in iteration  172  is  15.27817789472436\n",
      "loss in iteration  173  is  14.539581129304867\n",
      "loss in iteration  174  is  13.837256494918496\n",
      "loss in iteration  175  is  13.16963140650947\n",
      "loss in iteration  176  is  12.534648693715893\n",
      "loss in iteration  177  is  11.931100001849488\n",
      "loss in iteration  178  is  11.357126104213844\n",
      "loss in iteration  179  is  10.811352430331311\n",
      "loss in iteration  180  is  10.292388747092247\n",
      "loss in iteration  181  is  9.798604802725261\n",
      "loss in iteration  182  is  9.328982396027964\n",
      "loss in iteration  183  is  8.88257264082739\n",
      "loss in iteration  184  is  8.457639257953772\n",
      "loss in iteration  185  is  8.053588554981719\n",
      "loss in iteration  186  is  7.6690860459954795\n",
      "loss in iteration  187  is  7.303233406228151\n",
      "loss in iteration  188  is  6.955158146800745\n",
      "loss in iteration  189  is  6.623916855118446\n",
      "loss in iteration  190  is  6.308831488108216\n",
      "loss in iteration  191  is  6.009001545454204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in iteration  192  is  5.723603318588954\n",
      "loss in iteration  193  is  5.4520492377751655\n",
      "loss in iteration  194  is  5.193544191729488\n",
      "loss in iteration  195  is  4.947587462492193\n",
      "loss in iteration  196  is  4.713551747279455\n",
      "loss in iteration  197  is  4.490701842187775\n",
      "loss in iteration  198  is  4.278480700323033\n",
      "loss in iteration  199  is  4.076462666907077\n",
      "loss in iteration  200  is  3.8841364571285713\n",
      "loss in iteration  201  is  3.701007208648078\n",
      "loss in iteration  202  is  3.5266819183440874\n",
      "loss in iteration  203  is  3.3606723883558023\n",
      "loss in iteration  204  is  3.202646142089893\n",
      "loss in iteration  205  is  3.05211738285488\n",
      "loss in iteration  206  is  2.9087737065999293\n",
      "loss in iteration  207  is  2.772285860260407\n",
      "loss in iteration  208  is  2.6422355490161866\n",
      "loss in iteration  209  is  2.518495682093545\n",
      "loss in iteration  210  is  2.400571036819282\n",
      "loss in iteration  211  is  2.2882385574918014\n",
      "loss in iteration  212  is  2.18120089722181\n",
      "loss in iteration  213  is  2.0792647809447806\n",
      "loss in iteration  214  is  1.9821693093537505\n",
      "loss in iteration  215  is  1.8896418103084534\n",
      "loss in iteration  216  is  1.8015032354036684\n",
      "loss in iteration  217  is  1.7175254773420303\n",
      "loss in iteration  218  is  1.637554956834145\n",
      "loss in iteration  219  is  1.5612983820591468\n",
      "loss in iteration  220  is  1.4886619591422834\n",
      "loss in iteration  221  is  1.4194441932662933\n",
      "loss in iteration  222  is  1.35351886630479\n",
      "loss in iteration  223  is  1.2906837689788444\n",
      "loss in iteration  224  is  1.2307800304881251\n",
      "loss in iteration  225  is  1.173696682045274\n",
      "loss in iteration  226  is  1.1192882826466293\n",
      "loss in iteration  227  is  1.0674427632000336\n",
      "loss in iteration  228  is  1.0180280557666022\n",
      "loss in iteration  229  is  0.970939750049468\n",
      "loss in iteration  230  is  0.9260317187066938\n",
      "loss in iteration  231  is  0.8832433928925585\n",
      "loss in iteration  232  is  0.8424427085977798\n",
      "loss in iteration  233  is  0.8035540963337895\n",
      "loss in iteration  234  is  0.7664746844329091\n",
      "loss in iteration  235  is  0.7311333069180657\n",
      "loss in iteration  236  is  0.6974521954639205\n",
      "loss in iteration  237  is  0.6653220697491158\n",
      "loss in iteration  238  is  0.6346897704304114\n",
      "loss in iteration  239  is  0.6054870376554096\n",
      "loss in iteration  240  is  0.5776410013279076\n",
      "loss in iteration  241  is  0.5510860627797288\n",
      "loss in iteration  242  is  0.5257772080809866\n",
      "loss in iteration  243  is  0.5016345073559537\n",
      "loss in iteration  244  is  0.478610763505071\n",
      "loss in iteration  245  is  0.45665171649196096\n",
      "loss in iteration  246  is  0.43571715703396324\n",
      "loss in iteration  247  is  0.41574798925723466\n",
      "loss in iteration  248  is  0.3967099985152892\n",
      "loss in iteration  249  is  0.3785555925034418\n",
      "loss in iteration  250  is  0.36123943892486043\n",
      "loss in iteration  251  is  0.34471373410737577\n",
      "loss in iteration  252  is  0.32895564944150746\n",
      "loss in iteration  253  is  0.3139208357542535\n",
      "loss in iteration  254  is  0.299583145856597\n",
      "loss in iteration  255  is  0.2859032235656118\n",
      "loss in iteration  256  is  0.27285734336546474\n",
      "loss in iteration  257  is  0.2604106755663441\n",
      "loss in iteration  258  is  0.24853764781078197\n",
      "loss in iteration  259  is  0.23720953798592276\n",
      "loss in iteration  260  is  0.2264029533911224\n",
      "loss in iteration  261  is  0.2160960466102118\n",
      "loss in iteration  262  is  0.2062655371388798\n",
      "loss in iteration  263  is  0.19688287186839035\n",
      "loss in iteration  264  is  0.1879294148169901\n",
      "loss in iteration  265  is  0.17938602135944456\n",
      "loss in iteration  266  is  0.17123500626826618\n",
      "loss in iteration  267  is  0.16346025512302081\n",
      "loss in iteration  268  is  0.15604105131459933\n",
      "loss in iteration  269  is  0.1489596325706049\n",
      "loss in iteration  270  is  0.14220201063430316\n",
      "loss in iteration  271  is  0.13575406589196798\n",
      "loss in iteration  272  is  0.12960105949731612\n",
      "loss in iteration  273  is  0.12372836209592752\n",
      "loss in iteration  274  is  0.11812546799883944\n",
      "loss in iteration  275  is  0.11277689359019694\n",
      "loss in iteration  276  is  0.10767630691727696\n",
      "loss in iteration  277  is  0.1028045632086753\n",
      "loss in iteration  278  is  0.09815585007008262\n",
      "loss in iteration  279  is  0.09371794348266181\n",
      "loss in iteration  280  is  0.08948338043374682\n",
      "loss in iteration  281  is  0.08544077025342475\n",
      "loss in iteration  282  is  0.0815833887371587\n",
      "loss in iteration  283  is  0.07790059455366473\n",
      "loss in iteration  284  is  0.07438691052210888\n",
      "loss in iteration  285  is  0.07103246767079884\n",
      "loss in iteration  286  is  0.06782961004860567\n",
      "loss in iteration  287  is  0.06477281915401098\n",
      "loss in iteration  288  is  0.061854932234841106\n",
      "loss in iteration  289  is  0.05906949649749404\n",
      "loss in iteration  290  is  0.0564102383465816\n",
      "loss in iteration  291  is  0.05387078973211617\n",
      "loss in iteration  292  is  0.05144716833540091\n",
      "loss in iteration  293  is  0.049133028114102875\n",
      "loss in iteration  294  is  0.04692373886360028\n",
      "loss in iteration  295  is  0.04481418249705263\n",
      "loss in iteration  296  is  0.0428003807880438\n",
      "loss in iteration  297  is  0.040877470214166184\n",
      "loss in iteration  298  is  0.03904160636935125\n",
      "loss in iteration  299  is  0.03728890138734753\n",
      "loss in iteration  300  is  0.03561531475223419\n",
      "loss in iteration  301  is  0.034017276045167744\n",
      "loss in iteration  302  is  0.03249181845998325\n",
      "loss in iteration  303  is  0.03103608511896815\n",
      "loss in iteration  304  is  0.02964491191772952\n",
      "loss in iteration  305  is  0.028316442564335447\n",
      "loss in iteration  306  is  0.027047711030530348\n",
      "loss in iteration  307  is  0.02583682288257064\n",
      "loss in iteration  308  is  0.024680074855974336\n",
      "loss in iteration  309  is  0.023575570582325366\n",
      "loss in iteration  310  is  0.02252050268654951\n",
      "loss in iteration  311  is  0.02151316470520718\n",
      "loss in iteration  312  is  0.020551052236993918\n",
      "loss in iteration  313  is  0.019632399397339486\n",
      "loss in iteration  314  is  0.018755033958915295\n",
      "loss in iteration  315  is  0.017917018135136378\n",
      "loss in iteration  316  is  0.017116991170193725\n",
      "loss in iteration  317  is  0.016352793685025697\n",
      "loss in iteration  318  is  0.01562273465296548\n",
      "loss in iteration  319  is  0.014925526904648097\n",
      "loss in iteration  320  is  0.014259743500543366\n",
      "loss in iteration  321  is  0.013623572606626408\n",
      "loss in iteration  322  is  0.013016065361717167\n",
      "loss in iteration  323  is  0.012435661562471508\n",
      "loss in iteration  324  is  0.011881477173921642\n",
      "loss in iteration  325  is  0.011352011954075805\n",
      "loss in iteration  326  is  0.010846374766757818\n",
      "loss in iteration  327  is  0.01036325581802646\n",
      "loss in iteration  328  is  0.009901950323097465\n",
      "loss in iteration  329  is  0.00946122503976072\n",
      "loss in iteration  330  is  0.009040411915382065\n",
      "loss in iteration  331  is  0.008638150315475247\n",
      "loss in iteration  332  is  0.008254127572602025\n",
      "loss in iteration  333  is  0.007887071407001293\n",
      "loss in iteration  334  is  0.007536415086831455\n",
      "loss in iteration  335  is  0.0072014663976725755\n",
      "loss in iteration  336  is  0.006881439492786017\n",
      "loss in iteration  337  is  0.006575807086153086\n",
      "loss in iteration  338  is  0.006283703528114247\n",
      "loss in iteration  339  is  0.006004715163328558\n",
      "loss in iteration  340  is  0.005738107663064903\n",
      "loss in iteration  341  is  0.00548343845837416\n",
      "loss in iteration  342  is  0.005240083580352343\n",
      "loss in iteration  343  is  0.005007714104355773\n",
      "loss in iteration  344  is  0.00478566081428846\n",
      "loss in iteration  345  is  0.004573460976766285\n",
      "loss in iteration  346  is  0.0043707278592366484\n",
      "loss in iteration  347  is  0.0041769747536755195\n",
      "loss in iteration  348  is  0.003991884543268508\n",
      "loss in iteration  349  is  0.0038150508601222715\n",
      "loss in iteration  350  is  0.003646088762604915\n",
      "loss in iteration  351  is  0.003484619963127363\n",
      "loss in iteration  352  is  0.0033303462702196805\n",
      "loss in iteration  353  is  0.003182902290662097\n",
      "loss in iteration  354  is  0.003042049858865901\n",
      "loss in iteration  355  is  0.0029074677115429283\n",
      "loss in iteration  356  is  0.002778866480225725\n",
      "loss in iteration  357  is  0.002656053922355155\n",
      "loss in iteration  358  is  0.0025385903183894353\n",
      "loss in iteration  359  is  0.0024263666806585704\n",
      "loss in iteration  360  is  0.0023190999822851267\n",
      "loss in iteration  361  is  0.002216625548538304\n",
      "loss in iteration  362  is  0.00211868166710378\n",
      "loss in iteration  363  is  0.0020250908580368666\n",
      "loss in iteration  364  is  0.0019356527918159973\n",
      "loss in iteration  365  is  0.0018501796836374665\n",
      "loss in iteration  366  is  0.0017684978133063823\n",
      "loss in iteration  367  is  0.0016904359949105677\n",
      "loss in iteration  368  is  0.0016158433087857696\n",
      "loss in iteration  369  is  0.0015445447918903308\n",
      "loss in iteration  370  is  0.0014764399042320265\n",
      "loss in iteration  371  is  0.0014113494316987042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in iteration  372  is  0.0013491295402145497\n",
      "loss in iteration  373  is  0.0012896469326285891\n",
      "loss in iteration  374  is  0.0012327948990126483\n",
      "loss in iteration  375  is  0.0011784608433346613\n",
      "loss in iteration  376  is  0.0011265307899121506\n",
      "loss in iteration  377  is  0.0010769059664015143\n",
      "loss in iteration  378  is  0.001029466198160382\n",
      "loss in iteration  379  is  0.0009841326735778068\n",
      "loss in iteration  380  is  0.0009407928748787345\n",
      "loss in iteration  381  is  0.0008993767721379316\n",
      "loss in iteration  382  is  0.000859787895417955\n",
      "loss in iteration  383  is  0.0008219507313241755\n",
      "loss in iteration  384  is  0.0007858034976890186\n",
      "loss in iteration  385  is  0.0007512421428655559\n",
      "loss in iteration  386  is  0.0007182030998991886\n",
      "loss in iteration  387  is  0.0006866172833850167\n",
      "loss in iteration  388  is  0.0006564270641637958\n",
      "loss in iteration  389  is  0.0006275733860394819\n",
      "loss in iteration  390  is  0.0005999887143964411\n",
      "loss in iteration  391  is  0.0005736181954927766\n",
      "loss in iteration  392  is  0.0005484100007747839\n",
      "loss in iteration  393  is  0.0005243222649233056\n",
      "loss in iteration  394  is  0.000501291512827502\n",
      "loss in iteration  395  is  0.0004792743733877392\n",
      "loss in iteration  396  is  0.00045822508255796484\n",
      "loss in iteration  397  is  0.0004381144099833893\n",
      "loss in iteration  398  is  0.0004188863014280318\n",
      "loss in iteration  399  is  0.0004005013449991734\n",
      "loss in iteration  400  is  0.00038292552171595917\n",
      "loss in iteration  401  is  0.0003661222852418051\n",
      "loss in iteration  402  is  0.0003500592380116885\n",
      "loss in iteration  403  is  0.00033470348445107214\n",
      "loss in iteration  404  is  0.00032002428624018094\n",
      "loss in iteration  405  is  0.0003059898012330623\n",
      "loss in iteration  406  is  0.0002925777021671491\n",
      "loss in iteration  407  is  0.00027975026743481555\n",
      "loss in iteration  408  is  0.0002674889115571201\n",
      "loss in iteration  409  is  0.00025576577520875247\n",
      "loss in iteration  410  is  0.00024455762033109173\n",
      "loss in iteration  411  is  0.0002338508968010125\n",
      "loss in iteration  412  is  0.00022360619233634002\n",
      "loss in iteration  413  is  0.00021381212815981964\n",
      "loss in iteration  414  is  0.00020444754924457302\n",
      "loss in iteration  415  is  0.0001954974510385265\n",
      "loss in iteration  416  is  0.00018693931154969585\n",
      "loss in iteration  417  is  0.0001787556560199742\n",
      "loss in iteration  418  is  0.0001709324006164461\n",
      "loss in iteration  419  is  0.00016345160229074132\n",
      "loss in iteration  420  is  0.00015630003268751763\n",
      "loss in iteration  421  is  0.0001494609582588565\n",
      "loss in iteration  422  is  0.00014292394701489054\n",
      "loss in iteration  423  is  0.00013667274115294954\n",
      "loss in iteration  424  is  0.00013069841763127752\n",
      "loss in iteration  425  is  0.00012498456455064925\n",
      "loss in iteration  426  is  0.00011951995832142564\n",
      "loss in iteration  427  is  0.00011429504735486832\n",
      "loss in iteration  428  is  0.00010929921559957292\n",
      "loss in iteration  429  is  0.00010452282164888482\n",
      "loss in iteration  430  is  9.995579873981679e-05\n",
      "loss in iteration  431  is  9.55882314451426e-05\n",
      "loss in iteration  432  is  9.14123822766947e-05\n",
      "loss in iteration  433  is  8.741919263312989e-05\n",
      "loss in iteration  434  is  8.360125969488535e-05\n",
      "loss in iteration  435  is  7.994996108004281e-05\n",
      "loss in iteration  436  is  7.645940534194323e-05\n",
      "loss in iteration  437  is  7.312095150899377e-05\n",
      "loss in iteration  438  is  6.993171332114017e-05\n",
      "loss in iteration  439  is  6.687981825438537e-05\n",
      "loss in iteration  440  is  6.396156815306751e-05\n",
      "loss in iteration  441  is  6.117058820558527e-05\n",
      "loss in iteration  442  is  5.850149616087863e-05\n",
      "loss in iteration  443  is  5.594935292867664e-05\n",
      "loss in iteration  444  is  5.350870657838318e-05\n",
      "loss in iteration  445  is  5.117486748367001e-05\n",
      "loss in iteration  446  is  4.8942938569458054e-05\n",
      "loss in iteration  447  is  4.680960945961105e-05\n",
      "loss in iteration  448  is  4.476872084656042e-05\n",
      "loss in iteration  449  is  4.281686561623362e-05\n",
      "loss in iteration  450  is  4.0950495068465693e-05\n",
      "loss in iteration  451  is  3.916610615010355e-05\n",
      "loss in iteration  452  is  3.745998497300948e-05\n",
      "loss in iteration  453  is  3.582765828916423e-05\n",
      "loss in iteration  454  is  3.42666382330417e-05\n",
      "loss in iteration  455  is  3.2773841230139875e-05\n",
      "loss in iteration  456  is  3.1346328742276986e-05\n",
      "loss in iteration  457  is  2.9981008038506282e-05\n",
      "loss in iteration  458  is  2.8675235385091903e-05\n",
      "loss in iteration  459  is  2.7426497958062044e-05\n",
      "loss in iteration  460  is  2.6232271354382475e-05\n",
      "loss in iteration  461  is  2.5090352244128443e-05\n",
      "loss in iteration  462  is  2.3998350498349226e-05\n",
      "loss in iteration  463  is  2.295361481000422e-05\n",
      "loss in iteration  464  is  2.1954699713922403e-05\n",
      "loss in iteration  465  is  2.099975207645732e-05\n",
      "loss in iteration  466  is  2.0086173666580883e-05\n",
      "loss in iteration  467  is  1.9212198808146586e-05\n",
      "loss in iteration  468  is  1.8376379803527473e-05\n",
      "loss in iteration  469  is  1.757697162742858e-05\n",
      "loss in iteration  470  is  1.6812340780036962e-05\n",
      "loss in iteration  471  is  1.6081261260727005e-05\n",
      "loss in iteration  472  is  1.538208066462636e-05\n",
      "loss in iteration  473  is  1.4713233175124387e-05\n",
      "loss in iteration  474  is  1.4073477625381183e-05\n",
      "loss in iteration  475  is  1.3461646314342044e-05\n",
      "loss in iteration  476  is  1.2876468307422819e-05\n",
      "loss in iteration  477  is  1.2316781671098026e-05\n",
      "loss in iteration  478  is  1.178164929947229e-05\n",
      "loss in iteration  479  is  1.1269898582181537e-05\n",
      "loss in iteration  480  is  1.0780182659429462e-05\n",
      "loss in iteration  481  is  1.0311788554476406e-05\n",
      "loss in iteration  482  is  9.863822937374963e-06\n",
      "loss in iteration  483  is  9.435363473414744e-06\n",
      "loss in iteration  484  is  9.025519742880955e-06\n",
      "loss in iteration  485  is  8.633536169359782e-06\n",
      "loss in iteration  486  is  8.258646706748863e-06\n",
      "loss in iteration  487  is  7.90005737412711e-06\n",
      "loss in iteration  488  is  7.557103613523997e-06\n",
      "loss in iteration  489  is  7.229023908471772e-06\n",
      "loss in iteration  490  is  6.915212941035355e-06\n",
      "loss in iteration  491  is  6.6150408567900126e-06\n",
      "loss in iteration  492  is  6.3280883251101544e-06\n",
      "loss in iteration  493  is  6.053508806652238e-06\n",
      "loss in iteration  494  is  5.790791331453542e-06\n",
      "loss in iteration  495  is  5.539555634900746e-06\n",
      "loss in iteration  496  is  5.299181322647029e-06\n",
      "loss in iteration  497  is  5.069288256504512e-06\n",
      "loss in iteration  498  is  4.8493846818530294e-06\n",
      "loss in iteration  499  is  4.639034999309763e-06\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机初始化训练数据 \n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "lr = 1e-6\n",
    "for iter in range(500):\n",
    "    # Forward Pass\n",
    "    h = x.dot(w1) #(N, H)\n",
    "    a = np.maximum(0, h) #(N, H)\n",
    "    y_pred = a.dot(w2) #(N, D_out)\n",
    "    \n",
    "    # Loss Computation, use MSE\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(\"loss in iteration \", iter, \" is \", loss)\n",
    "    \n",
    "    # Backward Pass\n",
    "    # compute gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y) #(N, D_out)\n",
    "    grad_w2 = a.T.dot(grad_y_pred)\n",
    "    grad_a = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_a.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # update parameters\n",
    "    w1 -= lr * grad_w1\n",
    "    w2 -= lr * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1395cefd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T05:45:38.652941Z",
     "start_time": "2021-09-04T05:45:38.633333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.86150163, -0.83943181,  0.17537205, ..., -2.11838564,\n",
       "          1.24132794,  1.20330228],\n",
       "        [ 0.53916615,  1.45380525, -0.69056269, ..., -1.34368336,\n",
       "          0.12676959,  1.13601711],\n",
       "        [ 1.29313905,  1.15980405,  0.18437948, ..., -0.02814784,\n",
       "         -1.39120364,  1.24071425],\n",
       "        ...,\n",
       "        [-1.95291176,  1.01387744,  1.18553102, ...,  0.93305743,\n",
       "          0.75565341,  0.32012504],\n",
       "        [-1.10008437, -0.45796562,  0.12604735, ..., -0.59049471,\n",
       "         -0.14954924,  1.25975674],\n",
       "        [-1.21683632,  1.24055819, -0.70096318, ..., -0.33003863,\n",
       "          0.75989032, -0.49131992]]),\n",
       " array([[-1.56213991e+00,  1.16134735e+00,  1.63773439e+00,\n",
       "          2.95979792e-01, -8.17967185e-01, -1.67013837e+00,\n",
       "          1.03454718e-01,  2.17690286e+00, -7.64046298e-01,\n",
       "          1.81340190e-01],\n",
       "        [-9.93059454e-01,  1.49665512e-01, -9.12375034e-01,\n",
       "          2.91621952e-02, -1.07601366e+00,  6.41184734e-01,\n",
       "         -5.43980666e-01,  8.12865359e-01,  2.20465402e+00,\n",
       "         -1.16918654e+00],\n",
       "        [-2.70720801e-02, -1.18976302e+00,  2.36386362e-01,\n",
       "          5.45706828e-01,  9.07800665e-01, -3.62834130e-01,\n",
       "         -3.21104558e-01,  9.72594688e-01, -1.32751443e+00,\n",
       "          8.78374545e-01],\n",
       "        [-3.21314022e-01, -6.96879587e-01,  8.74880356e-02,\n",
       "         -1.73302303e+00,  6.94412780e-01, -9.47259561e-01,\n",
       "         -5.02997107e-01, -6.34157910e-02,  4.04077484e-01,\n",
       "         -1.10526624e+00],\n",
       "        [-1.40207651e+00,  2.88220745e+00, -1.85325888e+00,\n",
       "         -5.86577648e-01, -9.52925434e-02, -1.31171191e-01,\n",
       "          8.34252391e-01, -1.33032947e+00,  1.15087480e+00,\n",
       "          4.17535420e-01],\n",
       "        [ 1.10608226e+00, -8.14693239e-01,  8.76266873e-01,\n",
       "         -6.84810798e-01,  7.32268140e-02,  1.51772577e-01,\n",
       "          2.42643196e-01, -9.34560839e-01, -1.21579363e+00,\n",
       "         -1.25664749e+00],\n",
       "        [-9.06304676e-01,  1.35354689e+00,  1.63992886e+00,\n",
       "          2.98367415e-02,  1.43691567e+00,  5.63898723e-01,\n",
       "          1.23014127e+00,  6.53766283e-01, -2.46768154e-01,\n",
       "          1.64176830e+00],\n",
       "        [ 7.11489389e-01, -7.89467006e-01,  2.27875127e-01,\n",
       "          5.07750762e-01, -3.32366914e-02, -2.79744348e-01,\n",
       "         -8.43616152e-01, -1.55644803e+00,  1.10467383e+00,\n",
       "         -9.88275041e-01],\n",
       "        [ 1.37056699e-01, -2.55729754e-01,  2.28518095e+00,\n",
       "          3.95298796e-01, -6.07499532e-01,  1.70600826e+00,\n",
       "          1.08848867e+00, -8.81144795e-02, -6.88240793e-01,\n",
       "         -1.63212227e+00],\n",
       "        [ 1.75727456e+00, -4.75413252e-01,  1.15434236e+00,\n",
       "          1.11342142e+00, -2.42781191e+00, -9.30091267e-01,\n",
       "         -3.01029279e-02, -8.63635663e-02, -1.31591583e+00,\n",
       "          2.49645769e+00],\n",
       "        [ 5.12202299e-01,  7.86887538e-01, -1.77495350e+00,\n",
       "          3.02539491e-01,  1.14936500e+00, -1.55554170e+00,\n",
       "         -6.80128272e-01, -2.60865297e-01, -5.21094843e-01,\n",
       "         -1.06816956e+00],\n",
       "        [ 1.35390259e-01, -4.29222818e-01,  7.37529083e-01,\n",
       "         -2.98152306e-02,  9.51443149e-01,  2.16781112e+00,\n",
       "         -1.56473611e-01,  4.07497279e-01,  3.00114448e-01,\n",
       "          2.27033108e+00],\n",
       "        [-8.68178200e-01,  2.02022809e+00,  1.42043036e+00,\n",
       "         -3.00298620e-01, -5.39588270e-01,  1.87132245e-01,\n",
       "          2.26672589e+00,  1.40937788e+00,  1.92833711e-01,\n",
       "         -4.94005322e-01],\n",
       "        [-4.33616329e-01, -1.17711685e+00, -2.85275685e-01,\n",
       "          1.44831160e+00, -1.80065071e+00,  1.95567438e+00,\n",
       "          7.61719597e-01, -2.89730502e+00,  7.43322583e-01,\n",
       "         -7.22437838e-01],\n",
       "        [ 9.54552477e-01, -3.50418268e-01, -9.67737630e-01,\n",
       "          2.94394293e-01,  9.70512443e-01,  6.41078694e-01,\n",
       "         -1.18237239e+00,  1.35584684e+00, -2.07711707e+00,\n",
       "          9.23883938e-01],\n",
       "        [ 1.03492902e+00,  4.55769053e-01,  1.93909339e+00,\n",
       "          6.44395701e-01, -6.15331782e-01, -3.49065686e-01,\n",
       "          5.33535804e-01,  3.11985855e-01,  1.71718055e-01,\n",
       "         -5.04957058e-01],\n",
       "        [ 6.07012194e-01, -1.59302718e-01, -1.93106909e+00,\n",
       "          5.13501649e-01, -5.28483068e-01,  3.45140708e-01,\n",
       "         -1.62161974e+00,  5.33773016e-01,  2.08409311e+00,\n",
       "          4.05230176e-01],\n",
       "        [-2.50699069e-01, -3.31671834e-01, -1.08709480e+00,\n",
       "         -8.44807810e-01,  9.66102914e-01,  3.08738466e-01,\n",
       "         -8.73213415e-01, -8.41766853e-01,  7.19153526e-02,\n",
       "         -1.55224905e+00],\n",
       "        [ 7.48741569e-01,  7.98483765e-02, -2.59745746e-02,\n",
       "         -2.14698832e-01,  1.04508797e+00, -6.36747377e-01,\n",
       "         -1.81591939e-01, -1.14042437e+00, -1.04171500e+00,\n",
       "         -1.29834744e-01],\n",
       "        [-8.25635827e-01,  3.07713206e-02, -1.63147849e+00,\n",
       "         -6.64652883e-01, -3.69832170e-01,  6.72666779e-01,\n",
       "         -7.33116878e-01, -4.68095020e-02,  1.05796746e+00,\n",
       "         -7.70806052e-01],\n",
       "        [-1.19067031e+00,  1.63963678e+00,  7.60233188e-02,\n",
       "          1.61216199e+00,  8.96931075e-02, -5.94694205e-01,\n",
       "          2.02591351e-02,  1.07502338e+00,  6.54117243e-02,\n",
       "          1.51743068e+00],\n",
       "        [-2.42437871e+00, -3.32336972e-01, -7.91794723e-01,\n",
       "         -3.39379687e-02, -3.71450798e-02, -2.18612694e-01,\n",
       "          1.89156010e+00,  5.30995547e-01,  1.29111767e+00,\n",
       "          9.94399321e-02],\n",
       "        [ 7.59748432e-01, -1.39196752e+00,  1.54863216e-01,\n",
       "          6.52008413e-01, -8.30324311e-01,  2.95135286e-01,\n",
       "          3.28366305e-01,  3.62171550e-01,  4.17848444e-01,\n",
       "         -4.44099088e-01],\n",
       "        [ 1.10875116e+00,  1.94538833e+00,  8.25378437e-01,\n",
       "          5.40326473e-01, -1.10969435e-01, -1.03210531e-01,\n",
       "          1.67352225e+00, -1.12309316e+00, -2.49123566e-01,\n",
       "          2.77894607e-01],\n",
       "        [ 6.83670813e-02,  5.41952033e-01,  7.71509312e-01,\n",
       "          1.46677015e+00,  8.17137823e-01, -1.78833952e-01,\n",
       "          1.61696687e+00,  1.71290637e-01,  2.47016500e-01,\n",
       "         -1.04213534e+00],\n",
       "        [-4.15946850e-02, -4.67885640e-01,  4.52830771e-01,\n",
       "         -2.28743004e-01,  1.46310591e+00,  1.28845965e+00,\n",
       "          1.62918818e+00,  9.25189814e-01, -7.40061058e-01,\n",
       "         -1.99871071e-01],\n",
       "        [-7.44284822e-01,  2.64551818e-03,  5.37030926e-01,\n",
       "          1.14026484e+00, -9.33804472e-01,  1.39408843e-01,\n",
       "         -9.05170502e-02,  1.51772020e-01,  1.30633529e+00,\n",
       "          7.19390260e-01],\n",
       "        [ 2.72651637e-01, -1.14084832e+00, -3.15355870e-01,\n",
       "         -6.07111567e-01,  1.68155709e+00,  1.31295207e+00,\n",
       "         -1.95059864e+00,  2.24591278e+00, -8.86859853e-01,\n",
       "          1.20748735e+00],\n",
       "        [-1.08290681e+00,  3.78963310e-01,  8.25202952e-02,\n",
       "          6.74602127e-01,  1.59552928e-02,  3.50286738e-01,\n",
       "          1.36164272e+00, -3.26948437e-01,  1.06196188e+00,\n",
       "          6.79081208e-01],\n",
       "        [ 1.13272885e+00,  1.29314552e+00, -1.08967156e+00,\n",
       "          6.80874511e-01,  8.21259866e-01,  4.78012196e-01,\n",
       "         -1.12981114e-01, -7.28357125e-01, -4.32832050e-01,\n",
       "         -4.89670137e-02],\n",
       "        [ 9.84987102e-01,  7.02540251e-01, -1.32823720e+00,\n",
       "         -1.47193909e+00,  4.94659503e-01, -4.10100580e-01,\n",
       "         -1.13108457e+00,  7.38363960e-01,  2.64943765e+00,\n",
       "          1.27581740e-01],\n",
       "        [ 1.24823126e+00,  1.52531944e+00, -4.23490490e-02,\n",
       "         -4.81566003e-01, -7.54029135e-01, -5.83527701e-01,\n",
       "          9.60601387e-01,  7.12397658e-01,  8.49411867e-01,\n",
       "          7.93716237e-01],\n",
       "        [-2.63064420e-01,  1.08901000e+00,  3.28790656e-01,\n",
       "          1.16739308e+00, -7.32154965e-01, -7.03313732e-01,\n",
       "          9.65887219e-01,  8.65969675e-02,  1.11707351e+00,\n",
       "         -3.48292742e-01],\n",
       "        [-5.40539302e-01,  4.09872615e-01,  1.95977179e+00,\n",
       "         -6.33397030e-01, -8.97675916e-01,  2.80878169e-01,\n",
       "          3.58369893e-01, -7.00915417e-01,  1.80213214e-01,\n",
       "         -1.09891247e+00],\n",
       "        [ 3.59549928e-01,  1.00060140e+00, -2.03508984e-01,\n",
       "         -4.38934910e-01, -1.08600898e+00, -1.64295695e+00,\n",
       "          1.56770181e+00,  1.42009726e+00,  9.41038770e-01,\n",
       "          1.08377580e+00],\n",
       "        [-4.44128245e-01, -3.66897825e-01, -4.44748900e-01,\n",
       "          1.16132755e+00, -2.14663466e+00, -3.87372407e-02,\n",
       "         -3.52279698e-02,  4.23741430e-01,  2.52152059e+00,\n",
       "         -1.41531277e+00],\n",
       "        [-5.39391326e-01, -8.67136727e-01, -1.07262431e+00,\n",
       "          6.07290519e-02, -5.44677409e-01,  8.62454445e-01,\n",
       "         -1.95120568e+00,  1.62129157e-01,  4.40466456e-01,\n",
       "          1.38467769e+00],\n",
       "        [-1.07375431e+00, -2.99964518e+00, -1.45783612e+00,\n",
       "         -7.61465068e-01, -8.04342998e-01, -2.70402314e-01,\n",
       "         -4.79364987e-01,  8.31123737e-02, -3.52026666e-01,\n",
       "          3.75890792e-01],\n",
       "        [ 1.54789860e+00,  1.13588486e+00, -2.49971251e+00,\n",
       "          3.93769552e-01, -1.32269318e+00,  2.11836755e-01,\n",
       "          2.48967679e-02, -1.33155971e+00,  1.98318189e+00,\n",
       "          3.63065243e-01],\n",
       "        [ 6.95187125e-01,  2.08080840e-01, -3.63475172e-01,\n",
       "         -2.36841237e-01,  2.27973766e+00,  1.78913873e-01,\n",
       "         -1.28705782e-02, -1.54276278e+00,  9.36860851e-02,\n",
       "          1.66203635e+00],\n",
       "        [ 4.37842282e-01,  3.09089166e-01, -5.24436185e-01,\n",
       "         -1.42960270e+00, -3.92504511e-01,  5.88449436e-01,\n",
       "         -2.19969974e-01,  2.76783489e-01,  1.72547664e+00,\n",
       "          2.26760211e-01],\n",
       "        [ 5.92573157e-01,  4.54012669e-01,  1.75565316e+00,\n",
       "         -1.12396392e+00, -1.00735163e+00,  2.82824035e-01,\n",
       "          9.20142657e-01, -3.40554001e-01, -9.81145541e-02,\n",
       "         -1.07233806e-02],\n",
       "        [ 4.13303253e-01,  1.96758239e-01, -1.69897836e+00,\n",
       "         -4.82330935e-01,  1.01498677e-03, -7.46201396e-01,\n",
       "          9.59383637e-01, -6.16739045e-01, -3.80267407e-01,\n",
       "         -3.29057891e-01],\n",
       "        [-1.80148549e+00,  1.38731035e-01,  2.02814716e+00,\n",
       "         -1.02854039e-02, -2.57167100e-01,  3.62560395e-01,\n",
       "         -2.90585793e-01,  1.06891824e+00,  5.89352518e-01,\n",
       "          6.14815375e-01],\n",
       "        [ 3.64177675e-01, -1.29014136e+00, -2.01379071e-01,\n",
       "         -1.49649865e+00, -1.44374221e+00, -1.98750774e+00,\n",
       "         -1.49621944e-01,  1.89200859e+00,  6.91411239e-01,\n",
       "          1.44507551e+00],\n",
       "        [ 3.20150327e-01, -2.13708215e-01, -2.27831214e-01,\n",
       "         -4.43239082e-01,  1.55986343e-01,  1.92280527e+00,\n",
       "         -3.41298993e-01, -5.49086166e-01, -6.87726414e-01,\n",
       "          9.83593601e-01],\n",
       "        [ 3.05537642e-01,  2.72382348e-01, -7.63046393e-01,\n",
       "         -5.32947915e-01,  5.59408967e-01,  6.93167894e-01,\n",
       "          7.32060549e-01, -9.86314457e-01,  1.95409475e-01,\n",
       "          5.14893404e-01],\n",
       "        [-2.92514074e-01,  1.58184511e+00, -9.66860399e-01,\n",
       "          1.20947612e+00,  1.15426082e+00,  8.32513594e-01,\n",
       "          2.17947368e+00, -1.07019483e-01, -9.16076433e-01,\n",
       "          3.76969298e-01],\n",
       "        [-1.29233347e+00, -4.31515061e-01, -4.85850901e-01,\n",
       "         -5.73144800e-01, -1.51079875e+00, -3.30861819e-01,\n",
       "          1.62990645e+00, -3.76970742e-01,  5.63470901e-01,\n",
       "         -1.28404378e+00],\n",
       "        [ 1.19281069e+00,  8.54795892e-01,  1.58505370e-01,\n",
       "          7.49575308e-01,  3.92826409e-01,  1.76607347e+00,\n",
       "          6.70367322e-01,  1.83525935e-02, -3.65097423e-01,\n",
       "          3.36849514e-01],\n",
       "        [ 1.45781648e+00, -9.47538555e-01,  4.21606422e-01,\n",
       "         -3.46325036e-01,  3.74630879e-01,  2.60188760e-01,\n",
       "         -4.58988163e-01,  1.10605911e+00,  1.01768006e+00,\n",
       "         -8.18056745e-01],\n",
       "        [-9.63310843e-01, -5.00099211e-01, -3.92548250e-02,\n",
       "         -8.90313493e-01,  3.45193336e-02,  2.68515874e-01,\n",
       "         -2.60145344e-01,  2.67146851e-01, -1.24938845e+00,\n",
       "          4.38726849e-01],\n",
       "        [-1.69888173e+00,  1.73334711e+00,  1.68612390e+00,\n",
       "         -3.10258430e-03,  6.14436159e-01,  5.93077152e-01,\n",
       "         -3.29743171e-01,  4.90998705e-01, -1.13192007e+00,\n",
       "          5.67555462e-01],\n",
       "        [ 1.30163740e+00, -5.65142168e-01,  6.68810318e-01,\n",
       "         -5.51272907e-01, -2.02766146e-01,  1.59814564e-01,\n",
       "         -8.86116464e-01, -7.74149644e-01,  1.03960665e-02,\n",
       "          1.03791880e+00],\n",
       "        [-5.84891882e-01,  4.61773318e-01,  2.12675819e-01,\n",
       "         -2.36656690e-01,  9.68881192e-01,  4.44744999e-01,\n",
       "          9.74583455e-01,  6.88108946e-01, -1.88045190e-01,\n",
       "         -6.79315917e-01],\n",
       "        [-3.73817064e-01, -1.64814450e+00, -7.17982500e-02,\n",
       "         -9.63816012e-01,  8.15877687e-01,  1.21687268e+00,\n",
       "          7.37946352e-01, -7.58320851e-01,  2.90655310e-01,\n",
       "         -1.39412465e+00],\n",
       "        [ 3.68396716e-01,  5.90601312e-01,  8.17326070e-02,\n",
       "         -9.95054832e-02, -1.17618127e+00,  1.24912153e+00,\n",
       "          1.22584004e+00,  2.27101063e-01,  1.10035173e+00,\n",
       "          2.35091752e-01],\n",
       "        [-8.18098089e-01, -1.17225826e+00, -1.19971188e+00,\n",
       "         -1.01239056e-01, -9.41750395e-01, -3.97158787e-01,\n",
       "         -3.10808904e-01, -6.02953170e-01, -4.28168458e-01,\n",
       "          9.05175841e-01],\n",
       "        [ 1.04036890e+00,  1.61862059e+00, -2.03780861e+00,\n",
       "          8.36031483e-01,  1.03507861e+00, -7.75807986e-02,\n",
       "          1.81280888e+00,  1.17958940e+00, -1.81837713e+00,\n",
       "         -5.33075234e-01],\n",
       "        [-1.41247189e-01, -8.28735345e-01, -2.26197353e-01,\n",
       "         -1.52453774e+00, -1.23597272e+00, -4.45316421e-01,\n",
       "          5.10286976e-01,  8.53977294e-01, -5.29750651e-01,\n",
       "          1.14471866e+00],\n",
       "        [ 4.69829537e-01,  1.32938200e+00,  4.45450891e-01,\n",
       "          3.58323569e-01, -4.54531962e-02,  1.52237743e+00,\n",
       "          4.10901975e-01, -4.86270419e-01,  7.25138499e-01,\n",
       "         -4.52516378e-01],\n",
       "        [ 1.40126006e+00,  3.76422340e-01,  2.47411615e-01,\n",
       "         -4.90625305e-01,  1.61317479e-01,  2.53297359e-01,\n",
       "         -7.83644802e-01, -7.52161370e-01, -1.63899462e+00,\n",
       "         -4.81494756e-01],\n",
       "        [ 1.00278682e+00, -6.41340289e-01,  6.65316532e-02,\n",
       "         -3.77746225e-02,  1.30384576e-02, -9.61765238e-01,\n",
       "         -6.86834009e-01,  9.68523685e-04,  2.04085109e-01,\n",
       "         -1.54076587e+00],\n",
       "        [ 1.48280920e+00,  8.12092345e-01, -1.58846706e+00,\n",
       "         -6.40630066e-01, -1.11209874e+00, -5.55324563e-01,\n",
       "         -2.48410828e+00, -3.64498036e-01, -2.81448459e-02,\n",
       "          1.33728896e+00]]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c196efe5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T05:46:25.356020Z",
     "start_time": "2021-09-04T05:46:25.340487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.56220745e+00,  1.16128308e+00,  1.63760832e+00,\n",
       "         2.96158896e-01, -8.17756401e-01, -1.67014857e+00,\n",
       "         1.03387222e-01,  2.17667236e+00, -7.64133954e-01,\n",
       "         1.81420526e-01],\n",
       "       [-9.93015986e-01,  1.49699094e-01, -9.12337878e-01,\n",
       "         2.91129530e-02, -1.07610624e+00,  6.41171191e-01,\n",
       "        -5.43958181e-01,  8.12922358e-01,  2.20467764e+00,\n",
       "        -1.16921995e+00],\n",
       "       [-2.71669158e-02, -1.18990355e+00,  2.36143696e-01,\n",
       "         5.46101206e-01,  9.08103025e-01, -3.62806788e-01,\n",
       "        -3.21140114e-01,  9.72338864e-01, -1.32779712e+00,\n",
       "         8.78520128e-01],\n",
       "       [-3.21284495e-01, -6.96878431e-01,  8.75365772e-02,\n",
       "        -1.73310226e+00,  6.94339617e-01, -9.47260516e-01,\n",
       "        -5.02968114e-01, -6.33577733e-02,  4.04136811e-01,\n",
       "        -1.10527904e+00],\n",
       "       [-1.40213789e+00,  2.88220324e+00, -1.85334288e+00,\n",
       "        -5.86368538e-01, -9.51082154e-02, -1.31180448e-01,\n",
       "         8.34215738e-01, -1.33044563e+00,  1.15077516e+00,\n",
       "         4.17546983e-01],\n",
       "       [ 1.10610016e+00, -8.14659127e-01,  8.76271941e-01,\n",
       "        -6.84996957e-01,  7.31081947e-02,  1.51777901e-01,\n",
       "         2.42628345e-01, -9.34466792e-01, -1.21572644e+00,\n",
       "        -1.25667599e+00],\n",
       "       [-9.06244723e-01,  1.35359320e+00,  1.63996487e+00,\n",
       "         2.97101150e-02,  1.43679633e+00,  5.63860119e-01,\n",
       "         1.23016454e+00,  6.53908831e-01, -2.46708226e-01,\n",
       "         1.64169182e+00],\n",
       "       [ 7.11499383e-01, -7.89459180e-01,  2.27910544e-01,\n",
       "         5.07707763e-01, -3.32653533e-02, -2.79739690e-01,\n",
       "        -8.43604124e-01, -1.55642916e+00,  1.10471494e+00,\n",
       "        -9.88275715e-01],\n",
       "       [ 1.37032251e-01, -2.55756011e-01,  2.28514672e+00,\n",
       "         3.95353507e-01, -6.07453684e-01,  1.70602052e+00,\n",
       "         1.08847556e+00, -8.81761186e-02, -6.88262215e-01,\n",
       "        -1.63209430e+00],\n",
       "       [ 1.75726614e+00, -4.75421775e-01,  1.15425253e+00,\n",
       "         1.11356982e+00, -2.42769979e+00, -9.30114350e-01,\n",
       "        -3.01303468e-02, -8.64516400e-02, -1.31601779e+00,\n",
       "         2.49646069e+00],\n",
       "       [ 5.12252204e-01,  7.86923041e-01, -1.77489252e+00,\n",
       "         3.02414108e-01,  1.14922940e+00, -1.55552499e+00,\n",
       "        -6.80084168e-01, -2.60802412e-01, -5.21039083e-01,\n",
       "        -1.06821477e+00],\n",
       "       [ 1.35376300e-01, -4.29214818e-01,  7.37524641e-01,\n",
       "        -2.97448418e-02,  9.51461465e-01,  2.16780621e+00,\n",
       "        -1.56472487e-01,  4.07491013e-01,  3.00091959e-01,\n",
       "         2.27032927e+00],\n",
       "       [-8.68129597e-01,  2.02023617e+00,  1.42061933e+00,\n",
       "        -3.00549283e-01, -5.39892612e-01,  1.87183993e-01,\n",
       "         2.26677293e+00,  1.40964335e+00,  1.93067384e-01,\n",
       "        -4.94054657e-01],\n",
       "       [-4.33627609e-01, -1.17713418e+00, -2.85254224e-01,\n",
       "         1.44829446e+00, -1.80067162e+00,  1.95569796e+00,\n",
       "         7.61721800e-01, -2.89730005e+00,  7.43395027e-01,\n",
       "        -7.22439550e-01],\n",
       "       [ 9.54568860e-01, -3.50390499e-01, -9.67757309e-01,\n",
       "         2.94363045e-01,  9.70483222e-01,  6.41064891e-01,\n",
       "        -1.18238103e+00,  1.35588004e+00, -2.07713434e+00,\n",
       "         9.23868845e-01],\n",
       "       [ 1.03486144e+00,  4.55698878e-01,  1.93912432e+00,\n",
       "         6.44417187e-01, -6.15178892e-01, -3.49037767e-01,\n",
       "         5.33428943e-01,  3.11915210e-01,  1.71732257e-01,\n",
       "        -5.04941447e-01],\n",
       "       [ 6.07001948e-01, -1.59303417e-01, -1.93110631e+00,\n",
       "         5.13526951e-01, -5.28458346e-01,  3.45138489e-01,\n",
       "        -1.62163367e+00,  5.33742177e-01,  2.08406384e+00,\n",
       "         4.05245061e-01],\n",
       "       [-2.50683018e-01, -3.31664593e-01, -1.08705295e+00,\n",
       "        -8.44844019e-01,  9.66060511e-01,  3.08713915e-01,\n",
       "        -8.73208401e-01, -8.41692844e-01,  7.19322137e-02,\n",
       "        -1.55226658e+00],\n",
       "       [ 7.48780799e-01,  7.98457331e-02, -2.59307163e-02,\n",
       "        -2.14803051e-01,  1.04499147e+00, -6.36728946e-01,\n",
       "        -1.81561826e-01, -1.14037444e+00, -1.04161411e+00,\n",
       "        -1.29849409e-01],\n",
       "       [-8.25641967e-01,  3.07698481e-02, -1.63144430e+00,\n",
       "        -6.64604030e-01, -3.69848881e-01,  6.72654926e-01,\n",
       "        -7.33104142e-01, -4.67649844e-02,  1.05798341e+00,\n",
       "        -7.70800252e-01],\n",
       "       [-1.19074177e+00,  1.63965315e+00,  7.58333885e-02,\n",
       "         1.61260304e+00,  8.99601623e-02, -5.94675668e-01,\n",
       "         2.02169570e-02,  1.07468695e+00,  6.52223441e-02,\n",
       "         1.51750607e+00],\n",
       "       [-2.42429450e+00, -3.32400840e-01, -7.91700195e-01,\n",
       "        -3.39048392e-02, -3.71355254e-02, -2.18627757e-01,\n",
       "         1.89163156e+00,  5.30988527e-01,  1.29116746e+00,\n",
       "         9.94225936e-02],\n",
       "       [ 7.59769206e-01, -1.39195766e+00,  1.54922005e-01,\n",
       "         6.51897742e-01, -8.30390267e-01,  2.95144664e-01,\n",
       "         3.28369186e-01,  3.62253364e-01,  4.17892943e-01,\n",
       "        -4.44151425e-01],\n",
       "       [ 1.10877399e+00,  1.94536862e+00,  8.25519172e-01,\n",
       "         5.40225091e-01, -1.11143337e-01, -1.03190167e-01,\n",
       "         1.67361977e+00, -1.12294596e+00, -2.48920733e-01,\n",
       "         2.77866860e-01],\n",
       "       [ 6.83701667e-02,  5.41949285e-01,  7.71497278e-01,\n",
       "         1.46679257e+00,  8.17146751e-01, -1.78834554e-01,\n",
       "         1.61697541e+00,  1.71275022e-01,  2.47022161e-01,\n",
       "        -1.04214380e+00],\n",
       "       [-4.16261245e-02, -4.67885426e-01,  4.52790600e-01,\n",
       "        -2.28696647e-01,  1.46318658e+00,  1.28846738e+00,\n",
       "         1.62914539e+00,  9.25122538e-01, -7.40099276e-01,\n",
       "        -1.99840623e-01],\n",
       "       [-7.44235788e-01,  2.66600937e-03,  5.37140988e-01,\n",
       "         1.14000024e+00, -9.34047681e-01,  1.39472852e-01,\n",
       "        -9.04280660e-02,  1.51859045e-01,  1.30657545e+00,\n",
       "         7.19390844e-01],\n",
       "       [ 2.72692749e-01, -1.14079459e+00, -3.15242534e-01,\n",
       "        -6.07245807e-01,  1.68144753e+00,  1.31296913e+00,\n",
       "        -1.95059071e+00,  2.24604669e+00, -8.86778884e-01,\n",
       "         1.20744180e+00],\n",
       "       [-1.08290839e+00,  3.78978870e-01,  8.25039120e-02,\n",
       "         6.74626190e-01,  1.59963149e-02,  3.50279790e-01,\n",
       "         1.36162569e+00, -3.26970537e-01,  1.06193180e+00,\n",
       "         6.79074258e-01],\n",
       "       [ 1.13274203e+00,  1.29317240e+00, -1.08963955e+00,\n",
       "         6.80823600e-01,  8.21221070e-01,  4.78016276e-01,\n",
       "        -1.12996635e-01, -7.28287943e-01, -4.32821212e-01,\n",
       "        -4.89944665e-02],\n",
       "       [ 9.84982192e-01,  7.02560309e-01, -1.32828824e+00,\n",
       "        -1.47190177e+00,  4.94702363e-01, -4.10113588e-01,\n",
       "        -1.13108590e+00,  7.38325943e-01,  2.64937407e+00,\n",
       "         1.27581035e-01],\n",
       "       [ 1.24822811e+00,  1.52533538e+00, -4.23542788e-02,\n",
       "        -4.81631160e-01, -7.54057165e-01, -5.83528226e-01,\n",
       "         9.60587011e-01,  7.12432002e-01,  8.49424944e-01,\n",
       "         7.93693921e-01],\n",
       "       [-2.63249454e-01,  1.08893622e+00,  3.28517443e-01,\n",
       "         1.16770374e+00, -7.31794846e-01, -7.03288878e-01,\n",
       "         9.65807736e-01,  8.61388372e-02,  1.11689326e+00,\n",
       "        -3.48188785e-01],\n",
       "       [-5.40556528e-01,  4.09888507e-01,  1.95980013e+00,\n",
       "        -6.33553733e-01, -8.97745397e-01,  2.80838831e-01,\n",
       "         3.58349237e-01, -7.00782952e-01,  1.80223045e-01,\n",
       "        -1.09895291e+00],\n",
       "       [ 3.59594803e-01,  1.00059477e+00, -2.03423701e-01,\n",
       "        -4.39075061e-01, -1.08610370e+00, -1.64295667e+00,\n",
       "         1.56774440e+00,  1.42020896e+00,  9.41129893e-01,\n",
       "         1.08375395e+00],\n",
       "       [-4.44134230e-01, -3.66912406e-01, -4.44728470e-01,\n",
       "         1.16134472e+00, -2.14661506e+00, -3.87316096e-02,\n",
       "        -3.52316199e-02,  4.23744966e-01,  2.52152711e+00,\n",
       "        -1.41530843e+00],\n",
       "       [-5.39397170e-01, -8.67132636e-01, -1.07267154e+00,\n",
       "         6.07940752e-02, -5.44630338e-01,  8.62440817e-01,\n",
       "        -1.95121952e+00,  1.62084835e-01,  4.40413078e-01,\n",
       "         1.38468852e+00],\n",
       "       [-1.07376961e+00, -2.99967804e+00, -1.45781685e+00,\n",
       "        -7.61432889e-01, -8.04301090e-01, -2.70377205e-01,\n",
       "        -4.79359143e-01,  8.30867771e-02, -3.52036359e-01,\n",
       "         3.75916698e-01],\n",
       "       [ 1.54794205e+00,  1.13590843e+00, -2.49960435e+00,\n",
       "         3.93674163e-01, -1.32281500e+00,  2.11818236e-01,\n",
       "         2.49217866e-02, -1.33140250e+00,  1.98330212e+00,\n",
       "         3.62999670e-01],\n",
       "       [ 6.95171803e-01,  2.08101641e-01, -3.63536041e-01,\n",
       "        -2.36785486e-01,  2.27981198e+00,  1.78905010e-01,\n",
       "        -1.28714250e-02, -1.54286394e+00,  9.36094322e-02,\n",
       "         1.66206523e+00],\n",
       "       [ 4.37787480e-01,  3.09070252e-01, -5.24567821e-01,\n",
       "        -1.42938722e+00, -3.92367539e-01,  5.88456935e-01,\n",
       "        -2.20000846e-01,  2.76620226e-01,  1.72539332e+00,\n",
       "         2.26820107e-01],\n",
       "       [ 5.92599885e-01,  4.54045224e-01,  1.75569042e+00,\n",
       "        -1.12401639e+00, -1.00744134e+00,  2.82820239e-01,\n",
       "         9.20174797e-01, -3.40495751e-01, -9.80680813e-02,\n",
       "        -1.07346619e-02],\n",
       "       [ 4.13310249e-01,  1.96788811e-01, -1.69906046e+00,\n",
       "        -4.82286390e-01,  1.09029674e-03, -7.46214941e-01,\n",
       "         9.59343741e-01, -6.16806344e-01, -3.80410791e-01,\n",
       "        -3.29042404e-01],\n",
       "       [-1.80151553e+00,  1.38711337e-01,  2.02810200e+00,\n",
       "        -1.02162740e-02, -2.57086772e-01,  3.62563144e-01,\n",
       "        -2.90620445e-01,  1.06884418e+00,  5.89292190e-01,\n",
       "         6.14835806e-01],\n",
       "       [ 3.64160240e-01, -1.29015770e+00, -2.01399276e-01,\n",
       "        -1.49648375e+00, -1.44368131e+00, -1.98750215e+00,\n",
       "        -1.49645705e-01,  1.89195918e+00,  6.91382439e-01,\n",
       "         1.44510375e+00],\n",
       "       [ 3.20155122e-01, -2.13728806e-01, -2.27784815e-01,\n",
       "        -4.43260240e-01,  1.55970643e-01,  1.92280941e+00,\n",
       "        -3.41288050e-01, -5.49054277e-01, -6.87706263e-01,\n",
       "         9.83595063e-01],\n",
       "       [ 3.05585412e-01,  2.72401580e-01, -7.63018405e-01,\n",
       "        -5.33017156e-01,  5.59328840e-01,  6.93132572e-01,\n",
       "         7.32078410e-01, -9.86225262e-01,  1.95427326e-01,\n",
       "         5.14845727e-01],\n",
       "       [-2.92444418e-01,  1.58191169e+00, -9.66900415e-01,\n",
       "         1.20949034e+00,  1.15420219e+00,  8.32476562e-01,\n",
       "         2.17953026e+00, -1.07002371e-01, -9.16149037e-01,\n",
       "         3.76953096e-01],\n",
       "       [-1.29232689e+00, -4.31481443e-01, -4.85835661e-01,\n",
       "        -5.73117954e-01, -1.51081479e+00, -3.30890664e-01,\n",
       "         1.62991956e+00, -3.76939479e-01,  5.63476790e-01,\n",
       "        -1.28406098e+00],\n",
       "       [ 1.19283485e+00,  8.54753227e-01,  1.58701614e-01,\n",
       "         7.49319944e-01,  3.92699702e-01,  1.76609335e+00,\n",
       "         6.70318036e-01,  1.85923291e-02, -3.64965460e-01,\n",
       "         3.36857070e-01],\n",
       "       [ 1.45782610e+00, -9.47534310e-01,  4.21565707e-01,\n",
       "        -3.46324432e-01,  3.74643713e-01,  2.60180091e-01,\n",
       "        -4.58994955e-01,  1.10602780e+00,  1.01763962e+00,\n",
       "        -8.18051499e-01],\n",
       "       [-9.63306104e-01, -5.00105699e-01, -3.92369193e-02,\n",
       "        -8.90330049e-01,  3.45081964e-02,  2.68511256e-01,\n",
       "        -2.60139363e-01,  2.67174426e-01, -1.24937384e+00,\n",
       "         4.38718004e-01],\n",
       "       [-1.69889839e+00,  1.73331040e+00,  1.68614998e+00,\n",
       "        -3.07629364e-03,  6.14494921e-01,  5.93092862e-01,\n",
       "        -3.29774169e-01,  4.90975904e-01, -1.13190033e+00,\n",
       "         5.67563377e-01],\n",
       "       [ 1.30160032e+00, -5.65170162e-01,  6.68734438e-01,\n",
       "        -5.51140467e-01, -2.02635906e-01,  1.59806702e-01,\n",
       "        -8.86138619e-01, -7.74265083e-01,  1.03072398e-02,\n",
       "         1.03794687e+00],\n",
       "       [-5.84912349e-01,  4.61756980e-01,  2.12596445e-01,\n",
       "        -2.36564885e-01,  9.68939899e-01,  4.44753916e-01,\n",
       "         9.74599908e-01,  6.87964922e-01, -1.88114561e-01,\n",
       "        -6.79282993e-01],\n",
       "       [-3.73827419e-01, -1.64814483e+00, -7.18294714e-02,\n",
       "        -9.63778900e-01,  8.15903268e-01,  1.21687373e+00,\n",
       "         7.37950859e-01, -7.58342959e-01,  2.90624414e-01,\n",
       "        -1.39411776e+00],\n",
       "       [ 3.68413088e-01,  5.90614503e-01,  8.17228039e-02,\n",
       "        -9.95575166e-02, -1.17620562e+00,  1.24911719e+00,\n",
       "         1.22583365e+00,  2.27126626e-01,  1.10034196e+00,\n",
       "         2.35081994e-01],\n",
       "       [-8.18081391e-01, -1.17223696e+00, -1.19967844e+00,\n",
       "        -1.01319327e-01, -9.41863428e-01, -3.97145975e-01,\n",
       "        -3.10789527e-01, -6.02885425e-01, -4.28128747e-01,\n",
       "         9.05156012e-01],\n",
       "       [ 1.04030590e+00,  1.61856329e+00, -2.03781443e+00,\n",
       "         8.36140650e-01,  1.03520720e+00, -7.75709981e-02,\n",
       "         1.81276947e+00,  1.17950444e+00, -1.81842927e+00,\n",
       "        -5.33048641e-01],\n",
       "       [-1.41182397e-01, -8.28691800e-01, -2.26049807e-01,\n",
       "        -1.52486541e+00, -1.23617803e+00, -4.45373937e-01,\n",
       "         5.10327078e-01,  8.54279839e-01, -5.29657237e-01,\n",
       "         1.14462336e+00],\n",
       "       [ 4.69826808e-01,  1.32938758e+00,  4.45490470e-01,\n",
       "         3.58286920e-01, -4.54791023e-02,  1.52238758e+00,\n",
       "         4.10907569e-01, -4.86218073e-01,  7.25190378e-01,\n",
       "        -4.52528220e-01],\n",
       "       [ 1.40125689e+00,  3.76406269e-01,  2.47412037e-01,\n",
       "        -4.90610192e-01,  1.61326224e-01,  2.53310600e-01,\n",
       "        -7.83632240e-01, -7.52202473e-01, -1.63899360e+00,\n",
       "        -4.81463753e-01],\n",
       "       [ 1.00278986e+00, -6.41329033e-01,  6.65069821e-02,\n",
       "        -3.77509755e-02,  1.30556295e-02, -9.61776265e-01,\n",
       "        -6.86834393e-01,  9.52920270e-04,  2.04041716e-01,\n",
       "        -1.54076239e+00],\n",
       "       [ 1.48280550e+00,  8.12110636e-01, -1.58851275e+00,\n",
       "        -6.40559648e-01, -1.11206486e+00, -5.55351102e-01,\n",
       "        -2.48409895e+00, -3.64544787e-01, -2.81893292e-02,\n",
       "         1.33730526e+00]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = x.dot(w1) #(N, H)\n",
    "a = np.maximum(0, h) #(N, H)\n",
    "y_pred = a.dot(w2) #(N, D_out)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "acd3761c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T05:46:45.927854Z",
     "start_time": "2021-09-04T05:46:45.909476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.75403814e-05, -6.42677195e-05, -1.26061634e-04,\n",
       "         1.79103959e-04,  2.10783536e-04, -1.02045501e-05,\n",
       "        -6.74961873e-05, -2.30505834e-04, -8.76564272e-05,\n",
       "         8.03360131e-05],\n",
       "       [ 4.34688665e-05,  3.35821021e-05,  3.71561257e-05,\n",
       "        -4.92421862e-05, -9.25840883e-05, -1.35425055e-05,\n",
       "         2.24847498e-05,  5.69991850e-05,  2.36199400e-05,\n",
       "        -3.34131937e-05],\n",
       "       [-9.48356660e-05, -1.40528637e-04, -2.42666334e-04,\n",
       "         3.94378462e-04,  3.02359820e-04,  2.73411445e-05,\n",
       "        -3.55557969e-05, -2.55823105e-04, -2.82688481e-04,\n",
       "         1.45582983e-04],\n",
       "       [ 2.95272442e-05,  1.15565219e-06,  4.85415788e-05,\n",
       "        -7.92337177e-05, -7.31627885e-05, -9.55519929e-07,\n",
       "         2.89932396e-05,  5.80176964e-05,  5.93275081e-05,\n",
       "        -1.27924753e-05],\n",
       "       [-6.13807310e-05, -4.21208723e-06, -8.40015598e-05,\n",
       "         2.09110603e-04,  1.84328005e-04, -9.25678353e-06,\n",
       "        -3.66533000e-05, -1.16154860e-04, -9.96363547e-05,\n",
       "         1.15630847e-05],\n",
       "       [ 1.78998849e-05,  3.41117890e-05,  5.06757810e-06,\n",
       "        -1.86158635e-04, -1.18619300e-04,  5.32413641e-06,\n",
       "        -1.48508369e-05,  9.40468316e-05,  6.71856060e-05,\n",
       "        -2.84978283e-05],\n",
       "       [ 5.99528614e-05,  4.63149834e-05,  3.60100842e-05,\n",
       "        -1.26626415e-04, -1.19338158e-04, -3.86034774e-05,\n",
       "         2.32686848e-05,  1.42547439e-04,  5.99280423e-05,\n",
       "        -7.64746321e-05],\n",
       "       [ 9.99333195e-06,  7.82619461e-06,  3.54170607e-05,\n",
       "        -4.29989187e-05, -2.86619307e-05,  4.65811007e-06,\n",
       "         1.20273412e-05,  1.88785585e-05,  4.11137527e-05,\n",
       "        -6.74275643e-07],\n",
       "       [-2.44483749e-05, -2.62572427e-05, -3.42235878e-05,\n",
       "         5.47105859e-05,  4.58481421e-05,  1.22559476e-05,\n",
       "        -1.31156130e-05, -6.16390747e-05, -2.14211018e-05,\n",
       "         2.79729158e-05],\n",
       "       [-8.42960662e-06, -8.52217498e-06, -8.98296289e-05,\n",
       "         1.48404681e-04,  1.12126193e-04, -2.30826755e-05,\n",
       "        -2.74189259e-05, -8.80736978e-05, -1.01957896e-04,\n",
       "         3.00007860e-06],\n",
       "       [ 4.99048919e-05,  3.55030220e-05,  6.09786449e-05,\n",
       "        -1.25383499e-04, -1.35604240e-04,  1.67164216e-05,\n",
       "         4.41040362e-05,  6.28852956e-05,  5.57599351e-05,\n",
       "        -4.52133734e-05],\n",
       "       [-1.39585342e-05,  8.00025415e-06, -4.44212444e-06,\n",
       "         7.03887344e-05,  1.83157150e-05, -4.91293104e-06,\n",
       "         1.12376093e-06, -6.26556110e-06, -2.24892011e-05,\n",
       "        -1.81046991e-06],\n",
       "       [ 4.86034522e-05,  8.08059911e-06,  1.88971715e-04,\n",
       "        -2.50663644e-04, -3.04342208e-04,  5.17474800e-05,\n",
       "         4.70386904e-05,  2.65468564e-04,  2.33673090e-04,\n",
       "        -4.93348197e-05],\n",
       "       [-1.12796267e-05, -1.73358540e-05,  2.14611137e-05,\n",
       "        -1.71318055e-05, -2.09093216e-05,  2.35757343e-05,\n",
       "         2.20271162e-06,  4.96820194e-06,  7.24445243e-05,\n",
       "        -1.71190275e-06],\n",
       "       [ 1.63830170e-05,  2.77685724e-05, -1.96782997e-05,\n",
       "        -3.12480382e-05, -2.92203565e-05, -1.38037087e-05,\n",
       "        -8.63781260e-06,  3.31997355e-05, -1.72755706e-05,\n",
       "        -1.50930597e-05],\n",
       "       [-6.75830871e-05, -7.01746861e-05,  3.09314601e-05,\n",
       "         2.14855077e-05,  1.52889524e-04,  2.79192822e-05,\n",
       "        -1.06860756e-04, -7.06455296e-05,  1.42027479e-05,\n",
       "         1.56109716e-05],\n",
       "       [-1.02465523e-05, -6.99803738e-07, -3.72194990e-05,\n",
       "         2.53010990e-05,  2.47224398e-05, -2.21927649e-06,\n",
       "        -1.39319850e-05, -3.08394975e-05, -2.92749492e-05,\n",
       "         1.48846688e-05],\n",
       "       [ 1.60510333e-05,  7.24113672e-06,  4.18494025e-05,\n",
       "        -3.62088930e-05, -4.24031475e-05, -2.45511833e-05,\n",
       "         5.01412630e-06,  7.40088052e-05,  1.68611003e-05,\n",
       "        -1.75371973e-05],\n",
       "       [ 3.92304223e-05, -2.64332071e-06,  4.38583025e-05,\n",
       "        -1.04218770e-04, -9.64978847e-05,  1.84309444e-05,\n",
       "         3.01137662e-05,  4.99313495e-05,  1.00889742e-04,\n",
       "        -1.46645469e-05],\n",
       "       [-6.14043181e-06, -1.47250314e-06,  3.41908850e-05,\n",
       "         4.88528937e-05, -1.67109120e-05, -1.18533150e-05,\n",
       "         1.27363517e-05,  4.45176104e-05,  1.59580815e-05,\n",
       "         5.80050351e-06],\n",
       "       [-7.14634171e-05,  1.63654569e-05, -1.89930322e-04,\n",
       "         4.41049771e-04,  2.67054787e-04,  1.85373694e-05,\n",
       "        -4.21781321e-05, -3.36434379e-04, -1.89380217e-04,\n",
       "         7.53933219e-05],\n",
       "       [ 8.42102798e-05, -6.38678414e-05,  9.45278444e-05,\n",
       "         3.31294434e-05,  9.55439315e-06, -1.50631410e-05,\n",
       "         7.14627458e-05, -7.02003340e-06,  4.97841963e-05,\n",
       "        -1.73385322e-05],\n",
       "       [ 2.07737406e-05,  9.86582607e-06,  5.87883196e-05,\n",
       "        -1.10671296e-04, -6.59562108e-05,  9.37791452e-06,\n",
       "         2.88088326e-06,  8.18135992e-05,  4.44988837e-05,\n",
       "        -5.23368699e-05],\n",
       "       [ 2.28338934e-05, -1.97100135e-05,  1.40735496e-04,\n",
       "        -1.01381866e-04, -1.73901212e-04,  2.03647389e-05,\n",
       "         9.75177366e-05,  1.47194847e-04,  2.02833660e-04,\n",
       "        -2.77470317e-05],\n",
       "       [ 3.08536503e-06, -2.74818185e-06, -1.20341991e-05,\n",
       "         2.24236888e-05,  8.92867246e-06, -6.01905789e-07,\n",
       "         8.54360873e-06, -1.56155141e-05,  5.66110513e-06,\n",
       "        -8.45887742e-06],\n",
       "       [-3.14394579e-05,  2.13872748e-07, -4.01717416e-05,\n",
       "         4.63565579e-05,  8.06602112e-05,  7.73151814e-06,\n",
       "        -4.27888244e-05, -6.72760708e-05, -3.82179554e-05,\n",
       "         3.04478826e-05],\n",
       "       [ 4.90334096e-05,  2.04911874e-05,  1.10062100e-04,\n",
       "        -2.64596926e-04, -2.43208378e-04,  6.40085813e-05,\n",
       "         8.89842284e-05,  8.70253838e-05,  2.40161601e-04,\n",
       "         5.84375256e-07],\n",
       "       [ 4.11120918e-05,  5.37273928e-05,  1.13336295e-04,\n",
       "        -1.34240126e-04, -1.09558586e-04,  1.70666088e-05,\n",
       "         7.93493939e-06,  1.33908971e-04,  8.09694995e-05,\n",
       "        -4.55545350e-05],\n",
       "       [-1.58751548e-06,  1.55596880e-05, -1.63831334e-05,\n",
       "         2.40634479e-05,  4.10221188e-05, -6.94886754e-06,\n",
       "        -1.70375579e-05, -2.21000151e-05, -3.00785213e-05,\n",
       "        -6.94919884e-06],\n",
       "       [ 1.31816745e-05,  2.68795106e-05,  3.20058642e-05,\n",
       "        -5.09110810e-05, -3.87962908e-05,  4.07980152e-06,\n",
       "        -1.55210231e-05,  6.91820239e-05,  1.08381707e-05,\n",
       "        -2.74527148e-05],\n",
       "       [-4.91014288e-06,  2.00585189e-05, -5.10348478e-05,\n",
       "         3.73210620e-05,  4.28606462e-05, -1.30082446e-05,\n",
       "        -1.33599884e-06, -3.80169417e-05, -6.35705667e-05,\n",
       "        -7.05530525e-07],\n",
       "       [-3.15114823e-06,  1.59466489e-05, -5.22987198e-06,\n",
       "        -6.51570024e-05, -2.80302039e-05, -5.25280620e-07,\n",
       "        -1.43752262e-05,  3.43448172e-05,  1.30766274e-05,\n",
       "        -2.23164247e-05],\n",
       "       [-1.85034138e-04, -7.37858819e-05, -2.73213171e-04,\n",
       "         3.10658840e-04,  3.60118164e-04,  2.48542241e-05,\n",
       "        -7.94832307e-05, -4.58130308e-04, -1.80246387e-04,\n",
       "         1.03956796e-04],\n",
       "       [-1.72259122e-05,  1.58918260e-05,  2.83415047e-05,\n",
       "        -1.56702652e-04, -6.94808783e-05, -3.93381120e-05,\n",
       "        -2.06561168e-05,  1.32465419e-04,  9.83050195e-06,\n",
       "        -4.04413466e-05],\n",
       "       [ 4.48749196e-05, -6.62931652e-06,  8.52826457e-05,\n",
       "        -1.40151391e-04, -9.47221502e-05,  2.84331728e-07,\n",
       "         4.25906944e-05,  1.11697057e-04,  9.11230448e-05,\n",
       "        -2.18431204e-05],\n",
       "       [-5.98522047e-06, -1.45809257e-05,  2.04299088e-05,\n",
       "         1.71613378e-05,  1.95993507e-05,  5.63110850e-06,\n",
       "        -3.65003040e-06,  3.53556043e-06,  6.52321528e-06,\n",
       "         4.33802260e-06],\n",
       "       [-5.84473131e-06,  4.09079866e-06, -4.72281506e-05,\n",
       "         6.50233206e-05,  4.70707019e-05, -1.36276322e-05,\n",
       "        -1.38362317e-05, -4.43223218e-05, -5.33780900e-05,\n",
       "         1.08268580e-05],\n",
       "       [-1.52981037e-05, -3.28518518e-05,  1.92634672e-05,\n",
       "         3.21794194e-05,  4.19083103e-05,  2.51095808e-05,\n",
       "         5.84386869e-06, -2.55966101e-05, -9.69381530e-06,\n",
       "         2.59057739e-05],\n",
       "       [ 4.34493281e-05,  2.35691069e-05,  1.08157879e-04,\n",
       "        -9.53888811e-05, -1.21822401e-04, -1.85189787e-05,\n",
       "         2.50187442e-05,  1.57201617e-04,  1.20226216e-04,\n",
       "        -6.55737837e-05],\n",
       "       [-1.53222853e-05,  2.08013533e-05, -6.08686781e-05,\n",
       "         5.57505089e-05,  7.43288467e-05, -8.86365598e-06,\n",
       "        -8.46843021e-07, -1.01161290e-04, -7.66528767e-05,\n",
       "         2.88756590e-05],\n",
       "       [-5.48026901e-05, -1.89142334e-05, -1.31635831e-04,\n",
       "         2.15479551e-04,  1.36971875e-04,  7.49964195e-06,\n",
       "        -3.08719666e-05, -1.63262923e-04, -8.33218740e-05,\n",
       "         5.98955253e-05],\n",
       "       [ 2.67277017e-05,  3.25543235e-05,  3.72575208e-05,\n",
       "        -5.24763180e-05, -8.97105270e-05, -3.79608047e-06,\n",
       "         3.21399152e-05,  5.82502979e-05,  4.64727745e-05,\n",
       "        -1.12812896e-05],\n",
       "       [ 6.99651353e-06,  3.05719981e-05, -8.20929838e-05,\n",
       "         4.45448729e-05,  7.53099704e-05, -1.35447739e-05,\n",
       "        -3.98962848e-05, -6.72988493e-05, -1.43383942e-04,\n",
       "         1.54877760e-05],\n",
       "       [-3.00449135e-05, -1.96975036e-05, -4.51620501e-05,\n",
       "         6.91298622e-05,  8.03274716e-05,  2.74892051e-06,\n",
       "        -3.46520865e-05, -7.40605324e-05, -6.03281714e-05,\n",
       "         2.04313558e-05],\n",
       "       [-1.74351455e-05, -1.63384292e-05, -2.02051948e-05,\n",
       "         1.49004266e-05,  6.09029438e-05,  5.59238160e-06,\n",
       "        -2.37606021e-05, -4.94126926e-05, -2.88004607e-05,\n",
       "         2.82395376e-05],\n",
       "       [ 4.79523963e-06, -2.05905784e-05,  4.63988009e-05,\n",
       "        -2.11580303e-05, -1.57006806e-05,  4.14072584e-06,\n",
       "         1.09429589e-05,  3.18885423e-05,  2.01510377e-05,\n",
       "         1.46185750e-06],\n",
       "       [ 4.77699707e-05,  1.92316141e-05,  2.79875503e-05,\n",
       "        -6.92407552e-05, -8.01264437e-05, -3.53226509e-05,\n",
       "         1.78611013e-05,  8.91952219e-05,  1.78505570e-05,\n",
       "        -4.76771323e-05],\n",
       "       [ 6.96564039e-05,  6.65760396e-05, -4.00157123e-05,\n",
       "         1.42214384e-05, -5.86258560e-05, -3.70319085e-05,\n",
       "         5.65796068e-05,  1.71115809e-05, -7.26048139e-05,\n",
       "        -1.62017209e-05],\n",
       "       [ 6.57340126e-06,  3.36176354e-05,  1.52407597e-05,\n",
       "         2.68461488e-05, -1.60398261e-05, -2.88456566e-05,\n",
       "         1.31142456e-05,  3.12628488e-05,  5.88934612e-06,\n",
       "        -1.72038514e-05],\n",
       "       [ 2.41509029e-05, -4.26647211e-05,  1.96243963e-04,\n",
       "        -2.55364254e-04, -1.26707715e-04,  1.98845908e-05,\n",
       "        -4.92864133e-05,  2.39735573e-04,  1.31963420e-04,\n",
       "         7.55584199e-06],\n",
       "       [ 9.61886430e-06,  4.24442856e-06, -4.07147786e-05,\n",
       "         6.03643344e-07,  1.28335575e-05, -8.66961503e-06,\n",
       "        -6.79125678e-06, -3.13069195e-05, -4.04467595e-05,\n",
       "         5.24607989e-06],\n",
       "       [ 4.73909285e-06, -6.48758038e-06,  1.79056315e-05,\n",
       "        -1.65565129e-05, -1.11371474e-05, -4.61813178e-06,\n",
       "         5.98120296e-06,  2.75749174e-05,  1.46061997e-05,\n",
       "        -8.84521170e-06],\n",
       "       [-1.66612773e-05, -3.67088950e-05,  2.60825789e-05,\n",
       "         2.62906648e-05,  5.87621518e-05,  1.57102573e-05,\n",
       "        -3.09974775e-05, -2.28002336e-05,  1.97374647e-05,\n",
       "         7.91514733e-06],\n",
       "       [-3.70825523e-05, -2.79942800e-05, -7.58796775e-05,\n",
       "         1.32439768e-04,  1.30239393e-04, -7.86185540e-06,\n",
       "        -2.21550718e-05, -1.15438780e-04, -8.88266938e-05,\n",
       "         2.80735483e-05],\n",
       "       [-2.04672162e-05, -1.63380389e-05, -7.93746897e-05,\n",
       "         9.18045691e-05,  5.87069067e-05,  8.91744899e-06,\n",
       "         1.64537415e-05, -1.44024818e-04, -6.93710161e-05,\n",
       "         3.29235781e-05],\n",
       "       [-1.03547337e-05, -3.27694906e-07, -3.12213691e-05,\n",
       "         3.71114770e-05,  2.55805386e-05,  1.04484942e-06,\n",
       "         4.50659490e-06, -2.21086339e-05, -3.08961440e-05,\n",
       "         6.88627978e-06],\n",
       "       [ 1.63716936e-05,  1.31904982e-05, -9.80311933e-06,\n",
       "        -5.20333888e-05, -2.43568132e-05, -4.34420004e-06,\n",
       "        -6.38926942e-06,  2.55632902e-05, -9.77115884e-06,\n",
       "        -9.75812669e-06],\n",
       "       [ 1.66980515e-05,  2.12984428e-05,  3.34358360e-05,\n",
       "        -8.02704998e-05, -1.13033210e-04,  1.28116620e-05,\n",
       "         1.93763290e-05,  6.77455521e-05,  3.97114909e-05,\n",
       "        -1.98287703e-05],\n",
       "       [-6.30028692e-05, -5.73043003e-05, -5.82061534e-06,\n",
       "         1.09166896e-04,  1.28592580e-04,  9.80051038e-06,\n",
       "        -3.94179453e-05, -8.49585790e-05, -5.21439834e-05,\n",
       "         2.65932063e-05],\n",
       "       [ 6.47926104e-05,  4.35446390e-05,  1.47546261e-04,\n",
       "        -3.27668287e-04, -2.05313802e-04, -5.75155797e-05,\n",
       "         4.01023463e-05,  3.02545329e-04,  9.34144963e-05,\n",
       "        -9.53078946e-05],\n",
       "       [-2.72857578e-06,  5.58528087e-06,  3.95788500e-05,\n",
       "        -3.66496270e-05, -2.59060146e-05,  1.01475531e-05,\n",
       "         5.59424605e-06,  5.23461041e-05,  5.18797573e-05,\n",
       "        -1.18417762e-05],\n",
       "       [-3.17385826e-06, -1.60702826e-05,  4.21976305e-07,\n",
       "         1.51136197e-05,  8.74545452e-06,  1.32418420e-05,\n",
       "         1.25621058e-05, -4.11031738e-05,  1.02762147e-06,\n",
       "         3.10032399e-05],\n",
       "       [ 3.03863822e-06,  1.12552508e-05, -2.46710270e-05,\n",
       "         2.36469801e-05,  1.71719591e-05, -1.10265085e-05,\n",
       "        -3.84092019e-07, -1.56034150e-05, -4.33935976e-05,\n",
       "         3.48271016e-06],\n",
       "       [-3.69626857e-06,  1.82915972e-05, -4.56899966e-05,\n",
       "         7.04175257e-05,  3.38796664e-05, -2.65388573e-05,\n",
       "         9.33208145e-06, -4.67506824e-05, -4.44832047e-05,\n",
       "         1.62976665e-05]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred - y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d163ed9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T05:48:02.648421Z",
     "start_time": "2021-09-04T05:48:02.644276Z"
    }
   },
   "source": [
    "简单两层神经网络，pytorch1，手动版\n",
    "---------\n",
    "- $h = W_1X$\n",
    "- $a = max(0, h)$\n",
    "- $y_{hat} = W_2a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5891959d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T05:52:52.317300Z",
     "start_time": "2021-09-04T05:52:49.740588Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in iteration  0  is  27734412.0\n",
      "loss in iteration  1  is  19798406.0\n",
      "loss in iteration  2  is  16866958.0\n",
      "loss in iteration  3  is  15804927.0\n",
      "loss in iteration  4  is  15264896.0\n",
      "loss in iteration  5  is  14327554.0\n",
      "loss in iteration  6  is  12723096.0\n",
      "loss in iteration  7  is  10463866.0\n",
      "loss in iteration  8  is  8040889.0\n",
      "loss in iteration  9  is  5810799.0\n",
      "loss in iteration  10  is  4052704.25\n",
      "loss in iteration  11  is  2775119.5\n",
      "loss in iteration  12  is  1908028.125\n",
      "loss in iteration  13  is  1333849.0\n",
      "loss in iteration  14  is  959365.0625\n",
      "loss in iteration  15  is  712753.6875\n",
      "loss in iteration  16  is  547858.5\n",
      "loss in iteration  17  is  434460.375\n",
      "loss in iteration  18  is  354083.59375\n",
      "loss in iteration  19  is  295229.25\n",
      "loss in iteration  20  is  250635.53125\n",
      "loss in iteration  21  is  215795.40625\n",
      "loss in iteration  22  is  187791.625\n",
      "loss in iteration  23  is  164820.265625\n",
      "loss in iteration  24  is  145623.765625\n",
      "loss in iteration  25  is  129350.703125\n",
      "loss in iteration  26  is  115386.40625\n",
      "loss in iteration  27  is  103300.2890625\n",
      "loss in iteration  28  is  92769.3359375\n",
      "loss in iteration  29  is  83536.3359375\n",
      "loss in iteration  30  is  75398.7578125\n",
      "loss in iteration  31  is  68204.15625\n",
      "loss in iteration  32  is  61813.359375\n",
      "loss in iteration  33  is  56118.78515625\n",
      "loss in iteration  34  is  51031.640625\n",
      "loss in iteration  35  is  46481.703125\n",
      "loss in iteration  36  is  42397.23828125\n",
      "loss in iteration  37  is  38724.5234375\n",
      "loss in iteration  38  is  35413.890625\n",
      "loss in iteration  39  is  32425.671875\n",
      "loss in iteration  40  is  29723.408203125\n",
      "loss in iteration  41  is  27275.8359375\n",
      "loss in iteration  42  is  25055.271484375\n",
      "loss in iteration  43  is  23038.291015625\n",
      "loss in iteration  44  is  21204.93359375\n",
      "loss in iteration  45  is  19535.837890625\n",
      "loss in iteration  46  is  18013.6015625\n",
      "loss in iteration  47  is  16634.2734375\n",
      "loss in iteration  48  is  15377.240234375\n",
      "loss in iteration  49  is  14227.1484375\n",
      "loss in iteration  50  is  13172.896484375\n",
      "loss in iteration  51  is  12205.36328125\n",
      "loss in iteration  52  is  11316.912109375\n",
      "loss in iteration  53  is  10499.94140625\n",
      "loss in iteration  54  is  9748.09375\n",
      "loss in iteration  55  is  9055.2119140625\n",
      "loss in iteration  56  is  8416.68359375\n",
      "loss in iteration  57  is  7827.3974609375\n",
      "loss in iteration  58  is  7283.07421875\n",
      "loss in iteration  59  is  6780.01611328125\n",
      "loss in iteration  60  is  6314.89990234375\n",
      "loss in iteration  61  is  5884.541015625\n",
      "loss in iteration  62  is  5485.9345703125\n",
      "loss in iteration  63  is  5116.63623046875\n",
      "loss in iteration  64  is  4774.08154296875\n",
      "loss in iteration  65  is  4456.45458984375\n",
      "loss in iteration  66  is  4161.53564453125\n",
      "loss in iteration  67  is  3887.594970703125\n",
      "loss in iteration  68  is  3633.076904296875\n",
      "loss in iteration  69  is  3396.37060546875\n",
      "loss in iteration  70  is  3176.3095703125\n",
      "loss in iteration  71  is  2971.525390625\n",
      "loss in iteration  72  is  2780.86376953125\n",
      "loss in iteration  73  is  2603.2197265625\n",
      "loss in iteration  74  is  2437.74560546875\n",
      "loss in iteration  75  is  2283.50244140625\n",
      "loss in iteration  76  is  2139.681884765625\n",
      "loss in iteration  77  is  2005.5013427734375\n",
      "loss in iteration  78  is  1880.211181640625\n",
      "loss in iteration  79  is  1763.2664794921875\n",
      "loss in iteration  80  is  1654.024658203125\n",
      "loss in iteration  81  is  1551.9598388671875\n",
      "loss in iteration  82  is  1456.54736328125\n",
      "loss in iteration  83  is  1367.322998046875\n",
      "loss in iteration  84  is  1283.874755859375\n",
      "loss in iteration  85  is  1205.811767578125\n",
      "loss in iteration  86  is  1132.755615234375\n",
      "loss in iteration  87  is  1064.3509521484375\n",
      "loss in iteration  88  is  1000.2970581054688\n",
      "loss in iteration  89  is  940.3378295898438\n",
      "loss in iteration  90  is  884.1519775390625\n",
      "loss in iteration  91  is  831.4673461914062\n",
      "loss in iteration  92  is  782.095458984375\n",
      "loss in iteration  93  is  735.8153686523438\n",
      "loss in iteration  94  is  692.396728515625\n",
      "loss in iteration  95  is  651.6570434570312\n",
      "loss in iteration  96  is  613.4407958984375\n",
      "loss in iteration  97  is  577.5869750976562\n",
      "loss in iteration  98  is  543.916259765625\n",
      "loss in iteration  99  is  512.299560546875\n",
      "loss in iteration  100  is  482.59283447265625\n",
      "loss in iteration  101  is  454.6913757324219\n",
      "loss in iteration  102  is  428.4670104980469\n",
      "loss in iteration  103  is  403.82806396484375\n",
      "loss in iteration  104  is  380.6614990234375\n",
      "loss in iteration  105  is  358.9321594238281\n",
      "loss in iteration  106  is  338.54327392578125\n",
      "loss in iteration  107  is  319.3699951171875\n",
      "loss in iteration  108  is  301.33575439453125\n",
      "loss in iteration  109  is  284.3598327636719\n",
      "loss in iteration  110  is  268.3841857910156\n",
      "loss in iteration  111  is  253.34329223632812\n",
      "loss in iteration  112  is  239.18386840820312\n",
      "loss in iteration  113  is  225.85308837890625\n",
      "loss in iteration  114  is  213.2927703857422\n",
      "loss in iteration  115  is  201.45960998535156\n",
      "loss in iteration  116  is  190.30807495117188\n",
      "loss in iteration  117  is  179.7982177734375\n",
      "loss in iteration  118  is  169.89735412597656\n",
      "loss in iteration  119  is  160.56097412109375\n",
      "loss in iteration  120  is  151.75564575195312\n",
      "loss in iteration  121  is  143.45249938964844\n",
      "loss in iteration  122  is  135.6204833984375\n",
      "loss in iteration  123  is  128.23284912109375\n",
      "loss in iteration  124  is  121.26274871826172\n",
      "loss in iteration  125  is  114.68232727050781\n",
      "loss in iteration  126  is  108.4749526977539\n",
      "loss in iteration  127  is  102.61388397216797\n",
      "loss in iteration  128  is  97.08175659179688\n",
      "loss in iteration  129  is  91.85782623291016\n",
      "loss in iteration  130  is  86.92311096191406\n",
      "loss in iteration  131  is  82.26545715332031\n",
      "loss in iteration  132  is  77.86373901367188\n",
      "loss in iteration  133  is  73.70761108398438\n",
      "loss in iteration  134  is  69.77938842773438\n",
      "loss in iteration  135  is  66.06656646728516\n",
      "loss in iteration  136  is  62.55781555175781\n",
      "loss in iteration  137  is  59.24100112915039\n",
      "loss in iteration  138  is  56.105621337890625\n",
      "loss in iteration  139  is  53.14321517944336\n",
      "loss in iteration  140  is  50.34071731567383\n",
      "loss in iteration  141  is  47.68902587890625\n",
      "loss in iteration  142  is  45.18316650390625\n",
      "loss in iteration  143  is  42.81245422363281\n",
      "loss in iteration  144  is  40.56975555419922\n",
      "loss in iteration  145  is  38.447505950927734\n",
      "loss in iteration  146  is  36.44038009643555\n",
      "loss in iteration  147  is  34.540077209472656\n",
      "loss in iteration  148  is  32.7419548034668\n",
      "loss in iteration  149  is  31.039913177490234\n",
      "loss in iteration  150  is  29.429481506347656\n",
      "loss in iteration  151  is  27.903934478759766\n",
      "loss in iteration  152  is  26.45932388305664\n",
      "loss in iteration  153  is  25.092126846313477\n",
      "loss in iteration  154  is  23.79721450805664\n",
      "loss in iteration  155  is  22.5726318359375\n",
      "loss in iteration  156  is  21.411983489990234\n",
      "loss in iteration  157  is  20.312734603881836\n",
      "loss in iteration  158  is  19.27143096923828\n",
      "loss in iteration  159  is  18.284515380859375\n",
      "loss in iteration  160  is  17.349679946899414\n",
      "loss in iteration  161  is  16.463850021362305\n",
      "loss in iteration  162  is  15.62412166595459\n",
      "loss in iteration  163  is  14.828184127807617\n",
      "loss in iteration  164  is  14.073833465576172\n",
      "loss in iteration  165  is  13.359203338623047\n",
      "loss in iteration  166  is  12.681035995483398\n",
      "loss in iteration  167  is  12.038307189941406\n",
      "loss in iteration  168  is  11.428386688232422\n",
      "loss in iteration  169  is  10.850489616394043\n",
      "loss in iteration  170  is  10.302338600158691\n",
      "loss in iteration  171  is  9.782629013061523\n",
      "loss in iteration  172  is  9.289965629577637\n",
      "loss in iteration  173  is  8.822270393371582\n",
      "loss in iteration  174  is  8.378767013549805\n",
      "loss in iteration  175  is  7.957501411437988\n",
      "loss in iteration  176  is  7.5587615966796875\n",
      "loss in iteration  177  is  7.179525375366211\n",
      "loss in iteration  178  is  6.820079803466797\n",
      "loss in iteration  179  is  6.479042053222656\n",
      "loss in iteration  180  is  6.154978275299072\n",
      "loss in iteration  181  is  5.847873687744141\n",
      "loss in iteration  182  is  5.556174278259277\n",
      "loss in iteration  183  is  5.279632568359375\n",
      "loss in iteration  184  is  5.0167236328125\n",
      "loss in iteration  185  is  4.767216205596924\n",
      "loss in iteration  186  is  4.530378818511963\n",
      "loss in iteration  187  is  4.305598258972168\n",
      "loss in iteration  188  is  4.092042922973633\n",
      "loss in iteration  189  is  3.889310836791992\n",
      "loss in iteration  190  is  3.696829319000244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in iteration  191  is  3.513948678970337\n",
      "loss in iteration  192  is  3.3406028747558594\n",
      "loss in iteration  193  is  3.1754579544067383\n",
      "loss in iteration  194  is  3.018998146057129\n",
      "loss in iteration  195  is  2.8702335357666016\n",
      "loss in iteration  196  is  2.7289366722106934\n",
      "loss in iteration  197  is  2.5945868492126465\n",
      "loss in iteration  198  is  2.4671106338500977\n",
      "loss in iteration  199  is  2.345942974090576\n",
      "loss in iteration  200  is  2.2308268547058105\n",
      "loss in iteration  201  is  2.1213040351867676\n",
      "loss in iteration  202  is  2.017411231994629\n",
      "loss in iteration  203  is  1.9185149669647217\n",
      "loss in iteration  204  is  1.8246804475784302\n",
      "loss in iteration  205  is  1.7354625463485718\n",
      "loss in iteration  206  is  1.650622844696045\n",
      "loss in iteration  207  is  1.5700196027755737\n",
      "loss in iteration  208  is  1.4934303760528564\n",
      "loss in iteration  209  is  1.4207167625427246\n",
      "loss in iteration  210  is  1.3514621257781982\n",
      "loss in iteration  211  is  1.2858026027679443\n",
      "loss in iteration  212  is  1.223250389099121\n",
      "loss in iteration  213  is  1.1637345552444458\n",
      "loss in iteration  214  is  1.1072379350662231\n",
      "loss in iteration  215  is  1.0533703565597534\n",
      "loss in iteration  216  is  1.0022902488708496\n",
      "loss in iteration  217  is  0.9536182880401611\n",
      "loss in iteration  218  is  0.907535970211029\n",
      "loss in iteration  219  is  0.8635334968566895\n",
      "loss in iteration  220  is  0.8217347860336304\n",
      "loss in iteration  221  is  0.7820252776145935\n",
      "loss in iteration  222  is  0.7442307472229004\n",
      "loss in iteration  223  is  0.7082415223121643\n",
      "loss in iteration  224  is  0.6740465760231018\n",
      "loss in iteration  225  is  0.6415196061134338\n",
      "loss in iteration  226  is  0.6105152368545532\n",
      "loss in iteration  227  is  0.5811480283737183\n",
      "loss in iteration  228  is  0.5530911087989807\n",
      "loss in iteration  229  is  0.5265151262283325\n",
      "loss in iteration  230  is  0.5011473298072815\n",
      "loss in iteration  231  is  0.4770427346229553\n",
      "loss in iteration  232  is  0.4541720449924469\n",
      "loss in iteration  233  is  0.43235623836517334\n",
      "loss in iteration  234  is  0.41157203912734985\n",
      "loss in iteration  235  is  0.3917843699455261\n",
      "loss in iteration  236  is  0.37301403284072876\n",
      "loss in iteration  237  is  0.3551531434059143\n",
      "loss in iteration  238  is  0.3381536602973938\n",
      "loss in iteration  239  is  0.3219950497150421\n",
      "loss in iteration  240  is  0.3065173923969269\n",
      "loss in iteration  241  is  0.2918814420700073\n",
      "loss in iteration  242  is  0.2779066562652588\n",
      "loss in iteration  243  is  0.2646031379699707\n",
      "loss in iteration  244  is  0.25198373198509216\n",
      "loss in iteration  245  is  0.23999759554862976\n",
      "loss in iteration  246  is  0.22850346565246582\n",
      "loss in iteration  247  is  0.21758900582790375\n",
      "loss in iteration  248  is  0.2072446048259735\n",
      "loss in iteration  249  is  0.19736000895500183\n",
      "loss in iteration  250  is  0.18794190883636475\n",
      "loss in iteration  251  is  0.17898781597614288\n",
      "loss in iteration  252  is  0.17041750252246857\n",
      "loss in iteration  253  is  0.16233062744140625\n",
      "loss in iteration  254  is  0.15461856126785278\n",
      "loss in iteration  255  is  0.14727388322353363\n",
      "loss in iteration  256  is  0.1402372419834137\n",
      "loss in iteration  257  is  0.13359881937503815\n",
      "loss in iteration  258  is  0.12726376950740814\n",
      "loss in iteration  259  is  0.12124902009963989\n",
      "loss in iteration  260  is  0.11548328399658203\n",
      "loss in iteration  261  is  0.11000730097293854\n",
      "loss in iteration  262  is  0.10478381812572479\n",
      "loss in iteration  263  is  0.09983479231595993\n",
      "loss in iteration  264  is  0.09511446207761765\n",
      "loss in iteration  265  is  0.09058944135904312\n",
      "loss in iteration  266  is  0.08630090206861496\n",
      "loss in iteration  267  is  0.08222533762454987\n",
      "loss in iteration  268  is  0.07835817337036133\n",
      "loss in iteration  269  is  0.07464897632598877\n",
      "loss in iteration  270  is  0.0711158737540245\n",
      "loss in iteration  271  is  0.06774413585662842\n",
      "loss in iteration  272  is  0.06457696110010147\n",
      "loss in iteration  273  is  0.06150474026799202\n",
      "loss in iteration  274  is  0.05861464887857437\n",
      "loss in iteration  275  is  0.05585332214832306\n",
      "loss in iteration  276  is  0.05323544517159462\n",
      "loss in iteration  277  is  0.05072735249996185\n",
      "loss in iteration  278  is  0.04833594337105751\n",
      "loss in iteration  279  is  0.04608798399567604\n",
      "loss in iteration  280  is  0.043895404785871506\n",
      "loss in iteration  281  is  0.0418248251080513\n",
      "loss in iteration  282  is  0.03985001891851425\n",
      "loss in iteration  283  is  0.037999026477336884\n",
      "loss in iteration  284  is  0.03621871769428253\n",
      "loss in iteration  285  is  0.03452173247933388\n",
      "loss in iteration  286  is  0.032907433807849884\n",
      "loss in iteration  287  is  0.03137115389108658\n",
      "loss in iteration  288  is  0.02990911900997162\n",
      "loss in iteration  289  is  0.02849387563765049\n",
      "loss in iteration  290  is  0.02716299332678318\n",
      "loss in iteration  291  is  0.025891587138175964\n",
      "loss in iteration  292  is  0.0246808510273695\n",
      "loss in iteration  293  is  0.0235289316624403\n",
      "loss in iteration  294  is  0.022431349381804466\n",
      "loss in iteration  295  is  0.0213747750967741\n",
      "loss in iteration  296  is  0.02039005421102047\n",
      "loss in iteration  297  is  0.01944114826619625\n",
      "loss in iteration  298  is  0.018537087365984917\n",
      "loss in iteration  299  is  0.017678916454315186\n",
      "loss in iteration  300  is  0.016866635531187057\n",
      "loss in iteration  301  is  0.016075989231467247\n",
      "loss in iteration  302  is  0.015344357118010521\n",
      "loss in iteration  303  is  0.014633957296609879\n",
      "loss in iteration  304  is  0.013959741219878197\n",
      "loss in iteration  305  is  0.013324100524187088\n",
      "loss in iteration  306  is  0.0127050019800663\n",
      "loss in iteration  307  is  0.012126835063099861\n",
      "loss in iteration  308  is  0.011570842005312443\n",
      "loss in iteration  309  is  0.011043870821595192\n",
      "loss in iteration  310  is  0.010538557544350624\n",
      "loss in iteration  311  is  0.0100536635145545\n",
      "loss in iteration  312  is  0.009598536416888237\n",
      "loss in iteration  313  is  0.009159690700471401\n",
      "loss in iteration  314  is  0.008742681704461575\n",
      "loss in iteration  315  is  0.008349413052201271\n",
      "loss in iteration  316  is  0.007974418811500072\n",
      "loss in iteration  317  is  0.007612468209117651\n",
      "loss in iteration  318  is  0.0072690509259700775\n",
      "loss in iteration  319  is  0.006944832392036915\n",
      "loss in iteration  320  is  0.006639475934207439\n",
      "loss in iteration  321  is  0.006337174214422703\n",
      "loss in iteration  322  is  0.006056177895516157\n",
      "loss in iteration  323  is  0.005788397043943405\n",
      "loss in iteration  324  is  0.005533210467547178\n",
      "loss in iteration  325  is  0.005292872432619333\n",
      "loss in iteration  326  is  0.005052373744547367\n",
      "loss in iteration  327  is  0.004830175545066595\n",
      "loss in iteration  328  is  0.004613521043211222\n",
      "loss in iteration  329  is  0.004414337687194347\n",
      "loss in iteration  330  is  0.0042205918580293655\n",
      "loss in iteration  331  is  0.0040392144583165646\n",
      "loss in iteration  332  is  0.0038657805416733027\n",
      "loss in iteration  333  is  0.0037010987289249897\n",
      "loss in iteration  334  is  0.0035469108261168003\n",
      "loss in iteration  335  is  0.003393386024981737\n",
      "loss in iteration  336  is  0.003250724170356989\n",
      "loss in iteration  337  is  0.0031106602400541306\n",
      "loss in iteration  338  is  0.002977602183818817\n",
      "loss in iteration  339  is  0.002854436170309782\n",
      "loss in iteration  340  is  0.0027332294266670942\n",
      "loss in iteration  341  is  0.0026216169353574514\n",
      "loss in iteration  342  is  0.0025133416056632996\n",
      "loss in iteration  343  is  0.002408664906397462\n",
      "loss in iteration  344  is  0.002308846917003393\n",
      "loss in iteration  345  is  0.0022192487958818674\n",
      "loss in iteration  346  is  0.002128366846591234\n",
      "loss in iteration  347  is  0.00204287376254797\n",
      "loss in iteration  348  is  0.001960893627256155\n",
      "loss in iteration  349  is  0.0018839070107787848\n",
      "loss in iteration  350  is  0.00180710107088089\n",
      "loss in iteration  351  is  0.001737853977829218\n",
      "loss in iteration  352  is  0.0016692621866241097\n",
      "loss in iteration  353  is  0.0016033757710829377\n",
      "loss in iteration  354  is  0.0015428586630150676\n",
      "loss in iteration  355  is  0.0014861536910757422\n",
      "loss in iteration  356  is  0.0014282367192208767\n",
      "loss in iteration  357  is  0.001373745035380125\n",
      "loss in iteration  358  is  0.0013197580119594932\n",
      "loss in iteration  359  is  0.0012705230619758368\n",
      "loss in iteration  360  is  0.0012240118812769651\n",
      "loss in iteration  361  is  0.0011777030304074287\n",
      "loss in iteration  362  is  0.001134214224293828\n",
      "loss in iteration  363  is  0.0010915683815255761\n",
      "loss in iteration  364  is  0.0010528158163651824\n",
      "loss in iteration  365  is  0.0010163559345528483\n",
      "loss in iteration  366  is  0.0009794997749850154\n",
      "loss in iteration  367  is  0.0009446685435250401\n",
      "loss in iteration  368  is  0.0009116065921261907\n",
      "loss in iteration  369  is  0.0008779892232269049\n",
      "loss in iteration  370  is  0.0008488273015245795\n",
      "loss in iteration  371  is  0.0008197317365556955\n",
      "loss in iteration  372  is  0.0007907329709269106\n",
      "loss in iteration  373  is  0.0007640819530934095\n",
      "loss in iteration  374  is  0.0007373758708126843\n",
      "loss in iteration  375  is  0.0007122078095562756\n",
      "loss in iteration  376  is  0.0006888544303365052\n",
      "loss in iteration  377  is  0.0006673504831269383\n",
      "loss in iteration  378  is  0.0006446066545322537\n",
      "loss in iteration  379  is  0.0006244083633646369\n",
      "loss in iteration  380  is  0.0006041506421752274\n",
      "loss in iteration  381  is  0.0005851032328791916\n",
      "loss in iteration  382  is  0.0005662448238581419\n",
      "loss in iteration  383  is  0.0005477492231875658\n",
      "loss in iteration  384  is  0.0005311485147103667\n",
      "loss in iteration  385  is  0.0005145284230820835\n",
      "loss in iteration  386  is  0.0004978460492566228\n",
      "loss in iteration  387  is  0.00048237424925900996\n",
      "loss in iteration  388  is  0.0004669761401601136\n",
      "loss in iteration  389  is  0.0004536074120551348\n",
      "loss in iteration  390  is  0.0004397750599309802\n",
      "loss in iteration  391  is  0.00042726346873678267\n",
      "loss in iteration  392  is  0.0004148603184148669\n",
      "loss in iteration  393  is  0.00040259325760416687\n",
      "loss in iteration  394  is  0.0003908321959897876\n",
      "loss in iteration  395  is  0.00037853518733754754\n",
      "loss in iteration  396  is  0.00036768021527677774\n",
      "loss in iteration  397  is  0.000356548756826669\n",
      "loss in iteration  398  is  0.0003466244088485837\n",
      "loss in iteration  399  is  0.00033677060855552554\n",
      "loss in iteration  400  is  0.0003283795958850533\n",
      "loss in iteration  401  is  0.00031838679569773376\n",
      "loss in iteration  402  is  0.00030978707945905626\n",
      "loss in iteration  403  is  0.0003007444320246577\n",
      "loss in iteration  404  is  0.00029214174719527364\n",
      "loss in iteration  405  is  0.0002847965224646032\n",
      "loss in iteration  406  is  0.000277671089861542\n",
      "loss in iteration  407  is  0.00026968790916725993\n",
      "loss in iteration  408  is  0.0002630584640428424\n",
      "loss in iteration  409  is  0.0002563552698120475\n",
      "loss in iteration  410  is  0.00024905422469601035\n",
      "loss in iteration  411  is  0.0002421444223728031\n",
      "loss in iteration  412  is  0.00023656347184441984\n",
      "loss in iteration  413  is  0.00023102504201233387\n",
      "loss in iteration  414  is  0.00022487781825475395\n",
      "loss in iteration  415  is  0.00021860821289010346\n",
      "loss in iteration  416  is  0.0002139051939593628\n",
      "loss in iteration  417  is  0.0002083815634250641\n",
      "loss in iteration  418  is  0.00020355354354251176\n",
      "loss in iteration  419  is  0.00019818411965388805\n",
      "loss in iteration  420  is  0.00019349355716258287\n",
      "loss in iteration  421  is  0.0001886279642349109\n",
      "loss in iteration  422  is  0.00018470048962626606\n",
      "loss in iteration  423  is  0.00018065307813230902\n",
      "loss in iteration  424  is  0.00017664668848738074\n",
      "loss in iteration  425  is  0.00017265911446884274\n",
      "loss in iteration  426  is  0.00016841216711327434\n",
      "loss in iteration  427  is  0.00016441880143247545\n",
      "loss in iteration  428  is  0.00016114453319460154\n",
      "loss in iteration  429  is  0.00015761505346745253\n",
      "loss in iteration  430  is  0.0001538615906611085\n",
      "loss in iteration  431  is  0.00015093412366695702\n",
      "loss in iteration  432  is  0.00014782750804442912\n",
      "loss in iteration  433  is  0.00014456403732765466\n",
      "loss in iteration  434  is  0.00014072343765292317\n",
      "loss in iteration  435  is  0.00013762220623902977\n",
      "loss in iteration  436  is  0.00013477279571816325\n",
      "loss in iteration  437  is  0.00013222426059655845\n",
      "loss in iteration  438  is  0.0001294422836508602\n",
      "loss in iteration  439  is  0.00012697966303676367\n",
      "loss in iteration  440  is  0.0001241632126038894\n",
      "loss in iteration  441  is  0.0001218758916365914\n",
      "loss in iteration  442  is  0.00011949747567996383\n",
      "loss in iteration  443  is  0.0001172786796814762\n",
      "loss in iteration  444  is  0.0001145581336459145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in iteration  445  is  0.00011236242426093668\n",
      "loss in iteration  446  is  0.00011021606769645587\n",
      "loss in iteration  447  is  0.00010791431850520894\n",
      "loss in iteration  448  is  0.00010616750660119578\n",
      "loss in iteration  449  is  0.00010428040695842355\n",
      "loss in iteration  450  is  0.00010192957415711135\n",
      "loss in iteration  451  is  9.99238109216094e-05\n",
      "loss in iteration  452  is  9.812598000280559e-05\n",
      "loss in iteration  453  is  9.622688230592757e-05\n",
      "loss in iteration  454  is  9.485559712629765e-05\n",
      "loss in iteration  455  is  9.284776751883328e-05\n",
      "loss in iteration  456  is  9.134085848927498e-05\n",
      "loss in iteration  457  is  8.986066677607596e-05\n",
      "loss in iteration  458  is  8.829431317280978e-05\n",
      "loss in iteration  459  is  8.647516369819641e-05\n",
      "loss in iteration  460  is  8.490298205288127e-05\n",
      "loss in iteration  461  is  8.326144597958773e-05\n",
      "loss in iteration  462  is  8.199574949685484e-05\n",
      "loss in iteration  463  is  8.066611917456612e-05\n",
      "loss in iteration  464  is  7.899195043137297e-05\n",
      "loss in iteration  465  is  7.769992953399196e-05\n",
      "loss in iteration  466  is  7.632896449649706e-05\n",
      "loss in iteration  467  is  7.483180525014177e-05\n",
      "loss in iteration  468  is  7.376696157734841e-05\n",
      "loss in iteration  469  is  7.250150520121679e-05\n",
      "loss in iteration  470  is  7.131518941605464e-05\n",
      "loss in iteration  471  is  7.015447044977918e-05\n",
      "loss in iteration  472  is  6.879238208057359e-05\n",
      "loss in iteration  473  is  6.76816634950228e-05\n",
      "loss in iteration  474  is  6.689825386274606e-05\n",
      "loss in iteration  475  is  6.549213139805943e-05\n",
      "loss in iteration  476  is  6.413493247237056e-05\n",
      "loss in iteration  477  is  6.322833360172808e-05\n",
      "loss in iteration  478  is  6.197713082656264e-05\n",
      "loss in iteration  479  is  6.107842636993155e-05\n",
      "loss in iteration  480  is  6.0023146943422034e-05\n",
      "loss in iteration  481  is  5.938378308201209e-05\n",
      "loss in iteration  482  is  5.856324060005136e-05\n",
      "loss in iteration  483  is  5.787753616459668e-05\n",
      "loss in iteration  484  is  5.693100320058875e-05\n",
      "loss in iteration  485  is  5.598015559371561e-05\n",
      "loss in iteration  486  is  5.506407251232304e-05\n",
      "loss in iteration  487  is  5.4238931625150144e-05\n",
      "loss in iteration  488  is  5.357824556995183e-05\n",
      "loss in iteration  489  is  5.277593663777225e-05\n",
      "loss in iteration  490  is  5.199597217142582e-05\n",
      "loss in iteration  491  is  5.115837848279625e-05\n",
      "loss in iteration  492  is  5.032928311266005e-05\n",
      "loss in iteration  493  is  4.976741911377758e-05\n",
      "loss in iteration  494  is  4.901665306533687e-05\n",
      "loss in iteration  495  is  4.856746090808883e-05\n",
      "loss in iteration  496  is  4.8030145990196615e-05\n",
      "loss in iteration  497  is  4.7486104449490085e-05\n",
      "loss in iteration  498  is  4.6970082621555775e-05\n",
      "loss in iteration  499  is  4.6085246140137315e-05\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机初始化训练数据 \n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H)\n",
    "w2 = torch.randn(H, D_out)\n",
    "\n",
    "lr = 1e-6\n",
    "for iter in range(500):\n",
    "    # Forward Pass\n",
    "    h = x.mm(w1) #mm is matrix multiplication\n",
    "    a = h.clamp(min=0) #clamp收紧范围(min,max)\n",
    "    y_pred = a.mm(w2) #(N, D_out)\n",
    "    \n",
    "    # Loss Computation, use MSE\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(\"loss in iteration \", iter, \" is \", loss)\n",
    "    \n",
    "    # Backward Pass\n",
    "    # compute gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y) #(N, D_out)\n",
    "    grad_w2 = a.t().mm(grad_y_pred)\n",
    "    grad_a = grad_y_pred.mm(w2.T)\n",
    "    grad_h = grad_a.clone()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # update parameters\n",
    "    w1 -= lr * grad_w1\n",
    "    w2 -= lr * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4729e282",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T05:57:02.547487Z",
     "start_time": "2021-09-04T05:57:02.538946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(2.)\n",
      "tensor(1.)\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-81-62e22c71506a>:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  print(y.grad)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "y = w*x + b\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(w.grad)\n",
    "print(x.grad)\n",
    "print(b.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921353a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T05:58:22.577461Z",
     "start_time": "2021-09-04T05:58:22.570220Z"
    }
   },
   "source": [
    "简单两层神经网络，pytorch1，手动版2\n",
    "---------\n",
    "- $h = W_1X$\n",
    "- $a = max(0, h)$\n",
    "- $y_{hat} = W_2a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b31225f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T06:08:30.793947Z",
     "start_time": "2021-09-04T06:08:27.776053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in iteration  0  is  45237992.0\n",
      "loss in iteration  1  is  48933680.0\n",
      "loss in iteration  2  is  50731908.0\n",
      "loss in iteration  3  is  40004728.0\n",
      "loss in iteration  4  is  22434656.0\n",
      "loss in iteration  5  is  9493030.0\n",
      "loss in iteration  6  is  4105105.25\n",
      "loss in iteration  7  is  2245620.5\n",
      "loss in iteration  8  is  1550243.5\n",
      "loss in iteration  9  is  1207482.375\n",
      "loss in iteration  10  is  989772.125\n",
      "loss in iteration  11  is  829751.625\n",
      "loss in iteration  12  is  704386.75\n",
      "loss in iteration  13  is  603166.875\n",
      "loss in iteration  14  is  520214.75\n",
      "loss in iteration  15  is  451336.6875\n",
      "loss in iteration  16  is  393580.125\n",
      "loss in iteration  17  is  344879.28125\n",
      "loss in iteration  18  is  303523.9375\n",
      "loss in iteration  19  is  268205.53125\n",
      "loss in iteration  20  is  237965.09375\n",
      "loss in iteration  21  is  211890.90625\n",
      "loss in iteration  22  is  189233.828125\n",
      "loss in iteration  23  is  169449.390625\n",
      "loss in iteration  24  is  152120.015625\n",
      "loss in iteration  25  is  136873.578125\n",
      "loss in iteration  26  is  123417.4296875\n",
      "loss in iteration  27  is  111526.96875\n",
      "loss in iteration  28  is  100985.71875\n",
      "loss in iteration  29  is  91596.328125\n",
      "loss in iteration  30  is  83212.640625\n",
      "loss in iteration  31  is  75707.375\n",
      "loss in iteration  32  is  68975.2734375\n",
      "loss in iteration  33  is  62926.7578125\n",
      "loss in iteration  34  is  57479.5546875\n",
      "loss in iteration  35  is  52569.34375\n",
      "loss in iteration  36  is  48138.5703125\n",
      "loss in iteration  37  is  44132.8828125\n",
      "loss in iteration  38  is  40501.53515625\n",
      "loss in iteration  39  is  37205.90625\n",
      "loss in iteration  40  is  34208.81640625\n",
      "loss in iteration  41  is  31480.806640625\n",
      "loss in iteration  42  is  28995.318359375\n",
      "loss in iteration  43  is  26727.845703125\n",
      "loss in iteration  44  is  24656.14453125\n",
      "loss in iteration  45  is  22762.224609375\n",
      "loss in iteration  46  is  21028.58984375\n",
      "loss in iteration  47  is  19440.30859375\n",
      "loss in iteration  48  is  17983.75390625\n",
      "loss in iteration  49  is  16646.74609375\n",
      "loss in iteration  50  is  15420.2412109375\n",
      "loss in iteration  51  is  14293.9033203125\n",
      "loss in iteration  52  is  13257.220703125\n",
      "loss in iteration  53  is  12303.078125\n",
      "loss in iteration  54  is  11423.98046875\n",
      "loss in iteration  55  is  10613.4453125\n",
      "loss in iteration  56  is  9865.001953125\n",
      "loss in iteration  57  is  9174.3271484375\n",
      "loss in iteration  58  is  8536.22265625\n",
      "loss in iteration  59  is  7946.181640625\n",
      "loss in iteration  60  is  7400.23583984375\n",
      "loss in iteration  61  is  6894.6923828125\n",
      "loss in iteration  62  is  6426.375\n",
      "loss in iteration  63  is  5992.5107421875\n",
      "loss in iteration  64  is  5590.1015625\n",
      "loss in iteration  65  is  5216.83251953125\n",
      "loss in iteration  66  is  4870.3125\n",
      "loss in iteration  67  is  4548.6103515625\n",
      "loss in iteration  68  is  4249.7060546875\n",
      "loss in iteration  69  is  3971.822021484375\n",
      "loss in iteration  70  is  3713.46435546875\n",
      "loss in iteration  71  is  3472.97607421875\n",
      "loss in iteration  72  is  3249.20263671875\n",
      "loss in iteration  73  is  3040.804443359375\n",
      "loss in iteration  74  is  2846.64501953125\n",
      "loss in iteration  75  is  2665.80615234375\n",
      "loss in iteration  76  is  2497.1796875\n",
      "loss in iteration  77  is  2339.9267578125\n",
      "loss in iteration  78  is  2193.27197265625\n",
      "loss in iteration  79  is  2056.377685546875\n",
      "loss in iteration  80  is  1928.5908203125\n",
      "loss in iteration  81  is  1809.201171875\n",
      "loss in iteration  82  is  1697.6654052734375\n",
      "loss in iteration  83  is  1593.476318359375\n",
      "loss in iteration  84  is  1496.0457763671875\n",
      "loss in iteration  85  is  1404.94091796875\n",
      "loss in iteration  86  is  1319.686279296875\n",
      "loss in iteration  87  is  1239.9017333984375\n",
      "loss in iteration  88  is  1165.165283203125\n",
      "loss in iteration  89  is  1095.1500244140625\n",
      "loss in iteration  90  is  1029.5909423828125\n",
      "loss in iteration  91  is  968.218017578125\n",
      "loss in iteration  92  is  910.6751708984375\n",
      "loss in iteration  93  is  856.7398681640625\n",
      "loss in iteration  94  is  806.1908569335938\n",
      "loss in iteration  95  is  758.7913208007812\n",
      "loss in iteration  96  is  714.3082275390625\n",
      "loss in iteration  97  is  672.583251953125\n",
      "loss in iteration  98  is  633.42919921875\n",
      "loss in iteration  99  is  596.6661987304688\n",
      "loss in iteration  100  is  562.1452026367188\n",
      "loss in iteration  101  is  529.7240600585938\n",
      "loss in iteration  102  is  499.2696533203125\n",
      "loss in iteration  103  is  470.64227294921875\n",
      "loss in iteration  104  is  443.74383544921875\n",
      "loss in iteration  105  is  418.4627685546875\n",
      "loss in iteration  106  is  394.6900939941406\n",
      "loss in iteration  107  is  372.32989501953125\n",
      "loss in iteration  108  is  351.297119140625\n",
      "loss in iteration  109  is  331.5071716308594\n",
      "loss in iteration  110  is  312.88665771484375\n",
      "loss in iteration  111  is  295.35443115234375\n",
      "loss in iteration  112  is  278.8550109863281\n",
      "loss in iteration  113  is  263.3129577636719\n",
      "loss in iteration  114  is  248.67852783203125\n",
      "loss in iteration  115  is  234.89805603027344\n",
      "loss in iteration  116  is  221.91741943359375\n",
      "loss in iteration  117  is  209.68446350097656\n",
      "loss in iteration  118  is  198.14979553222656\n",
      "loss in iteration  119  is  187.28005981445312\n",
      "loss in iteration  120  is  177.0341339111328\n",
      "loss in iteration  121  is  167.36952209472656\n",
      "loss in iteration  122  is  158.2554931640625\n",
      "loss in iteration  123  is  149.6593017578125\n",
      "loss in iteration  124  is  141.5462188720703\n",
      "loss in iteration  125  is  133.8927764892578\n",
      "loss in iteration  126  is  126.67135620117188\n",
      "loss in iteration  127  is  119.84962463378906\n",
      "loss in iteration  128  is  113.4132080078125\n",
      "loss in iteration  129  is  107.33515930175781\n",
      "loss in iteration  130  is  101.59439849853516\n",
      "loss in iteration  131  is  96.17144775390625\n",
      "loss in iteration  132  is  91.05054473876953\n",
      "loss in iteration  133  is  86.21057891845703\n",
      "loss in iteration  134  is  81.64080047607422\n",
      "loss in iteration  135  is  77.31830596923828\n",
      "loss in iteration  136  is  73.23423767089844\n",
      "loss in iteration  137  is  69.37274169921875\n",
      "loss in iteration  138  is  65.72422790527344\n",
      "loss in iteration  139  is  62.27355194091797\n",
      "loss in iteration  140  is  59.009124755859375\n",
      "loss in iteration  141  is  55.92238235473633\n",
      "loss in iteration  142  is  53.002262115478516\n",
      "loss in iteration  143  is  50.24038314819336\n",
      "loss in iteration  144  is  47.62724685668945\n",
      "loss in iteration  145  is  45.15422821044922\n",
      "loss in iteration  146  is  42.81375503540039\n",
      "loss in iteration  147  is  40.597869873046875\n",
      "loss in iteration  148  is  38.49999237060547\n",
      "loss in iteration  149  is  36.51460266113281\n",
      "loss in iteration  150  is  34.63513946533203\n",
      "loss in iteration  151  is  32.85511016845703\n",
      "loss in iteration  152  is  31.16973114013672\n",
      "loss in iteration  153  is  29.5733585357666\n",
      "loss in iteration  154  is  28.0609188079834\n",
      "loss in iteration  155  is  26.628795623779297\n",
      "loss in iteration  156  is  25.27056884765625\n",
      "loss in iteration  157  is  23.98452377319336\n",
      "loss in iteration  158  is  22.765445709228516\n",
      "loss in iteration  159  is  21.61013412475586\n",
      "loss in iteration  160  is  20.514713287353516\n",
      "loss in iteration  161  is  19.47810173034668\n",
      "loss in iteration  162  is  18.493793487548828\n",
      "loss in iteration  163  is  17.561193466186523\n",
      "loss in iteration  164  is  16.676725387573242\n",
      "loss in iteration  165  is  15.83737564086914\n",
      "loss in iteration  166  is  15.042583465576172\n",
      "loss in iteration  167  is  14.288000106811523\n",
      "loss in iteration  168  is  13.57320785522461\n",
      "loss in iteration  169  is  12.89337158203125\n",
      "loss in iteration  170  is  12.249693870544434\n",
      "loss in iteration  171  is  11.638823509216309\n",
      "loss in iteration  172  is  11.059057235717773\n",
      "loss in iteration  173  is  10.508484840393066\n",
      "loss in iteration  174  is  9.986551284790039\n",
      "loss in iteration  175  is  9.490898132324219\n",
      "loss in iteration  176  is  9.020186424255371\n",
      "loss in iteration  177  is  8.573873519897461\n",
      "loss in iteration  178  is  8.150214195251465\n",
      "loss in iteration  179  is  7.747646331787109\n",
      "loss in iteration  180  is  7.365523815155029\n",
      "loss in iteration  181  is  7.002705097198486\n",
      "loss in iteration  182  is  6.65811824798584\n",
      "loss in iteration  183  is  6.330878734588623\n",
      "loss in iteration  184  is  6.0202317237854\n",
      "loss in iteration  185  is  5.724997520446777\n",
      "loss in iteration  186  is  5.444614887237549\n",
      "loss in iteration  187  is  5.178333282470703\n",
      "loss in iteration  188  is  4.9252729415893555\n",
      "loss in iteration  189  is  4.6848530769348145\n",
      "loss in iteration  190  is  4.456296920776367\n",
      "loss in iteration  191  is  4.239449501037598\n",
      "loss in iteration  192  is  4.033105850219727\n",
      "loss in iteration  193  is  3.836902618408203\n",
      "loss in iteration  194  is  3.650390148162842\n",
      "loss in iteration  195  is  3.473461151123047\n",
      "loss in iteration  196  is  3.30527925491333\n",
      "loss in iteration  197  is  3.145087957382202\n",
      "loss in iteration  198  is  2.9931857585906982\n",
      "loss in iteration  199  is  2.8485493659973145\n",
      "loss in iteration  200  is  2.711092233657837\n",
      "loss in iteration  201  is  2.580321788787842\n",
      "loss in iteration  202  is  2.4559543132781982\n",
      "loss in iteration  203  is  2.3376758098602295\n",
      "loss in iteration  204  is  2.2253098487854004\n",
      "loss in iteration  205  is  2.1183583736419678\n",
      "loss in iteration  206  is  2.0166940689086914\n",
      "loss in iteration  207  is  1.920011043548584\n",
      "loss in iteration  208  is  1.8279838562011719\n",
      "loss in iteration  209  is  1.7405509948730469\n",
      "loss in iteration  210  is  1.6572099924087524\n",
      "loss in iteration  211  is  1.5781792402267456\n",
      "loss in iteration  212  is  1.5027108192443848\n",
      "loss in iteration  213  is  1.4309417009353638\n",
      "loss in iteration  214  is  1.3627405166625977\n",
      "loss in iteration  215  is  1.297922134399414\n",
      "loss in iteration  216  is  1.2360312938690186\n",
      "loss in iteration  217  is  1.1773123741149902\n",
      "loss in iteration  218  is  1.1213473081588745\n",
      "loss in iteration  219  is  1.0681512355804443\n",
      "loss in iteration  220  is  1.0174506902694702\n",
      "loss in iteration  221  is  0.969131350517273\n",
      "loss in iteration  222  is  0.9232280850410461\n",
      "loss in iteration  223  is  0.879604160785675\n",
      "loss in iteration  224  is  0.8379209041595459\n",
      "loss in iteration  225  is  0.7983276844024658\n",
      "loss in iteration  226  is  0.760628342628479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in iteration  227  is  0.7247432470321655\n",
      "loss in iteration  228  is  0.6905959844589233\n",
      "loss in iteration  229  is  0.6579777002334595\n",
      "loss in iteration  230  is  0.6269513368606567\n",
      "loss in iteration  231  is  0.5974600315093994\n",
      "loss in iteration  232  is  0.5694156885147095\n",
      "loss in iteration  233  is  0.5426852107048035\n",
      "loss in iteration  234  is  0.5171175003051758\n",
      "loss in iteration  235  is  0.4929123818874359\n",
      "loss in iteration  236  is  0.46974751353263855\n",
      "loss in iteration  237  is  0.4477561116218567\n",
      "loss in iteration  238  is  0.4267318546772003\n",
      "loss in iteration  239  is  0.406850665807724\n",
      "loss in iteration  240  is  0.3877498209476471\n",
      "loss in iteration  241  is  0.3695850968360901\n",
      "loss in iteration  242  is  0.3523317277431488\n",
      "loss in iteration  243  is  0.3359047770500183\n",
      "loss in iteration  244  is  0.32023340463638306\n",
      "loss in iteration  245  is  0.3052760660648346\n",
      "loss in iteration  246  is  0.29101794958114624\n",
      "loss in iteration  247  is  0.2774584889411926\n",
      "loss in iteration  248  is  0.26453515887260437\n",
      "loss in iteration  249  is  0.25224021077156067\n",
      "loss in iteration  250  is  0.24051839113235474\n",
      "loss in iteration  251  is  0.22936074435710907\n",
      "loss in iteration  252  is  0.2186596691608429\n",
      "loss in iteration  253  is  0.20853739976882935\n",
      "loss in iteration  254  is  0.19883480668067932\n",
      "loss in iteration  255  is  0.18963056802749634\n",
      "loss in iteration  256  is  0.18084114789962769\n",
      "loss in iteration  257  is  0.17243526875972748\n",
      "loss in iteration  258  is  0.16444514691829681\n",
      "loss in iteration  259  is  0.15682625770568848\n",
      "loss in iteration  260  is  0.14959733188152313\n",
      "loss in iteration  261  is  0.14267705380916595\n",
      "loss in iteration  262  is  0.13609105348587036\n",
      "loss in iteration  263  is  0.1298048049211502\n",
      "loss in iteration  264  is  0.12376950681209564\n",
      "loss in iteration  265  is  0.11810530722141266\n",
      "loss in iteration  266  is  0.1126611977815628\n",
      "loss in iteration  267  is  0.10747930407524109\n",
      "loss in iteration  268  is  0.10251271724700928\n",
      "loss in iteration  269  is  0.09780333936214447\n",
      "loss in iteration  270  is  0.09330856800079346\n",
      "loss in iteration  271  is  0.089007169008255\n",
      "loss in iteration  272  is  0.0848986878991127\n",
      "loss in iteration  273  is  0.08100911229848862\n",
      "loss in iteration  274  is  0.0773027092218399\n",
      "loss in iteration  275  is  0.07375234365463257\n",
      "loss in iteration  276  is  0.07036850601434708\n",
      "loss in iteration  277  is  0.06714817881584167\n",
      "loss in iteration  278  is  0.06407251209020615\n",
      "loss in iteration  279  is  0.06112571805715561\n",
      "loss in iteration  280  is  0.05831874534487724\n",
      "loss in iteration  281  is  0.05564427003264427\n",
      "loss in iteration  282  is  0.05312041565775871\n",
      "loss in iteration  283  is  0.05068083852529526\n",
      "loss in iteration  284  is  0.0483696348965168\n",
      "loss in iteration  285  is  0.04616260156035423\n",
      "loss in iteration  286  is  0.04405847191810608\n",
      "loss in iteration  287  is  0.04203467816114426\n",
      "loss in iteration  288  is  0.04011392220854759\n",
      "loss in iteration  289  is  0.038291871547698975\n",
      "loss in iteration  290  is  0.03654302656650543\n",
      "loss in iteration  291  is  0.03487616404891014\n",
      "loss in iteration  292  is  0.0332975760102272\n",
      "loss in iteration  293  is  0.031784411519765854\n",
      "loss in iteration  294  is  0.03034823387861252\n",
      "loss in iteration  295  is  0.0289552491158247\n",
      "loss in iteration  296  is  0.027642639353871346\n",
      "loss in iteration  297  is  0.026403086259961128\n",
      "loss in iteration  298  is  0.02521081455051899\n",
      "loss in iteration  299  is  0.02407226152718067\n",
      "loss in iteration  300  is  0.022991612553596497\n",
      "loss in iteration  301  is  0.021956169977784157\n",
      "loss in iteration  302  is  0.02095961570739746\n",
      "loss in iteration  303  is  0.02001364715397358\n",
      "loss in iteration  304  is  0.019104955717921257\n",
      "loss in iteration  305  is  0.0182504840195179\n",
      "loss in iteration  306  is  0.017428627237677574\n",
      "loss in iteration  307  is  0.016650309786200523\n",
      "loss in iteration  308  is  0.015903649851679802\n",
      "loss in iteration  309  is  0.015188585966825485\n",
      "loss in iteration  310  is  0.014507247135043144\n",
      "loss in iteration  311  is  0.013865872286260128\n",
      "loss in iteration  312  is  0.013243213295936584\n",
      "loss in iteration  313  is  0.012650123797357082\n",
      "loss in iteration  314  is  0.012087095528841019\n",
      "loss in iteration  315  is  0.011553589254617691\n",
      "loss in iteration  316  is  0.011043226346373558\n",
      "loss in iteration  317  is  0.010552909225225449\n",
      "loss in iteration  318  is  0.010090003721415997\n",
      "loss in iteration  319  is  0.00964992307126522\n",
      "loss in iteration  320  is  0.009223446249961853\n",
      "loss in iteration  321  is  0.008821779862046242\n",
      "loss in iteration  322  is  0.008436205796897411\n",
      "loss in iteration  323  is  0.008070787414908409\n",
      "loss in iteration  324  is  0.0077223749831318855\n",
      "loss in iteration  325  is  0.007385313976556063\n",
      "loss in iteration  326  is  0.007065327372401953\n",
      "loss in iteration  327  is  0.006760038901120424\n",
      "loss in iteration  328  is  0.00647575780749321\n",
      "loss in iteration  329  is  0.00619447510689497\n",
      "loss in iteration  330  is  0.005926870740950108\n",
      "loss in iteration  331  is  0.005676553584635258\n",
      "loss in iteration  332  is  0.005434849765151739\n",
      "loss in iteration  333  is  0.005201080814003944\n",
      "loss in iteration  334  is  0.004981828387826681\n",
      "loss in iteration  335  is  0.004779373295605183\n",
      "loss in iteration  336  is  0.004576197825372219\n",
      "loss in iteration  337  is  0.004381389357149601\n",
      "loss in iteration  338  is  0.004200347699224949\n",
      "loss in iteration  339  is  0.00402493542060256\n",
      "loss in iteration  340  is  0.0038588177412748337\n",
      "loss in iteration  341  is  0.003699246793985367\n",
      "loss in iteration  342  is  0.0035480293445289135\n",
      "loss in iteration  343  is  0.003404987510293722\n",
      "loss in iteration  344  is  0.0032644597813487053\n",
      "loss in iteration  345  is  0.0031353796366602182\n",
      "loss in iteration  346  is  0.0030080415308475494\n",
      "loss in iteration  347  is  0.0028887048829346895\n",
      "loss in iteration  348  is  0.002772181760519743\n",
      "loss in iteration  349  is  0.0026633678935468197\n",
      "loss in iteration  350  is  0.0025602111127227545\n",
      "loss in iteration  351  is  0.002462387317791581\n",
      "loss in iteration  352  is  0.0023626629263162613\n",
      "loss in iteration  353  is  0.002273067831993103\n",
      "loss in iteration  354  is  0.0021851554047316313\n",
      "loss in iteration  355  is  0.0021044230088591576\n",
      "loss in iteration  356  is  0.0020238584838807583\n",
      "loss in iteration  357  is  0.0019488557009026408\n",
      "loss in iteration  358  is  0.0018763033440336585\n",
      "loss in iteration  359  is  0.0018050371436402202\n",
      "loss in iteration  360  is  0.0017409331630915403\n",
      "loss in iteration  361  is  0.001675623469054699\n",
      "loss in iteration  362  is  0.0016163057880476117\n",
      "loss in iteration  363  is  0.0015559435123577714\n",
      "loss in iteration  364  is  0.0014994153752923012\n",
      "loss in iteration  365  is  0.0014460555976256728\n",
      "loss in iteration  366  is  0.00139180151745677\n",
      "loss in iteration  367  is  0.0013431287370622158\n",
      "loss in iteration  368  is  0.0012937712017446756\n",
      "loss in iteration  369  is  0.0012497350107878447\n",
      "loss in iteration  370  is  0.0012071041855961084\n",
      "loss in iteration  371  is  0.001164991408586502\n",
      "loss in iteration  372  is  0.0011254377895966172\n",
      "loss in iteration  373  is  0.001087430864572525\n",
      "loss in iteration  374  is  0.001050856546498835\n",
      "loss in iteration  375  is  0.0010162662947550416\n",
      "loss in iteration  376  is  0.0009833918884396553\n",
      "loss in iteration  377  is  0.0009510311065241694\n",
      "loss in iteration  378  is  0.0009195270831696689\n",
      "loss in iteration  379  is  0.0008879679953679442\n",
      "loss in iteration  380  is  0.0008597779087722301\n",
      "loss in iteration  381  is  0.0008314913720823824\n",
      "loss in iteration  382  is  0.0008057046215981245\n",
      "loss in iteration  383  is  0.0007790941162966192\n",
      "loss in iteration  384  is  0.000755064538680017\n",
      "loss in iteration  385  is  0.0007316321134567261\n",
      "loss in iteration  386  is  0.0007082932279445231\n",
      "loss in iteration  387  is  0.0006859771092422307\n",
      "loss in iteration  388  is  0.00066574034281075\n",
      "loss in iteration  389  is  0.0006452600355260074\n",
      "loss in iteration  390  is  0.000625761691480875\n",
      "loss in iteration  391  is  0.0006071793613955379\n",
      "loss in iteration  392  is  0.000589257397223264\n",
      "loss in iteration  393  is  0.0005725753144361079\n",
      "loss in iteration  394  is  0.0005549085326492786\n",
      "loss in iteration  395  is  0.0005371796432882547\n",
      "loss in iteration  396  is  0.0005223256885074079\n",
      "loss in iteration  397  is  0.0005076563102193177\n",
      "loss in iteration  398  is  0.0004928916459903121\n",
      "loss in iteration  399  is  0.0004782774194609374\n",
      "loss in iteration  400  is  0.00046573617146350443\n",
      "loss in iteration  401  is  0.0004524493124336004\n",
      "loss in iteration  402  is  0.0004395149298943579\n",
      "loss in iteration  403  is  0.0004280409775674343\n",
      "loss in iteration  404  is  0.0004181193362455815\n",
      "loss in iteration  405  is  0.0004055746248923242\n",
      "loss in iteration  406  is  0.0003951731778215617\n",
      "loss in iteration  407  is  0.0003845409955829382\n",
      "loss in iteration  408  is  0.00037510061520151794\n",
      "loss in iteration  409  is  0.0003656720800790936\n",
      "loss in iteration  410  is  0.00035646214382722974\n",
      "loss in iteration  411  is  0.00034720852272585034\n",
      "loss in iteration  412  is  0.00033891969360411167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in iteration  413  is  0.00033017992973327637\n",
      "loss in iteration  414  is  0.00032198402914218605\n",
      "loss in iteration  415  is  0.00031398568535223603\n",
      "loss in iteration  416  is  0.00030613254057243466\n",
      "loss in iteration  417  is  0.0002985360915772617\n",
      "loss in iteration  418  is  0.0002911134506575763\n",
      "loss in iteration  419  is  0.00028437876608222723\n",
      "loss in iteration  420  is  0.0002774300810415298\n",
      "loss in iteration  421  is  0.00027108530048280954\n",
      "loss in iteration  422  is  0.00026479034568183124\n",
      "loss in iteration  423  is  0.0002582093875389546\n",
      "loss in iteration  424  is  0.00025298199034295976\n",
      "loss in iteration  425  is  0.00024692140868864954\n",
      "loss in iteration  426  is  0.00024123826005961746\n",
      "loss in iteration  427  is  0.0002352296287426725\n",
      "loss in iteration  428  is  0.0002308788534719497\n",
      "loss in iteration  429  is  0.00022513550356961787\n",
      "loss in iteration  430  is  0.00022015180729795247\n",
      "loss in iteration  431  is  0.0002154518006136641\n",
      "loss in iteration  432  is  0.00021096538694109768\n",
      "loss in iteration  433  is  0.00020639185095205903\n",
      "loss in iteration  434  is  0.00020240792946424335\n",
      "loss in iteration  435  is  0.0001984519767574966\n",
      "loss in iteration  436  is  0.0001940454385476187\n",
      "loss in iteration  437  is  0.00018984850612469018\n",
      "loss in iteration  438  is  0.00018588497187010944\n",
      "loss in iteration  439  is  0.00018218177137896419\n",
      "loss in iteration  440  is  0.00017823546659201384\n",
      "loss in iteration  441  is  0.0001748574577504769\n",
      "loss in iteration  442  is  0.00017122147255577147\n",
      "loss in iteration  443  is  0.00016765478358138353\n",
      "loss in iteration  444  is  0.00016415237041655928\n",
      "loss in iteration  445  is  0.00016108159616123885\n",
      "loss in iteration  446  is  0.0001580206153448671\n",
      "loss in iteration  447  is  0.00015497302229050547\n",
      "loss in iteration  448  is  0.00015182564675342292\n",
      "loss in iteration  449  is  0.00014876635395921767\n",
      "loss in iteration  450  is  0.00014599732821807265\n",
      "loss in iteration  451  is  0.00014339888002723455\n",
      "loss in iteration  452  is  0.00014111299242358655\n",
      "loss in iteration  453  is  0.00013847099035046995\n",
      "loss in iteration  454  is  0.0001357232395093888\n",
      "loss in iteration  455  is  0.0001332787942374125\n",
      "loss in iteration  456  is  0.00013049916015006602\n",
      "loss in iteration  457  is  0.0001285733305849135\n",
      "loss in iteration  458  is  0.00012570923718158156\n",
      "loss in iteration  459  is  0.0001236038951901719\n",
      "loss in iteration  460  is  0.00012174275616416708\n",
      "loss in iteration  461  is  0.00011945163714699447\n",
      "loss in iteration  462  is  0.0001175123470602557\n",
      "loss in iteration  463  is  0.0001153880511992611\n",
      "loss in iteration  464  is  0.00011311745038256049\n",
      "loss in iteration  465  is  0.00011137437832076102\n",
      "loss in iteration  466  is  0.00010932931036222726\n",
      "loss in iteration  467  is  0.00010784223559312522\n",
      "loss in iteration  468  is  0.0001060099748428911\n",
      "loss in iteration  469  is  0.0001043649681378156\n",
      "loss in iteration  470  is  0.00010242055577691644\n",
      "loss in iteration  471  is  0.00010065619426313788\n",
      "loss in iteration  472  is  9.907961066346616e-05\n",
      "loss in iteration  473  is  9.729094745125622e-05\n",
      "loss in iteration  474  is  9.577896707924083e-05\n",
      "loss in iteration  475  is  9.431381477043033e-05\n",
      "loss in iteration  476  is  9.270202281186357e-05\n",
      "loss in iteration  477  is  9.123082418227568e-05\n",
      "loss in iteration  478  is  8.987373439595103e-05\n",
      "loss in iteration  479  is  8.846219134284183e-05\n",
      "loss in iteration  480  is  8.735941082704812e-05\n",
      "loss in iteration  481  is  8.607482595834881e-05\n",
      "loss in iteration  482  is  8.466134022455662e-05\n",
      "loss in iteration  483  is  8.34188685985282e-05\n",
      "loss in iteration  484  is  8.210211672121659e-05\n",
      "loss in iteration  485  is  8.085465378826484e-05\n",
      "loss in iteration  486  is  7.984969852259383e-05\n",
      "loss in iteration  487  is  7.85363808972761e-05\n",
      "loss in iteration  488  is  7.741659646853805e-05\n",
      "loss in iteration  489  is  7.645331788808107e-05\n",
      "loss in iteration  490  is  7.549145084340125e-05\n",
      "loss in iteration  491  is  7.462335634045303e-05\n",
      "loss in iteration  492  is  7.331928645726293e-05\n",
      "loss in iteration  493  is  7.241001003421843e-05\n",
      "loss in iteration  494  is  7.130909216357395e-05\n",
      "loss in iteration  495  is  7.032963185338303e-05\n",
      "loss in iteration  496  is  6.944284541532397e-05\n",
      "loss in iteration  497  is  6.84117985656485e-05\n",
      "loss in iteration  498  is  6.75311021041125e-05\n",
      "loss in iteration  499  is  6.66528067085892e-05\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机初始化训练数据 \n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "lr = 1e-6\n",
    "for iter in range(500):\n",
    "    # Forward Pass\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # Loss Computation, use MSE\n",
    "    loss = (y_pred - y).pow(2).sum() #computation graph\n",
    "    print(\"loss in iteration \", iter, \" is \", loss.item())\n",
    "    \n",
    "    # Backward Pass\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # update parameters\n",
    "    with torch.no_grad():\n",
    "        w1 -= lr * w1.grad\n",
    "        w2 -= lr * w2.grad\n",
    "        w1.grad.zero_() #需要清零，不然会累加\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89747f8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T06:09:11.213016Z",
     "start_time": "2021-09-04T06:09:11.208756Z"
    }
   },
   "source": [
    "简单两层神经网络，pytorch1，用nn库\n",
    "---------\n",
    "- $h = W_1X$\n",
    "- $a = max(0, h)$\n",
    "- $y_{hat} = W_2a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2a98a373",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T06:21:31.314338Z",
     "start_time": "2021-09-04T06:21:27.658454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(27731418., grad_fn=<MseLossBackward>)\n",
      "1 tensor(22547084., grad_fn=<MseLossBackward>)\n",
      "2 tensor(22564818., grad_fn=<MseLossBackward>)\n",
      "3 tensor(24078502., grad_fn=<MseLossBackward>)\n",
      "4 tensor(24177390., grad_fn=<MseLossBackward>)\n",
      "5 tensor(21284418., grad_fn=<MseLossBackward>)\n",
      "6 tensor(15743310., grad_fn=<MseLossBackward>)\n",
      "7 tensor(10000848., grad_fn=<MseLossBackward>)\n",
      "8 tensor(5718158., grad_fn=<MseLossBackward>)\n",
      "9 tensor(3200308., grad_fn=<MseLossBackward>)\n",
      "10 tensor(1864658.3750, grad_fn=<MseLossBackward>)\n",
      "11 tensor(1183579.5000, grad_fn=<MseLossBackward>)\n",
      "12 tensor(824959.2500, grad_fn=<MseLossBackward>)\n",
      "13 tensor(622996.2500, grad_fn=<MseLossBackward>)\n",
      "14 tensor(497804.2188, grad_fn=<MseLossBackward>)\n",
      "15 tensor(412511.1875, grad_fn=<MseLossBackward>)\n",
      "16 tensor(349588.1562, grad_fn=<MseLossBackward>)\n",
      "17 tensor(300503.3438, grad_fn=<MseLossBackward>)\n",
      "18 tensor(260746.5469, grad_fn=<MseLossBackward>)\n",
      "19 tensor(227831.0781, grad_fn=<MseLossBackward>)\n",
      "20 tensor(200134., grad_fn=<MseLossBackward>)\n",
      "21 tensor(176590.6875, grad_fn=<MseLossBackward>)\n",
      "22 tensor(156403.9219, grad_fn=<MseLossBackward>)\n",
      "23 tensor(139054., grad_fn=<MseLossBackward>)\n",
      "24 tensor(124001.6328, grad_fn=<MseLossBackward>)\n",
      "25 tensor(110889.6641, grad_fn=<MseLossBackward>)\n",
      "26 tensor(99433.4062, grad_fn=<MseLossBackward>)\n",
      "27 tensor(89374.0859, grad_fn=<MseLossBackward>)\n",
      "28 tensor(80509.8359, grad_fn=<MseLossBackward>)\n",
      "29 tensor(72675.9141, grad_fn=<MseLossBackward>)\n",
      "30 tensor(65730.8594, grad_fn=<MseLossBackward>)\n",
      "31 tensor(59554.6875, grad_fn=<MseLossBackward>)\n",
      "32 tensor(54046.9805, grad_fn=<MseLossBackward>)\n",
      "33 tensor(49127.8555, grad_fn=<MseLossBackward>)\n",
      "34 tensor(44723.5742, grad_fn=<MseLossBackward>)\n",
      "35 tensor(40770.2227, grad_fn=<MseLossBackward>)\n",
      "36 tensor(37217.5156, grad_fn=<MseLossBackward>)\n",
      "37 tensor(34018.6406, grad_fn=<MseLossBackward>)\n",
      "38 tensor(31133.6250, grad_fn=<MseLossBackward>)\n",
      "39 tensor(28527.4688, grad_fn=<MseLossBackward>)\n",
      "40 tensor(26170.3242, grad_fn=<MseLossBackward>)\n",
      "41 tensor(24033.6816, grad_fn=<MseLossBackward>)\n",
      "42 tensor(22093.2441, grad_fn=<MseLossBackward>)\n",
      "43 tensor(20329.5039, grad_fn=<MseLossBackward>)\n",
      "44 tensor(18722.8379, grad_fn=<MseLossBackward>)\n",
      "45 tensor(17258.3457, grad_fn=<MseLossBackward>)\n",
      "46 tensor(15921.9736, grad_fn=<MseLossBackward>)\n",
      "47 tensor(14700.6914, grad_fn=<MseLossBackward>)\n",
      "48 tensor(13584.4688, grad_fn=<MseLossBackward>)\n",
      "49 tensor(12562.7148, grad_fn=<MseLossBackward>)\n",
      "50 tensor(11625.7764, grad_fn=<MseLossBackward>)\n",
      "51 tensor(10766.0625, grad_fn=<MseLossBackward>)\n",
      "52 tensor(9976.3848, grad_fn=<MseLossBackward>)\n",
      "53 tensor(9250.4609, grad_fn=<MseLossBackward>)\n",
      "54 tensor(8582.6182, grad_fn=<MseLossBackward>)\n",
      "55 tensor(7967.4414, grad_fn=<MseLossBackward>)\n",
      "56 tensor(7400.3398, grad_fn=<MseLossBackward>)\n",
      "57 tensor(6877.2852, grad_fn=<MseLossBackward>)\n",
      "58 tensor(6394.3877, grad_fn=<MseLossBackward>)\n",
      "59 tensor(5948.4399, grad_fn=<MseLossBackward>)\n",
      "60 tensor(5536.0879, grad_fn=<MseLossBackward>)\n",
      "61 tensor(5154.6152, grad_fn=<MseLossBackward>)\n",
      "62 tensor(4801.9766, grad_fn=<MseLossBackward>)\n",
      "63 tensor(4475.4424, grad_fn=<MseLossBackward>)\n",
      "64 tensor(4172.7856, grad_fn=<MseLossBackward>)\n",
      "65 tensor(3892.1160, grad_fn=<MseLossBackward>)\n",
      "66 tensor(3631.7095, grad_fn=<MseLossBackward>)\n",
      "67 tensor(3390.0771, grad_fn=<MseLossBackward>)\n",
      "68 tensor(3165.6025, grad_fn=<MseLossBackward>)\n",
      "69 tensor(2957.2146, grad_fn=<MseLossBackward>)\n",
      "70 tensor(2763.5642, grad_fn=<MseLossBackward>)\n",
      "71 tensor(2583.4282, grad_fn=<MseLossBackward>)\n",
      "72 tensor(2415.8499, grad_fn=<MseLossBackward>)\n",
      "73 tensor(2259.7913, grad_fn=<MseLossBackward>)\n",
      "74 tensor(2114.5811, grad_fn=<MseLossBackward>)\n",
      "75 tensor(1979.3040, grad_fn=<MseLossBackward>)\n",
      "76 tensor(1853.2611, grad_fn=<MseLossBackward>)\n",
      "77 tensor(1735.6665, grad_fn=<MseLossBackward>)\n",
      "78 tensor(1625.9651, grad_fn=<MseLossBackward>)\n",
      "79 tensor(1523.5798, grad_fn=<MseLossBackward>)\n",
      "80 tensor(1428.0505, grad_fn=<MseLossBackward>)\n",
      "81 tensor(1338.8380, grad_fn=<MseLossBackward>)\n",
      "82 tensor(1255.5117, grad_fn=<MseLossBackward>)\n",
      "83 tensor(1177.6316, grad_fn=<MseLossBackward>)\n",
      "84 tensor(1104.8341, grad_fn=<MseLossBackward>)\n",
      "85 tensor(1036.7725, grad_fn=<MseLossBackward>)\n",
      "86 tensor(973.1274, grad_fn=<MseLossBackward>)\n",
      "87 tensor(913.5891, grad_fn=<MseLossBackward>)\n",
      "88 tensor(857.8588, grad_fn=<MseLossBackward>)\n",
      "89 tensor(805.6956, grad_fn=<MseLossBackward>)\n",
      "90 tensor(756.8569, grad_fn=<MseLossBackward>)\n",
      "91 tensor(711.1191, grad_fn=<MseLossBackward>)\n",
      "92 tensor(668.2827, grad_fn=<MseLossBackward>)\n",
      "93 tensor(628.1303, grad_fn=<MseLossBackward>)\n",
      "94 tensor(590.4935, grad_fn=<MseLossBackward>)\n",
      "95 tensor(555.2141, grad_fn=<MseLossBackward>)\n",
      "96 tensor(522.1541, grad_fn=<MseLossBackward>)\n",
      "97 tensor(491.1372, grad_fn=<MseLossBackward>)\n",
      "98 tensor(462.0371, grad_fn=<MseLossBackward>)\n",
      "99 tensor(434.7313, grad_fn=<MseLossBackward>)\n",
      "100 tensor(409.1066, grad_fn=<MseLossBackward>)\n",
      "101 tensor(385.0530, grad_fn=<MseLossBackward>)\n",
      "102 tensor(362.4694, grad_fn=<MseLossBackward>)\n",
      "103 tensor(341.2673, grad_fn=<MseLossBackward>)\n",
      "104 tensor(321.3564, grad_fn=<MseLossBackward>)\n",
      "105 tensor(302.6508, grad_fn=<MseLossBackward>)\n",
      "106 tensor(285.0708, grad_fn=<MseLossBackward>)\n",
      "107 tensor(268.5623, grad_fn=<MseLossBackward>)\n",
      "108 tensor(253.0510, grad_fn=<MseLossBackward>)\n",
      "109 tensor(238.4674, grad_fn=<MseLossBackward>)\n",
      "110 tensor(224.7561, grad_fn=<MseLossBackward>)\n",
      "111 tensor(211.8609, grad_fn=<MseLossBackward>)\n",
      "112 tensor(199.7285, grad_fn=<MseLossBackward>)\n",
      "113 tensor(188.3181, grad_fn=<MseLossBackward>)\n",
      "114 tensor(177.5821, grad_fn=<MseLossBackward>)\n",
      "115 tensor(167.4803, grad_fn=<MseLossBackward>)\n",
      "116 tensor(157.9717, grad_fn=<MseLossBackward>)\n",
      "117 tensor(149.0239, grad_fn=<MseLossBackward>)\n",
      "118 tensor(140.6039, grad_fn=<MseLossBackward>)\n",
      "119 tensor(132.6695, grad_fn=<MseLossBackward>)\n",
      "120 tensor(125.1987, grad_fn=<MseLossBackward>)\n",
      "121 tensor(118.1664, grad_fn=<MseLossBackward>)\n",
      "122 tensor(111.5433, grad_fn=<MseLossBackward>)\n",
      "123 tensor(105.3055, grad_fn=<MseLossBackward>)\n",
      "124 tensor(99.4292, grad_fn=<MseLossBackward>)\n",
      "125 tensor(93.8886, grad_fn=<MseLossBackward>)\n",
      "126 tensor(88.6667, grad_fn=<MseLossBackward>)\n",
      "127 tensor(83.7430, grad_fn=<MseLossBackward>)\n",
      "128 tensor(79.1007, grad_fn=<MseLossBackward>)\n",
      "129 tensor(74.7257, grad_fn=<MseLossBackward>)\n",
      "130 tensor(70.6004, grad_fn=<MseLossBackward>)\n",
      "131 tensor(66.7098, grad_fn=<MseLossBackward>)\n",
      "132 tensor(63.0383, grad_fn=<MseLossBackward>)\n",
      "133 tensor(59.5773, grad_fn=<MseLossBackward>)\n",
      "134 tensor(56.3111, grad_fn=<MseLossBackward>)\n",
      "135 tensor(53.2267, grad_fn=<MseLossBackward>)\n",
      "136 tensor(50.3184, grad_fn=<MseLossBackward>)\n",
      "137 tensor(47.5728, grad_fn=<MseLossBackward>)\n",
      "138 tensor(44.9814, grad_fn=<MseLossBackward>)\n",
      "139 tensor(42.5352, grad_fn=<MseLossBackward>)\n",
      "140 tensor(40.2249, grad_fn=<MseLossBackward>)\n",
      "141 tensor(38.0452, grad_fn=<MseLossBackward>)\n",
      "142 tensor(35.9867, grad_fn=<MseLossBackward>)\n",
      "143 tensor(34.0422, grad_fn=<MseLossBackward>)\n",
      "144 tensor(32.2055, grad_fn=<MseLossBackward>)\n",
      "145 tensor(30.4701, grad_fn=<MseLossBackward>)\n",
      "146 tensor(28.8306, grad_fn=<MseLossBackward>)\n",
      "147 tensor(27.2820, grad_fn=<MseLossBackward>)\n",
      "148 tensor(25.8183, grad_fn=<MseLossBackward>)\n",
      "149 tensor(24.4368, grad_fn=<MseLossBackward>)\n",
      "150 tensor(23.1306, grad_fn=<MseLossBackward>)\n",
      "151 tensor(21.8956, grad_fn=<MseLossBackward>)\n",
      "152 tensor(20.7284, grad_fn=<MseLossBackward>)\n",
      "153 tensor(19.6247, grad_fn=<MseLossBackward>)\n",
      "154 tensor(18.5820, grad_fn=<MseLossBackward>)\n",
      "155 tensor(17.5954, grad_fn=<MseLossBackward>)\n",
      "156 tensor(16.6624, grad_fn=<MseLossBackward>)\n",
      "157 tensor(15.7795, grad_fn=<MseLossBackward>)\n",
      "158 tensor(14.9454, grad_fn=<MseLossBackward>)\n",
      "159 tensor(14.1564, grad_fn=<MseLossBackward>)\n",
      "160 tensor(13.4100, grad_fn=<MseLossBackward>)\n",
      "161 tensor(12.7041, grad_fn=<MseLossBackward>)\n",
      "162 tensor(12.0357, grad_fn=<MseLossBackward>)\n",
      "163 tensor(11.4035, grad_fn=<MseLossBackward>)\n",
      "164 tensor(10.8058, grad_fn=<MseLossBackward>)\n",
      "165 tensor(10.2404, grad_fn=<MseLossBackward>)\n",
      "166 tensor(9.7046, grad_fn=<MseLossBackward>)\n",
      "167 tensor(9.1977, grad_fn=<MseLossBackward>)\n",
      "168 tensor(8.7177, grad_fn=<MseLossBackward>)\n",
      "169 tensor(8.2635, grad_fn=<MseLossBackward>)\n",
      "170 tensor(7.8331, grad_fn=<MseLossBackward>)\n",
      "171 tensor(7.4263, grad_fn=<MseLossBackward>)\n",
      "172 tensor(7.0405, grad_fn=<MseLossBackward>)\n",
      "173 tensor(6.6755, grad_fn=<MseLossBackward>)\n",
      "174 tensor(6.3297, grad_fn=<MseLossBackward>)\n",
      "175 tensor(6.0022, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176 tensor(5.6920, grad_fn=<MseLossBackward>)\n",
      "177 tensor(5.3984, grad_fn=<MseLossBackward>)\n",
      "178 tensor(5.1203, grad_fn=<MseLossBackward>)\n",
      "179 tensor(4.8565, grad_fn=<MseLossBackward>)\n",
      "180 tensor(4.6069, grad_fn=<MseLossBackward>)\n",
      "181 tensor(4.3702, grad_fn=<MseLossBackward>)\n",
      "182 tensor(4.1460, grad_fn=<MseLossBackward>)\n",
      "183 tensor(3.9335, grad_fn=<MseLossBackward>)\n",
      "184 tensor(3.7320, grad_fn=<MseLossBackward>)\n",
      "185 tensor(3.5414, grad_fn=<MseLossBackward>)\n",
      "186 tensor(3.3606, grad_fn=<MseLossBackward>)\n",
      "187 tensor(3.1892, grad_fn=<MseLossBackward>)\n",
      "188 tensor(3.0265, grad_fn=<MseLossBackward>)\n",
      "189 tensor(2.8726, grad_fn=<MseLossBackward>)\n",
      "190 tensor(2.7264, grad_fn=<MseLossBackward>)\n",
      "191 tensor(2.5879, grad_fn=<MseLossBackward>)\n",
      "192 tensor(2.4566, grad_fn=<MseLossBackward>)\n",
      "193 tensor(2.3320, grad_fn=<MseLossBackward>)\n",
      "194 tensor(2.2141, grad_fn=<MseLossBackward>)\n",
      "195 tensor(2.1018, grad_fn=<MseLossBackward>)\n",
      "196 tensor(1.9958, grad_fn=<MseLossBackward>)\n",
      "197 tensor(1.8951, grad_fn=<MseLossBackward>)\n",
      "198 tensor(1.7994, grad_fn=<MseLossBackward>)\n",
      "199 tensor(1.7089, grad_fn=<MseLossBackward>)\n",
      "200 tensor(1.6227, grad_fn=<MseLossBackward>)\n",
      "201 tensor(1.5413, grad_fn=<MseLossBackward>)\n",
      "202 tensor(1.4639, grad_fn=<MseLossBackward>)\n",
      "203 tensor(1.3904, grad_fn=<MseLossBackward>)\n",
      "204 tensor(1.3206, grad_fn=<MseLossBackward>)\n",
      "205 tensor(1.2545, grad_fn=<MseLossBackward>)\n",
      "206 tensor(1.1917, grad_fn=<MseLossBackward>)\n",
      "207 tensor(1.1323, grad_fn=<MseLossBackward>)\n",
      "208 tensor(1.0756, grad_fn=<MseLossBackward>)\n",
      "209 tensor(1.0219, grad_fn=<MseLossBackward>)\n",
      "210 tensor(0.9710, grad_fn=<MseLossBackward>)\n",
      "211 tensor(0.9227, grad_fn=<MseLossBackward>)\n",
      "212 tensor(0.8768, grad_fn=<MseLossBackward>)\n",
      "213 tensor(0.8332, grad_fn=<MseLossBackward>)\n",
      "214 tensor(0.7919, grad_fn=<MseLossBackward>)\n",
      "215 tensor(0.7526, grad_fn=<MseLossBackward>)\n",
      "216 tensor(0.7153, grad_fn=<MseLossBackward>)\n",
      "217 tensor(0.6799, grad_fn=<MseLossBackward>)\n",
      "218 tensor(0.6462, grad_fn=<MseLossBackward>)\n",
      "219 tensor(0.6143, grad_fn=<MseLossBackward>)\n",
      "220 tensor(0.5839, grad_fn=<MseLossBackward>)\n",
      "221 tensor(0.5551, grad_fn=<MseLossBackward>)\n",
      "222 tensor(0.5277, grad_fn=<MseLossBackward>)\n",
      "223 tensor(0.5017, grad_fn=<MseLossBackward>)\n",
      "224 tensor(0.4771, grad_fn=<MseLossBackward>)\n",
      "225 tensor(0.4536, grad_fn=<MseLossBackward>)\n",
      "226 tensor(0.4313, grad_fn=<MseLossBackward>)\n",
      "227 tensor(0.4101, grad_fn=<MseLossBackward>)\n",
      "228 tensor(0.3900, grad_fn=<MseLossBackward>)\n",
      "229 tensor(0.3710, grad_fn=<MseLossBackward>)\n",
      "230 tensor(0.3528, grad_fn=<MseLossBackward>)\n",
      "231 tensor(0.3354, grad_fn=<MseLossBackward>)\n",
      "232 tensor(0.3190, grad_fn=<MseLossBackward>)\n",
      "233 tensor(0.3035, grad_fn=<MseLossBackward>)\n",
      "234 tensor(0.2886, grad_fn=<MseLossBackward>)\n",
      "235 tensor(0.2746, grad_fn=<MseLossBackward>)\n",
      "236 tensor(0.2612, grad_fn=<MseLossBackward>)\n",
      "237 tensor(0.2485, grad_fn=<MseLossBackward>)\n",
      "238 tensor(0.2364, grad_fn=<MseLossBackward>)\n",
      "239 tensor(0.2249, grad_fn=<MseLossBackward>)\n",
      "240 tensor(0.2139, grad_fn=<MseLossBackward>)\n",
      "241 tensor(0.2036, grad_fn=<MseLossBackward>)\n",
      "242 tensor(0.1937, grad_fn=<MseLossBackward>)\n",
      "243 tensor(0.1843, grad_fn=<MseLossBackward>)\n",
      "244 tensor(0.1754, grad_fn=<MseLossBackward>)\n",
      "245 tensor(0.1669, grad_fn=<MseLossBackward>)\n",
      "246 tensor(0.1588, grad_fn=<MseLossBackward>)\n",
      "247 tensor(0.1512, grad_fn=<MseLossBackward>)\n",
      "248 tensor(0.1438, grad_fn=<MseLossBackward>)\n",
      "249 tensor(0.1369, grad_fn=<MseLossBackward>)\n",
      "250 tensor(0.1303, grad_fn=<MseLossBackward>)\n",
      "251 tensor(0.1240, grad_fn=<MseLossBackward>)\n",
      "252 tensor(0.1180, grad_fn=<MseLossBackward>)\n",
      "253 tensor(0.1124, grad_fn=<MseLossBackward>)\n",
      "254 tensor(0.1070, grad_fn=<MseLossBackward>)\n",
      "255 tensor(0.1018, grad_fn=<MseLossBackward>)\n",
      "256 tensor(0.0970, grad_fn=<MseLossBackward>)\n",
      "257 tensor(0.0922, grad_fn=<MseLossBackward>)\n",
      "258 tensor(0.0878, grad_fn=<MseLossBackward>)\n",
      "259 tensor(0.0837, grad_fn=<MseLossBackward>)\n",
      "260 tensor(0.0796, grad_fn=<MseLossBackward>)\n",
      "261 tensor(0.0758, grad_fn=<MseLossBackward>)\n",
      "262 tensor(0.0722, grad_fn=<MseLossBackward>)\n",
      "263 tensor(0.0688, grad_fn=<MseLossBackward>)\n",
      "264 tensor(0.0655, grad_fn=<MseLossBackward>)\n",
      "265 tensor(0.0624, grad_fn=<MseLossBackward>)\n",
      "266 tensor(0.0594, grad_fn=<MseLossBackward>)\n",
      "267 tensor(0.0566, grad_fn=<MseLossBackward>)\n",
      "268 tensor(0.0539, grad_fn=<MseLossBackward>)\n",
      "269 tensor(0.0513, grad_fn=<MseLossBackward>)\n",
      "270 tensor(0.0489, grad_fn=<MseLossBackward>)\n",
      "271 tensor(0.0465, grad_fn=<MseLossBackward>)\n",
      "272 tensor(0.0443, grad_fn=<MseLossBackward>)\n",
      "273 tensor(0.0422, grad_fn=<MseLossBackward>)\n",
      "274 tensor(0.0402, grad_fn=<MseLossBackward>)\n",
      "275 tensor(0.0383, grad_fn=<MseLossBackward>)\n",
      "276 tensor(0.0365, grad_fn=<MseLossBackward>)\n",
      "277 tensor(0.0348, grad_fn=<MseLossBackward>)\n",
      "278 tensor(0.0332, grad_fn=<MseLossBackward>)\n",
      "279 tensor(0.0316, grad_fn=<MseLossBackward>)\n",
      "280 tensor(0.0301, grad_fn=<MseLossBackward>)\n",
      "281 tensor(0.0287, grad_fn=<MseLossBackward>)\n",
      "282 tensor(0.0273, grad_fn=<MseLossBackward>)\n",
      "283 tensor(0.0261, grad_fn=<MseLossBackward>)\n",
      "284 tensor(0.0248, grad_fn=<MseLossBackward>)\n",
      "285 tensor(0.0237, grad_fn=<MseLossBackward>)\n",
      "286 tensor(0.0226, grad_fn=<MseLossBackward>)\n",
      "287 tensor(0.0215, grad_fn=<MseLossBackward>)\n",
      "288 tensor(0.0205, grad_fn=<MseLossBackward>)\n",
      "289 tensor(0.0196, grad_fn=<MseLossBackward>)\n",
      "290 tensor(0.0187, grad_fn=<MseLossBackward>)\n",
      "291 tensor(0.0178, grad_fn=<MseLossBackward>)\n",
      "292 tensor(0.0170, grad_fn=<MseLossBackward>)\n",
      "293 tensor(0.0162, grad_fn=<MseLossBackward>)\n",
      "294 tensor(0.0154, grad_fn=<MseLossBackward>)\n",
      "295 tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "296 tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "297 tensor(0.0134, grad_fn=<MseLossBackward>)\n",
      "298 tensor(0.0128, grad_fn=<MseLossBackward>)\n",
      "299 tensor(0.0122, grad_fn=<MseLossBackward>)\n",
      "300 tensor(0.0116, grad_fn=<MseLossBackward>)\n",
      "301 tensor(0.0111, grad_fn=<MseLossBackward>)\n",
      "302 tensor(0.0106, grad_fn=<MseLossBackward>)\n",
      "303 tensor(0.0101, grad_fn=<MseLossBackward>)\n",
      "304 tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "305 tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "306 tensor(0.0088, grad_fn=<MseLossBackward>)\n",
      "307 tensor(0.0084, grad_fn=<MseLossBackward>)\n",
      "308 tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "309 tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "310 tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "311 tensor(0.0070, grad_fn=<MseLossBackward>)\n",
      "312 tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "313 tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "314 tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "315 tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "316 tensor(0.0056, grad_fn=<MseLossBackward>)\n",
      "317 tensor(0.0053, grad_fn=<MseLossBackward>)\n",
      "318 tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "319 tensor(0.0049, grad_fn=<MseLossBackward>)\n",
      "320 tensor(0.0047, grad_fn=<MseLossBackward>)\n",
      "321 tensor(0.0045, grad_fn=<MseLossBackward>)\n",
      "322 tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "323 tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "324 tensor(0.0039, grad_fn=<MseLossBackward>)\n",
      "325 tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "326 tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "327 tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "328 tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "329 tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "330 tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "331 tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "332 tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "333 tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "334 tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "335 tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "336 tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "337 tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "338 tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "339 tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "340 tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "341 tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "342 tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "343 tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "344 tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "345 tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "346 tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "347 tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "348 tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "349 tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "350 tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "351 tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "352 tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "353 tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "354 tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "355 tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "356 tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "357 tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "358 tensor(0.0010, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359 tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "360 tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "361 tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "362 tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "363 tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "364 tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "365 tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "366 tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "367 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "368 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "369 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "370 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "371 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "372 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "373 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "374 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "375 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "376 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "377 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "378 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "379 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "380 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "381 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "382 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "383 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "384 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "385 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "386 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "387 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "388 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "389 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "390 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "391 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "392 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "393 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "394 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "395 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "396 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "397 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "398 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "399 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "400 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "401 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "402 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "403 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "404 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "405 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "406 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "407 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "408 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "409 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "410 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "411 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "412 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "413 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "414 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "415 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "416 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "417 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "418 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "419 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "420 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "421 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "422 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "423 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "424 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "425 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "426 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "427 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "428 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "429 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "430 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "431 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "432 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "433 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "434 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "435 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "436 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "437 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "438 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "439 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "440 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "441 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "442 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "443 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "444 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "445 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "446 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "447 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "448 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "449 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "450 tensor(9.8901e-05, grad_fn=<MseLossBackward>)\n",
      "451 tensor(9.7013e-05, grad_fn=<MseLossBackward>)\n",
      "452 tensor(9.5608e-05, grad_fn=<MseLossBackward>)\n",
      "453 tensor(9.3902e-05, grad_fn=<MseLossBackward>)\n",
      "454 tensor(9.2477e-05, grad_fn=<MseLossBackward>)\n",
      "455 tensor(9.0666e-05, grad_fn=<MseLossBackward>)\n",
      "456 tensor(8.9105e-05, grad_fn=<MseLossBackward>)\n",
      "457 tensor(8.7595e-05, grad_fn=<MseLossBackward>)\n",
      "458 tensor(8.6055e-05, grad_fn=<MseLossBackward>)\n",
      "459 tensor(8.4853e-05, grad_fn=<MseLossBackward>)\n",
      "460 tensor(8.3569e-05, grad_fn=<MseLossBackward>)\n",
      "461 tensor(8.1902e-05, grad_fn=<MseLossBackward>)\n",
      "462 tensor(8.0471e-05, grad_fn=<MseLossBackward>)\n",
      "463 tensor(7.9241e-05, grad_fn=<MseLossBackward>)\n",
      "464 tensor(7.8084e-05, grad_fn=<MseLossBackward>)\n",
      "465 tensor(7.6908e-05, grad_fn=<MseLossBackward>)\n",
      "466 tensor(7.5813e-05, grad_fn=<MseLossBackward>)\n",
      "467 tensor(7.4333e-05, grad_fn=<MseLossBackward>)\n",
      "468 tensor(7.3331e-05, grad_fn=<MseLossBackward>)\n",
      "469 tensor(7.1727e-05, grad_fn=<MseLossBackward>)\n",
      "470 tensor(7.0929e-05, grad_fn=<MseLossBackward>)\n",
      "471 tensor(6.9815e-05, grad_fn=<MseLossBackward>)\n",
      "472 tensor(6.8647e-05, grad_fn=<MseLossBackward>)\n",
      "473 tensor(6.7617e-05, grad_fn=<MseLossBackward>)\n",
      "474 tensor(6.6645e-05, grad_fn=<MseLossBackward>)\n",
      "475 tensor(6.5866e-05, grad_fn=<MseLossBackward>)\n",
      "476 tensor(6.4658e-05, grad_fn=<MseLossBackward>)\n",
      "477 tensor(6.3764e-05, grad_fn=<MseLossBackward>)\n",
      "478 tensor(6.2981e-05, grad_fn=<MseLossBackward>)\n",
      "479 tensor(6.2045e-05, grad_fn=<MseLossBackward>)\n",
      "480 tensor(6.1184e-05, grad_fn=<MseLossBackward>)\n",
      "481 tensor(6.0231e-05, grad_fn=<MseLossBackward>)\n",
      "482 tensor(5.9315e-05, grad_fn=<MseLossBackward>)\n",
      "483 tensor(5.8451e-05, grad_fn=<MseLossBackward>)\n",
      "484 tensor(5.7601e-05, grad_fn=<MseLossBackward>)\n",
      "485 tensor(5.7088e-05, grad_fn=<MseLossBackward>)\n",
      "486 tensor(5.6071e-05, grad_fn=<MseLossBackward>)\n",
      "487 tensor(5.5250e-05, grad_fn=<MseLossBackward>)\n",
      "488 tensor(5.4557e-05, grad_fn=<MseLossBackward>)\n",
      "489 tensor(5.3938e-05, grad_fn=<MseLossBackward>)\n",
      "490 tensor(5.3499e-05, grad_fn=<MseLossBackward>)\n",
      "491 tensor(5.2688e-05, grad_fn=<MseLossBackward>)\n",
      "492 tensor(5.1811e-05, grad_fn=<MseLossBackward>)\n",
      "493 tensor(5.1033e-05, grad_fn=<MseLossBackward>)\n",
      "494 tensor(5.0399e-05, grad_fn=<MseLossBackward>)\n",
      "495 tensor(4.9729e-05, grad_fn=<MseLossBackward>)\n",
      "496 tensor(4.9074e-05, grad_fn=<MseLossBackward>)\n",
      "497 tensor(4.8256e-05, grad_fn=<MseLossBackward>)\n",
      "498 tensor(4.7746e-05, grad_fn=<MseLossBackward>)\n",
      "499 tensor(4.7076e-05, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机初始化训练数据 \n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "torch.nn.init.normal_(model[0].weight)\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "lr = 1e-6\n",
    "\n",
    "for iter in range(500):\n",
    "    y_pred = model(x) #相当于forward pass\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(iter, loss)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= lr * param.grad\n",
    "            \n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4f706eeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T06:17:41.202110Z",
     "start_time": "2021-09-04T06:17:41.197609Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1000, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "06c96c7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T06:18:04.717557Z",
     "start_time": "2021-09-04T06:18:04.711538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-1.1343e-02,  2.4281e-02,  2.1467e-02,  1.2758e-02,  2.3840e-02,\n",
       "         5.3177e-03,  2.6771e-02, -1.8295e-02,  2.6719e-02,  1.3791e-02,\n",
       "         2.1314e-02, -2.7342e-02, -2.7310e-02, -1.5360e-02,  1.7138e-02,\n",
       "        -7.7319e-03, -1.6765e-02,  6.8085e-03, -7.9364e-03,  7.2564e-03,\n",
       "        -2.3451e-02, -1.1289e-02, -3.0989e-02, -1.1127e-02,  2.4488e-02,\n",
       "        -1.1992e-02,  2.1911e-02, -2.5786e-02, -1.1258e-02, -9.0892e-03,\n",
       "        -2.3420e-02, -2.6882e-02,  1.3722e-02,  1.3681e-02,  1.0643e-02,\n",
       "        -2.5641e-02, -1.2618e-02,  2.3307e-03, -3.0101e-02,  1.1261e-02,\n",
       "         1.6102e-02, -7.1478e-03, -1.3907e-02,  1.7876e-02, -2.1592e-02,\n",
       "        -1.6141e-02, -1.3660e-02,  2.4899e-02, -9.1297e-03, -2.5465e-03,\n",
       "        -2.9484e-04,  7.3234e-03, -3.2386e-02, -1.0035e-02,  1.1539e-02,\n",
       "         2.7405e-02, -8.9893e-03, -2.8333e-02,  3.1081e-02,  2.7402e-02,\n",
       "         2.0421e-02,  2.9589e-02,  1.1592e-03, -1.2858e-02,  9.1868e-03,\n",
       "         3.1584e-02,  2.4645e-02,  1.2261e-02, -1.3060e-03,  5.8505e-03,\n",
       "         9.4933e-03, -2.8925e-03, -2.6573e-03, -2.1989e-02, -7.5893e-04,\n",
       "         9.4575e-03, -2.4353e-02,  2.2772e-02,  3.1915e-03,  2.2625e-02,\n",
       "         1.9451e-02,  3.0693e-02, -2.4634e-02, -1.5797e-03, -2.7794e-03,\n",
       "        -2.9711e-02, -1.9297e-02, -6.0080e-03,  2.1110e-04,  2.7409e-02,\n",
       "         1.5398e-02,  6.8228e-05,  1.3179e-02, -1.8494e-02, -1.0892e-02,\n",
       "         1.4240e-02, -2.7034e-02,  2.8315e-02, -1.7493e-02, -9.3877e-03],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ba2b69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T06:22:01.946706Z",
     "start_time": "2021-09-04T06:22:01.939011Z"
    }
   },
   "source": [
    "简单两层神经网络，pytorch1，用optimizer\n",
    "---------\n",
    "- $h = W_1X$\n",
    "- $a = max(0, h)$\n",
    "- $y_{hat} = W_2a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cf4787e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T06:24:45.245961Z",
     "start_time": "2021-09-04T06:24:41.774297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(614.3233, grad_fn=<MseLossBackward>)\n",
      "1 tensor(598.0545, grad_fn=<MseLossBackward>)\n",
      "2 tensor(582.3097, grad_fn=<MseLossBackward>)\n",
      "3 tensor(566.9987, grad_fn=<MseLossBackward>)\n",
      "4 tensor(552.0994, grad_fn=<MseLossBackward>)\n",
      "5 tensor(537.5602, grad_fn=<MseLossBackward>)\n",
      "6 tensor(523.4012, grad_fn=<MseLossBackward>)\n",
      "7 tensor(509.6110, grad_fn=<MseLossBackward>)\n",
      "8 tensor(496.1618, grad_fn=<MseLossBackward>)\n",
      "9 tensor(483.0812, grad_fn=<MseLossBackward>)\n",
      "10 tensor(470.4288, grad_fn=<MseLossBackward>)\n",
      "11 tensor(458.1928, grad_fn=<MseLossBackward>)\n",
      "12 tensor(446.3528, grad_fn=<MseLossBackward>)\n",
      "13 tensor(434.8663, grad_fn=<MseLossBackward>)\n",
      "14 tensor(423.7292, grad_fn=<MseLossBackward>)\n",
      "15 tensor(412.9403, grad_fn=<MseLossBackward>)\n",
      "16 tensor(402.4753, grad_fn=<MseLossBackward>)\n",
      "17 tensor(392.3595, grad_fn=<MseLossBackward>)\n",
      "18 tensor(382.5652, grad_fn=<MseLossBackward>)\n",
      "19 tensor(373.0242, grad_fn=<MseLossBackward>)\n",
      "20 tensor(363.7207, grad_fn=<MseLossBackward>)\n",
      "21 tensor(354.6638, grad_fn=<MseLossBackward>)\n",
      "22 tensor(345.8987, grad_fn=<MseLossBackward>)\n",
      "23 tensor(337.3251, grad_fn=<MseLossBackward>)\n",
      "24 tensor(328.9990, grad_fn=<MseLossBackward>)\n",
      "25 tensor(320.8878, grad_fn=<MseLossBackward>)\n",
      "26 tensor(312.9896, grad_fn=<MseLossBackward>)\n",
      "27 tensor(305.2907, grad_fn=<MseLossBackward>)\n",
      "28 tensor(297.8101, grad_fn=<MseLossBackward>)\n",
      "29 tensor(290.5413, grad_fn=<MseLossBackward>)\n",
      "30 tensor(283.4284, grad_fn=<MseLossBackward>)\n",
      "31 tensor(276.4749, grad_fn=<MseLossBackward>)\n",
      "32 tensor(269.6983, grad_fn=<MseLossBackward>)\n",
      "33 tensor(263.1077, grad_fn=<MseLossBackward>)\n",
      "34 tensor(256.6809, grad_fn=<MseLossBackward>)\n",
      "35 tensor(250.4060, grad_fn=<MseLossBackward>)\n",
      "36 tensor(244.2914, grad_fn=<MseLossBackward>)\n",
      "37 tensor(238.2941, grad_fn=<MseLossBackward>)\n",
      "38 tensor(232.4403, grad_fn=<MseLossBackward>)\n",
      "39 tensor(226.7309, grad_fn=<MseLossBackward>)\n",
      "40 tensor(221.1422, grad_fn=<MseLossBackward>)\n",
      "41 tensor(215.6909, grad_fn=<MseLossBackward>)\n",
      "42 tensor(210.3683, grad_fn=<MseLossBackward>)\n",
      "43 tensor(205.1725, grad_fn=<MseLossBackward>)\n",
      "44 tensor(200.0995, grad_fn=<MseLossBackward>)\n",
      "45 tensor(195.1492, grad_fn=<MseLossBackward>)\n",
      "46 tensor(190.3230, grad_fn=<MseLossBackward>)\n",
      "47 tensor(185.6100, grad_fn=<MseLossBackward>)\n",
      "48 tensor(180.9846, grad_fn=<MseLossBackward>)\n",
      "49 tensor(176.4456, grad_fn=<MseLossBackward>)\n",
      "50 tensor(172.0015, grad_fn=<MseLossBackward>)\n",
      "51 tensor(167.6481, grad_fn=<MseLossBackward>)\n",
      "52 tensor(163.3908, grad_fn=<MseLossBackward>)\n",
      "53 tensor(159.2151, grad_fn=<MseLossBackward>)\n",
      "54 tensor(155.1208, grad_fn=<MseLossBackward>)\n",
      "55 tensor(151.1079, grad_fn=<MseLossBackward>)\n",
      "56 tensor(147.1795, grad_fn=<MseLossBackward>)\n",
      "57 tensor(143.3357, grad_fn=<MseLossBackward>)\n",
      "58 tensor(139.5708, grad_fn=<MseLossBackward>)\n",
      "59 tensor(135.8833, grad_fn=<MseLossBackward>)\n",
      "60 tensor(132.2723, grad_fn=<MseLossBackward>)\n",
      "61 tensor(128.7352, grad_fn=<MseLossBackward>)\n",
      "62 tensor(125.2775, grad_fn=<MseLossBackward>)\n",
      "63 tensor(121.8966, grad_fn=<MseLossBackward>)\n",
      "64 tensor(118.5955, grad_fn=<MseLossBackward>)\n",
      "65 tensor(115.3771, grad_fn=<MseLossBackward>)\n",
      "66 tensor(112.2272, grad_fn=<MseLossBackward>)\n",
      "67 tensor(109.1472, grad_fn=<MseLossBackward>)\n",
      "68 tensor(106.1413, grad_fn=<MseLossBackward>)\n",
      "69 tensor(103.1974, grad_fn=<MseLossBackward>)\n",
      "70 tensor(100.3264, grad_fn=<MseLossBackward>)\n",
      "71 tensor(97.5148, grad_fn=<MseLossBackward>)\n",
      "72 tensor(94.7736, grad_fn=<MseLossBackward>)\n",
      "73 tensor(92.1025, grad_fn=<MseLossBackward>)\n",
      "74 tensor(89.4949, grad_fn=<MseLossBackward>)\n",
      "75 tensor(86.9416, grad_fn=<MseLossBackward>)\n",
      "76 tensor(84.4487, grad_fn=<MseLossBackward>)\n",
      "77 tensor(82.0145, grad_fn=<MseLossBackward>)\n",
      "78 tensor(79.6380, grad_fn=<MseLossBackward>)\n",
      "79 tensor(77.3241, grad_fn=<MseLossBackward>)\n",
      "80 tensor(75.0679, grad_fn=<MseLossBackward>)\n",
      "81 tensor(72.8630, grad_fn=<MseLossBackward>)\n",
      "82 tensor(70.7129, grad_fn=<MseLossBackward>)\n",
      "83 tensor(68.6149, grad_fn=<MseLossBackward>)\n",
      "84 tensor(66.5680, grad_fn=<MseLossBackward>)\n",
      "85 tensor(64.5724, grad_fn=<MseLossBackward>)\n",
      "86 tensor(62.6274, grad_fn=<MseLossBackward>)\n",
      "87 tensor(60.7325, grad_fn=<MseLossBackward>)\n",
      "88 tensor(58.8830, grad_fn=<MseLossBackward>)\n",
      "89 tensor(57.0819, grad_fn=<MseLossBackward>)\n",
      "90 tensor(55.3265, grad_fn=<MseLossBackward>)\n",
      "91 tensor(53.6137, grad_fn=<MseLossBackward>)\n",
      "92 tensor(51.9434, grad_fn=<MseLossBackward>)\n",
      "93 tensor(50.3222, grad_fn=<MseLossBackward>)\n",
      "94 tensor(48.7408, grad_fn=<MseLossBackward>)\n",
      "95 tensor(47.2043, grad_fn=<MseLossBackward>)\n",
      "96 tensor(45.7065, grad_fn=<MseLossBackward>)\n",
      "97 tensor(44.2435, grad_fn=<MseLossBackward>)\n",
      "98 tensor(42.8178, grad_fn=<MseLossBackward>)\n",
      "99 tensor(41.4284, grad_fn=<MseLossBackward>)\n",
      "100 tensor(40.0746, grad_fn=<MseLossBackward>)\n",
      "101 tensor(38.7557, grad_fn=<MseLossBackward>)\n",
      "102 tensor(37.4721, grad_fn=<MseLossBackward>)\n",
      "103 tensor(36.2259, grad_fn=<MseLossBackward>)\n",
      "104 tensor(35.0143, grad_fn=<MseLossBackward>)\n",
      "105 tensor(33.8357, grad_fn=<MseLossBackward>)\n",
      "106 tensor(32.6888, grad_fn=<MseLossBackward>)\n",
      "107 tensor(31.5745, grad_fn=<MseLossBackward>)\n",
      "108 tensor(30.4920, grad_fn=<MseLossBackward>)\n",
      "109 tensor(29.4388, grad_fn=<MseLossBackward>)\n",
      "110 tensor(28.4176, grad_fn=<MseLossBackward>)\n",
      "111 tensor(27.4258, grad_fn=<MseLossBackward>)\n",
      "112 tensor(26.4614, grad_fn=<MseLossBackward>)\n",
      "113 tensor(25.5253, grad_fn=<MseLossBackward>)\n",
      "114 tensor(24.6187, grad_fn=<MseLossBackward>)\n",
      "115 tensor(23.7387, grad_fn=<MseLossBackward>)\n",
      "116 tensor(22.8857, grad_fn=<MseLossBackward>)\n",
      "117 tensor(22.0600, grad_fn=<MseLossBackward>)\n",
      "118 tensor(21.2580, grad_fn=<MseLossBackward>)\n",
      "119 tensor(20.4796, grad_fn=<MseLossBackward>)\n",
      "120 tensor(19.7254, grad_fn=<MseLossBackward>)\n",
      "121 tensor(18.9948, grad_fn=<MseLossBackward>)\n",
      "122 tensor(18.2857, grad_fn=<MseLossBackward>)\n",
      "123 tensor(17.6001, grad_fn=<MseLossBackward>)\n",
      "124 tensor(16.9347, grad_fn=<MseLossBackward>)\n",
      "125 tensor(16.2911, grad_fn=<MseLossBackward>)\n",
      "126 tensor(15.6694, grad_fn=<MseLossBackward>)\n",
      "127 tensor(15.0681, grad_fn=<MseLossBackward>)\n",
      "128 tensor(14.4865, grad_fn=<MseLossBackward>)\n",
      "129 tensor(13.9234, grad_fn=<MseLossBackward>)\n",
      "130 tensor(13.3807, grad_fn=<MseLossBackward>)\n",
      "131 tensor(12.8564, grad_fn=<MseLossBackward>)\n",
      "132 tensor(12.3487, grad_fn=<MseLossBackward>)\n",
      "133 tensor(11.8588, grad_fn=<MseLossBackward>)\n",
      "134 tensor(11.3871, grad_fn=<MseLossBackward>)\n",
      "135 tensor(10.9316, grad_fn=<MseLossBackward>)\n",
      "136 tensor(10.4929, grad_fn=<MseLossBackward>)\n",
      "137 tensor(10.0690, grad_fn=<MseLossBackward>)\n",
      "138 tensor(9.6601, grad_fn=<MseLossBackward>)\n",
      "139 tensor(9.2657, grad_fn=<MseLossBackward>)\n",
      "140 tensor(8.8857, grad_fn=<MseLossBackward>)\n",
      "141 tensor(8.5195, grad_fn=<MseLossBackward>)\n",
      "142 tensor(8.1663, grad_fn=<MseLossBackward>)\n",
      "143 tensor(7.8262, grad_fn=<MseLossBackward>)\n",
      "144 tensor(7.4985, grad_fn=<MseLossBackward>)\n",
      "145 tensor(7.1831, grad_fn=<MseLossBackward>)\n",
      "146 tensor(6.8796, grad_fn=<MseLossBackward>)\n",
      "147 tensor(6.5880, grad_fn=<MseLossBackward>)\n",
      "148 tensor(6.3075, grad_fn=<MseLossBackward>)\n",
      "149 tensor(6.0382, grad_fn=<MseLossBackward>)\n",
      "150 tensor(5.7791, grad_fn=<MseLossBackward>)\n",
      "151 tensor(5.5303, grad_fn=<MseLossBackward>)\n",
      "152 tensor(5.2911, grad_fn=<MseLossBackward>)\n",
      "153 tensor(5.0614, grad_fn=<MseLossBackward>)\n",
      "154 tensor(4.8408, grad_fn=<MseLossBackward>)\n",
      "155 tensor(4.6290, grad_fn=<MseLossBackward>)\n",
      "156 tensor(4.4259, grad_fn=<MseLossBackward>)\n",
      "157 tensor(4.2310, grad_fn=<MseLossBackward>)\n",
      "158 tensor(4.0441, grad_fn=<MseLossBackward>)\n",
      "159 tensor(3.8648, grad_fn=<MseLossBackward>)\n",
      "160 tensor(3.6928, grad_fn=<MseLossBackward>)\n",
      "161 tensor(3.5280, grad_fn=<MseLossBackward>)\n",
      "162 tensor(3.3701, grad_fn=<MseLossBackward>)\n",
      "163 tensor(3.2188, grad_fn=<MseLossBackward>)\n",
      "164 tensor(3.0740, grad_fn=<MseLossBackward>)\n",
      "165 tensor(2.9354, grad_fn=<MseLossBackward>)\n",
      "166 tensor(2.8026, grad_fn=<MseLossBackward>)\n",
      "167 tensor(2.6755, grad_fn=<MseLossBackward>)\n",
      "168 tensor(2.5539, grad_fn=<MseLossBackward>)\n",
      "169 tensor(2.4377, grad_fn=<MseLossBackward>)\n",
      "170 tensor(2.3264, grad_fn=<MseLossBackward>)\n",
      "171 tensor(2.2202, grad_fn=<MseLossBackward>)\n",
      "172 tensor(2.1189, grad_fn=<MseLossBackward>)\n",
      "173 tensor(2.0219, grad_fn=<MseLossBackward>)\n",
      "174 tensor(1.9292, grad_fn=<MseLossBackward>)\n",
      "175 tensor(1.8406, grad_fn=<MseLossBackward>)\n",
      "176 tensor(1.7559, grad_fn=<MseLossBackward>)\n",
      "177 tensor(1.6750, grad_fn=<MseLossBackward>)\n",
      "178 tensor(1.5976, grad_fn=<MseLossBackward>)\n",
      "179 tensor(1.5237, grad_fn=<MseLossBackward>)\n",
      "180 tensor(1.4532, grad_fn=<MseLossBackward>)\n",
      "181 tensor(1.3858, grad_fn=<MseLossBackward>)\n",
      "182 tensor(1.3215, grad_fn=<MseLossBackward>)\n",
      "183 tensor(1.2600, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184 tensor(1.2014, grad_fn=<MseLossBackward>)\n",
      "185 tensor(1.1455, grad_fn=<MseLossBackward>)\n",
      "186 tensor(1.0921, grad_fn=<MseLossBackward>)\n",
      "187 tensor(1.0413, grad_fn=<MseLossBackward>)\n",
      "188 tensor(0.9927, grad_fn=<MseLossBackward>)\n",
      "189 tensor(0.9465, grad_fn=<MseLossBackward>)\n",
      "190 tensor(0.9024, grad_fn=<MseLossBackward>)\n",
      "191 tensor(0.8603, grad_fn=<MseLossBackward>)\n",
      "192 tensor(0.8202, grad_fn=<MseLossBackward>)\n",
      "193 tensor(0.7820, grad_fn=<MseLossBackward>)\n",
      "194 tensor(0.7456, grad_fn=<MseLossBackward>)\n",
      "195 tensor(0.7109, grad_fn=<MseLossBackward>)\n",
      "196 tensor(0.6780, grad_fn=<MseLossBackward>)\n",
      "197 tensor(0.6466, grad_fn=<MseLossBackward>)\n",
      "198 tensor(0.6166, grad_fn=<MseLossBackward>)\n",
      "199 tensor(0.5881, grad_fn=<MseLossBackward>)\n",
      "200 tensor(0.5609, grad_fn=<MseLossBackward>)\n",
      "201 tensor(0.5351, grad_fn=<MseLossBackward>)\n",
      "202 tensor(0.5105, grad_fn=<MseLossBackward>)\n",
      "203 tensor(0.4870, grad_fn=<MseLossBackward>)\n",
      "204 tensor(0.4647, grad_fn=<MseLossBackward>)\n",
      "205 tensor(0.4434, grad_fn=<MseLossBackward>)\n",
      "206 tensor(0.4230, grad_fn=<MseLossBackward>)\n",
      "207 tensor(0.4036, grad_fn=<MseLossBackward>)\n",
      "208 tensor(0.3851, grad_fn=<MseLossBackward>)\n",
      "209 tensor(0.3674, grad_fn=<MseLossBackward>)\n",
      "210 tensor(0.3505, grad_fn=<MseLossBackward>)\n",
      "211 tensor(0.3344, grad_fn=<MseLossBackward>)\n",
      "212 tensor(0.3191, grad_fn=<MseLossBackward>)\n",
      "213 tensor(0.3045, grad_fn=<MseLossBackward>)\n",
      "214 tensor(0.2905, grad_fn=<MseLossBackward>)\n",
      "215 tensor(0.2772, grad_fn=<MseLossBackward>)\n",
      "216 tensor(0.2645, grad_fn=<MseLossBackward>)\n",
      "217 tensor(0.2525, grad_fn=<MseLossBackward>)\n",
      "218 tensor(0.2409, grad_fn=<MseLossBackward>)\n",
      "219 tensor(0.2300, grad_fn=<MseLossBackward>)\n",
      "220 tensor(0.2195, grad_fn=<MseLossBackward>)\n",
      "221 tensor(0.2096, grad_fn=<MseLossBackward>)\n",
      "222 tensor(0.2001, grad_fn=<MseLossBackward>)\n",
      "223 tensor(0.1911, grad_fn=<MseLossBackward>)\n",
      "224 tensor(0.1825, grad_fn=<MseLossBackward>)\n",
      "225 tensor(0.1743, grad_fn=<MseLossBackward>)\n",
      "226 tensor(0.1665, grad_fn=<MseLossBackward>)\n",
      "227 tensor(0.1590, grad_fn=<MseLossBackward>)\n",
      "228 tensor(0.1519, grad_fn=<MseLossBackward>)\n",
      "229 tensor(0.1452, grad_fn=<MseLossBackward>)\n",
      "230 tensor(0.1387, grad_fn=<MseLossBackward>)\n",
      "231 tensor(0.1326, grad_fn=<MseLossBackward>)\n",
      "232 tensor(0.1267, grad_fn=<MseLossBackward>)\n",
      "233 tensor(0.1212, grad_fn=<MseLossBackward>)\n",
      "234 tensor(0.1158, grad_fn=<MseLossBackward>)\n",
      "235 tensor(0.1108, grad_fn=<MseLossBackward>)\n",
      "236 tensor(0.1059, grad_fn=<MseLossBackward>)\n",
      "237 tensor(0.1013, grad_fn=<MseLossBackward>)\n",
      "238 tensor(0.0969, grad_fn=<MseLossBackward>)\n",
      "239 tensor(0.0928, grad_fn=<MseLossBackward>)\n",
      "240 tensor(0.0888, grad_fn=<MseLossBackward>)\n",
      "241 tensor(0.0850, grad_fn=<MseLossBackward>)\n",
      "242 tensor(0.0813, grad_fn=<MseLossBackward>)\n",
      "243 tensor(0.0779, grad_fn=<MseLossBackward>)\n",
      "244 tensor(0.0746, grad_fn=<MseLossBackward>)\n",
      "245 tensor(0.0714, grad_fn=<MseLossBackward>)\n",
      "246 tensor(0.0685, grad_fn=<MseLossBackward>)\n",
      "247 tensor(0.0656, grad_fn=<MseLossBackward>)\n",
      "248 tensor(0.0629, grad_fn=<MseLossBackward>)\n",
      "249 tensor(0.0603, grad_fn=<MseLossBackward>)\n",
      "250 tensor(0.0578, grad_fn=<MseLossBackward>)\n",
      "251 tensor(0.0554, grad_fn=<MseLossBackward>)\n",
      "252 tensor(0.0531, grad_fn=<MseLossBackward>)\n",
      "253 tensor(0.0510, grad_fn=<MseLossBackward>)\n",
      "254 tensor(0.0489, grad_fn=<MseLossBackward>)\n",
      "255 tensor(0.0469, grad_fn=<MseLossBackward>)\n",
      "256 tensor(0.0450, grad_fn=<MseLossBackward>)\n",
      "257 tensor(0.0432, grad_fn=<MseLossBackward>)\n",
      "258 tensor(0.0415, grad_fn=<MseLossBackward>)\n",
      "259 tensor(0.0398, grad_fn=<MseLossBackward>)\n",
      "260 tensor(0.0382, grad_fn=<MseLossBackward>)\n",
      "261 tensor(0.0367, grad_fn=<MseLossBackward>)\n",
      "262 tensor(0.0353, grad_fn=<MseLossBackward>)\n",
      "263 tensor(0.0339, grad_fn=<MseLossBackward>)\n",
      "264 tensor(0.0326, grad_fn=<MseLossBackward>)\n",
      "265 tensor(0.0313, grad_fn=<MseLossBackward>)\n",
      "266 tensor(0.0301, grad_fn=<MseLossBackward>)\n",
      "267 tensor(0.0289, grad_fn=<MseLossBackward>)\n",
      "268 tensor(0.0278, grad_fn=<MseLossBackward>)\n",
      "269 tensor(0.0267, grad_fn=<MseLossBackward>)\n",
      "270 tensor(0.0257, grad_fn=<MseLossBackward>)\n",
      "271 tensor(0.0247, grad_fn=<MseLossBackward>)\n",
      "272 tensor(0.0238, grad_fn=<MseLossBackward>)\n",
      "273 tensor(0.0229, grad_fn=<MseLossBackward>)\n",
      "274 tensor(0.0220, grad_fn=<MseLossBackward>)\n",
      "275 tensor(0.0212, grad_fn=<MseLossBackward>)\n",
      "276 tensor(0.0204, grad_fn=<MseLossBackward>)\n",
      "277 tensor(0.0196, grad_fn=<MseLossBackward>)\n",
      "278 tensor(0.0189, grad_fn=<MseLossBackward>)\n",
      "279 tensor(0.0182, grad_fn=<MseLossBackward>)\n",
      "280 tensor(0.0175, grad_fn=<MseLossBackward>)\n",
      "281 tensor(0.0168, grad_fn=<MseLossBackward>)\n",
      "282 tensor(0.0162, grad_fn=<MseLossBackward>)\n",
      "283 tensor(0.0156, grad_fn=<MseLossBackward>)\n",
      "284 tensor(0.0150, grad_fn=<MseLossBackward>)\n",
      "285 tensor(0.0145, grad_fn=<MseLossBackward>)\n",
      "286 tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "287 tensor(0.0134, grad_fn=<MseLossBackward>)\n",
      "288 tensor(0.0129, grad_fn=<MseLossBackward>)\n",
      "289 tensor(0.0125, grad_fn=<MseLossBackward>)\n",
      "290 tensor(0.0120, grad_fn=<MseLossBackward>)\n",
      "291 tensor(0.0116, grad_fn=<MseLossBackward>)\n",
      "292 tensor(0.0112, grad_fn=<MseLossBackward>)\n",
      "293 tensor(0.0107, grad_fn=<MseLossBackward>)\n",
      "294 tensor(0.0104, grad_fn=<MseLossBackward>)\n",
      "295 tensor(0.0100, grad_fn=<MseLossBackward>)\n",
      "296 tensor(0.0096, grad_fn=<MseLossBackward>)\n",
      "297 tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "298 tensor(0.0089, grad_fn=<MseLossBackward>)\n",
      "299 tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "300 tensor(0.0083, grad_fn=<MseLossBackward>)\n",
      "301 tensor(0.0080, grad_fn=<MseLossBackward>)\n",
      "302 tensor(0.0077, grad_fn=<MseLossBackward>)\n",
      "303 tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "304 tensor(0.0072, grad_fn=<MseLossBackward>)\n",
      "305 tensor(0.0069, grad_fn=<MseLossBackward>)\n",
      "306 tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "307 tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "308 tensor(0.0062, grad_fn=<MseLossBackward>)\n",
      "309 tensor(0.0060, grad_fn=<MseLossBackward>)\n",
      "310 tensor(0.0057, grad_fn=<MseLossBackward>)\n",
      "311 tensor(0.0055, grad_fn=<MseLossBackward>)\n",
      "312 tensor(0.0053, grad_fn=<MseLossBackward>)\n",
      "313 tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "314 tensor(0.0050, grad_fn=<MseLossBackward>)\n",
      "315 tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "316 tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "317 tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "318 tensor(0.0043, grad_fn=<MseLossBackward>)\n",
      "319 tensor(0.0041, grad_fn=<MseLossBackward>)\n",
      "320 tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "321 tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "322 tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "323 tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "324 tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "325 tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "326 tensor(0.0032, grad_fn=<MseLossBackward>)\n",
      "327 tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "328 tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "329 tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "330 tensor(0.0028, grad_fn=<MseLossBackward>)\n",
      "331 tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "332 tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "333 tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "334 tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "335 tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "336 tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "337 tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "338 tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "339 tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "340 tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "341 tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "342 tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "343 tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "344 tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "345 tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "346 tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "347 tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "348 tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "349 tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "350 tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "351 tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "352 tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "353 tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "354 tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "355 tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "356 tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "357 tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "358 tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "359 tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "360 tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "361 tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "362 tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "363 tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "364 tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "365 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "366 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "367 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "368 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "369 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "370 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "371 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "372 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "373 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "374 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "375 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "376 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "377 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "378 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "379 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "380 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "381 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "382 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "383 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "384 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "385 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "386 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "387 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "388 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "389 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "390 tensor(0.0003, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "392 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "393 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "394 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "395 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "396 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "397 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "398 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "399 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "400 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "401 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "402 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "403 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "404 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "405 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "406 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "407 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "408 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "409 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "410 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "411 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "412 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "413 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "414 tensor(9.9363e-05, grad_fn=<MseLossBackward>)\n",
      "415 tensor(9.5223e-05, grad_fn=<MseLossBackward>)\n",
      "416 tensor(9.1250e-05, grad_fn=<MseLossBackward>)\n",
      "417 tensor(8.7428e-05, grad_fn=<MseLossBackward>)\n",
      "418 tensor(8.3769e-05, grad_fn=<MseLossBackward>)\n",
      "419 tensor(8.0250e-05, grad_fn=<MseLossBackward>)\n",
      "420 tensor(7.6877e-05, grad_fn=<MseLossBackward>)\n",
      "421 tensor(7.3642e-05, grad_fn=<MseLossBackward>)\n",
      "422 tensor(7.0534e-05, grad_fn=<MseLossBackward>)\n",
      "423 tensor(6.7553e-05, grad_fn=<MseLossBackward>)\n",
      "424 tensor(6.4692e-05, grad_fn=<MseLossBackward>)\n",
      "425 tensor(6.1948e-05, grad_fn=<MseLossBackward>)\n",
      "426 tensor(5.9317e-05, grad_fn=<MseLossBackward>)\n",
      "427 tensor(5.6792e-05, grad_fn=<MseLossBackward>)\n",
      "428 tensor(5.4371e-05, grad_fn=<MseLossBackward>)\n",
      "429 tensor(5.2049e-05, grad_fn=<MseLossBackward>)\n",
      "430 tensor(4.9820e-05, grad_fn=<MseLossBackward>)\n",
      "431 tensor(4.7687e-05, grad_fn=<MseLossBackward>)\n",
      "432 tensor(4.5638e-05, grad_fn=<MseLossBackward>)\n",
      "433 tensor(4.3676e-05, grad_fn=<MseLossBackward>)\n",
      "434 tensor(4.1793e-05, grad_fn=<MseLossBackward>)\n",
      "435 tensor(3.9992e-05, grad_fn=<MseLossBackward>)\n",
      "436 tensor(3.8263e-05, grad_fn=<MseLossBackward>)\n",
      "437 tensor(3.6605e-05, grad_fn=<MseLossBackward>)\n",
      "438 tensor(3.5018e-05, grad_fn=<MseLossBackward>)\n",
      "439 tensor(3.3494e-05, grad_fn=<MseLossBackward>)\n",
      "440 tensor(3.2039e-05, grad_fn=<MseLossBackward>)\n",
      "441 tensor(3.0640e-05, grad_fn=<MseLossBackward>)\n",
      "442 tensor(2.9303e-05, grad_fn=<MseLossBackward>)\n",
      "443 tensor(2.8020e-05, grad_fn=<MseLossBackward>)\n",
      "444 tensor(2.6793e-05, grad_fn=<MseLossBackward>)\n",
      "445 tensor(2.5616e-05, grad_fn=<MseLossBackward>)\n",
      "446 tensor(2.4489e-05, grad_fn=<MseLossBackward>)\n",
      "447 tensor(2.3411e-05, grad_fn=<MseLossBackward>)\n",
      "448 tensor(2.2378e-05, grad_fn=<MseLossBackward>)\n",
      "449 tensor(2.1388e-05, grad_fn=<MseLossBackward>)\n",
      "450 tensor(2.0440e-05, grad_fn=<MseLossBackward>)\n",
      "451 tensor(1.9533e-05, grad_fn=<MseLossBackward>)\n",
      "452 tensor(1.8665e-05, grad_fn=<MseLossBackward>)\n",
      "453 tensor(1.7835e-05, grad_fn=<MseLossBackward>)\n",
      "454 tensor(1.7040e-05, grad_fn=<MseLossBackward>)\n",
      "455 tensor(1.6278e-05, grad_fn=<MseLossBackward>)\n",
      "456 tensor(1.5550e-05, grad_fn=<MseLossBackward>)\n",
      "457 tensor(1.4853e-05, grad_fn=<MseLossBackward>)\n",
      "458 tensor(1.4186e-05, grad_fn=<MseLossBackward>)\n",
      "459 tensor(1.3549e-05, grad_fn=<MseLossBackward>)\n",
      "460 tensor(1.2937e-05, grad_fn=<MseLossBackward>)\n",
      "461 tensor(1.2353e-05, grad_fn=<MseLossBackward>)\n",
      "462 tensor(1.1796e-05, grad_fn=<MseLossBackward>)\n",
      "463 tensor(1.1261e-05, grad_fn=<MseLossBackward>)\n",
      "464 tensor(1.0750e-05, grad_fn=<MseLossBackward>)\n",
      "465 tensor(1.0262e-05, grad_fn=<MseLossBackward>)\n",
      "466 tensor(9.7946e-06, grad_fn=<MseLossBackward>)\n",
      "467 tensor(9.3474e-06, grad_fn=<MseLossBackward>)\n",
      "468 tensor(8.9201e-06, grad_fn=<MseLossBackward>)\n",
      "469 tensor(8.5119e-06, grad_fn=<MseLossBackward>)\n",
      "470 tensor(8.1218e-06, grad_fn=<MseLossBackward>)\n",
      "471 tensor(7.7496e-06, grad_fn=<MseLossBackward>)\n",
      "472 tensor(7.3931e-06, grad_fn=<MseLossBackward>)\n",
      "473 tensor(7.0525e-06, grad_fn=<MseLossBackward>)\n",
      "474 tensor(6.7275e-06, grad_fn=<MseLossBackward>)\n",
      "475 tensor(6.4162e-06, grad_fn=<MseLossBackward>)\n",
      "476 tensor(6.1185e-06, grad_fn=<MseLossBackward>)\n",
      "477 tensor(5.8351e-06, grad_fn=<MseLossBackward>)\n",
      "478 tensor(5.5640e-06, grad_fn=<MseLossBackward>)\n",
      "479 tensor(5.3046e-06, grad_fn=<MseLossBackward>)\n",
      "480 tensor(5.0576e-06, grad_fn=<MseLossBackward>)\n",
      "481 tensor(4.8210e-06, grad_fn=<MseLossBackward>)\n",
      "482 tensor(4.5953e-06, grad_fn=<MseLossBackward>)\n",
      "483 tensor(4.3806e-06, grad_fn=<MseLossBackward>)\n",
      "484 tensor(4.1736e-06, grad_fn=<MseLossBackward>)\n",
      "485 tensor(3.9787e-06, grad_fn=<MseLossBackward>)\n",
      "486 tensor(3.7907e-06, grad_fn=<MseLossBackward>)\n",
      "487 tensor(3.6122e-06, grad_fn=<MseLossBackward>)\n",
      "488 tensor(3.4416e-06, grad_fn=<MseLossBackward>)\n",
      "489 tensor(3.2781e-06, grad_fn=<MseLossBackward>)\n",
      "490 tensor(3.1227e-06, grad_fn=<MseLossBackward>)\n",
      "491 tensor(2.9746e-06, grad_fn=<MseLossBackward>)\n",
      "492 tensor(2.8324e-06, grad_fn=<MseLossBackward>)\n",
      "493 tensor(2.6982e-06, grad_fn=<MseLossBackward>)\n",
      "494 tensor(2.5690e-06, grad_fn=<MseLossBackward>)\n",
      "495 tensor(2.4461e-06, grad_fn=<MseLossBackward>)\n",
      "496 tensor(2.3291e-06, grad_fn=<MseLossBackward>)\n",
      "497 tensor(2.2173e-06, grad_fn=<MseLossBackward>)\n",
      "498 tensor(2.1106e-06, grad_fn=<MseLossBackward>)\n",
      "499 tensor(2.0089e-06, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机初始化训练数据 \n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for iter in range(500):\n",
    "    y_pred = model(x) #相当于forward pass\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(iter, loss)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1dfaf113",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-04T06:30:40.382704Z",
     "start_time": "2021-09-04T06:30:36.572273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(713.7997, grad_fn=<MseLossBackward>)\n",
      "1 tensor(695.8496, grad_fn=<MseLossBackward>)\n",
      "2 tensor(678.4163, grad_fn=<MseLossBackward>)\n",
      "3 tensor(661.5437, grad_fn=<MseLossBackward>)\n",
      "4 tensor(645.1903, grad_fn=<MseLossBackward>)\n",
      "5 tensor(629.3596, grad_fn=<MseLossBackward>)\n",
      "6 tensor(613.9913, grad_fn=<MseLossBackward>)\n",
      "7 tensor(599.2146, grad_fn=<MseLossBackward>)\n",
      "8 tensor(584.8160, grad_fn=<MseLossBackward>)\n",
      "9 tensor(570.8252, grad_fn=<MseLossBackward>)\n",
      "10 tensor(557.1788, grad_fn=<MseLossBackward>)\n",
      "11 tensor(543.9304, grad_fn=<MseLossBackward>)\n",
      "12 tensor(531.0176, grad_fn=<MseLossBackward>)\n",
      "13 tensor(518.4645, grad_fn=<MseLossBackward>)\n",
      "14 tensor(506.3403, grad_fn=<MseLossBackward>)\n",
      "15 tensor(494.5287, grad_fn=<MseLossBackward>)\n",
      "16 tensor(483.0566, grad_fn=<MseLossBackward>)\n",
      "17 tensor(471.8892, grad_fn=<MseLossBackward>)\n",
      "18 tensor(461.0341, grad_fn=<MseLossBackward>)\n",
      "19 tensor(450.4300, grad_fn=<MseLossBackward>)\n",
      "20 tensor(440.0566, grad_fn=<MseLossBackward>)\n",
      "21 tensor(429.9656, grad_fn=<MseLossBackward>)\n",
      "22 tensor(420.2036, grad_fn=<MseLossBackward>)\n",
      "23 tensor(410.6740, grad_fn=<MseLossBackward>)\n",
      "24 tensor(401.3916, grad_fn=<MseLossBackward>)\n",
      "25 tensor(392.3505, grad_fn=<MseLossBackward>)\n",
      "26 tensor(383.5097, grad_fn=<MseLossBackward>)\n",
      "27 tensor(374.9110, grad_fn=<MseLossBackward>)\n",
      "28 tensor(366.5064, grad_fn=<MseLossBackward>)\n",
      "29 tensor(358.2722, grad_fn=<MseLossBackward>)\n",
      "30 tensor(350.2034, grad_fn=<MseLossBackward>)\n",
      "31 tensor(342.3327, grad_fn=<MseLossBackward>)\n",
      "32 tensor(334.6430, grad_fn=<MseLossBackward>)\n",
      "33 tensor(327.1306, grad_fn=<MseLossBackward>)\n",
      "34 tensor(319.7758, grad_fn=<MseLossBackward>)\n",
      "35 tensor(312.5712, grad_fn=<MseLossBackward>)\n",
      "36 tensor(305.5012, grad_fn=<MseLossBackward>)\n",
      "37 tensor(298.5829, grad_fn=<MseLossBackward>)\n",
      "38 tensor(291.7929, grad_fn=<MseLossBackward>)\n",
      "39 tensor(285.1475, grad_fn=<MseLossBackward>)\n",
      "40 tensor(278.6339, grad_fn=<MseLossBackward>)\n",
      "41 tensor(272.2280, grad_fn=<MseLossBackward>)\n",
      "42 tensor(265.9457, grad_fn=<MseLossBackward>)\n",
      "43 tensor(259.7684, grad_fn=<MseLossBackward>)\n",
      "44 tensor(253.7064, grad_fn=<MseLossBackward>)\n",
      "45 tensor(247.7580, grad_fn=<MseLossBackward>)\n",
      "46 tensor(241.9378, grad_fn=<MseLossBackward>)\n",
      "47 tensor(236.2573, grad_fn=<MseLossBackward>)\n",
      "48 tensor(230.6982, grad_fn=<MseLossBackward>)\n",
      "49 tensor(225.2303, grad_fn=<MseLossBackward>)\n",
      "50 tensor(219.8627, grad_fn=<MseLossBackward>)\n",
      "51 tensor(214.5968, grad_fn=<MseLossBackward>)\n",
      "52 tensor(209.4313, grad_fn=<MseLossBackward>)\n",
      "53 tensor(204.3602, grad_fn=<MseLossBackward>)\n",
      "54 tensor(199.3808, grad_fn=<MseLossBackward>)\n",
      "55 tensor(194.4875, grad_fn=<MseLossBackward>)\n",
      "56 tensor(189.6903, grad_fn=<MseLossBackward>)\n",
      "57 tensor(184.9804, grad_fn=<MseLossBackward>)\n",
      "58 tensor(180.3568, grad_fn=<MseLossBackward>)\n",
      "59 tensor(175.8172, grad_fn=<MseLossBackward>)\n",
      "60 tensor(171.3647, grad_fn=<MseLossBackward>)\n",
      "61 tensor(167.0008, grad_fn=<MseLossBackward>)\n",
      "62 tensor(162.7193, grad_fn=<MseLossBackward>)\n",
      "63 tensor(158.5172, grad_fn=<MseLossBackward>)\n",
      "64 tensor(154.3925, grad_fn=<MseLossBackward>)\n",
      "65 tensor(150.3462, grad_fn=<MseLossBackward>)\n",
      "66 tensor(146.3768, grad_fn=<MseLossBackward>)\n",
      "67 tensor(142.4809, grad_fn=<MseLossBackward>)\n",
      "68 tensor(138.6664, grad_fn=<MseLossBackward>)\n",
      "69 tensor(134.9329, grad_fn=<MseLossBackward>)\n",
      "70 tensor(131.2801, grad_fn=<MseLossBackward>)\n",
      "71 tensor(127.7045, grad_fn=<MseLossBackward>)\n",
      "72 tensor(124.1999, grad_fn=<MseLossBackward>)\n",
      "73 tensor(120.7684, grad_fn=<MseLossBackward>)\n",
      "74 tensor(117.4040, grad_fn=<MseLossBackward>)\n",
      "75 tensor(114.1116, grad_fn=<MseLossBackward>)\n",
      "76 tensor(110.8969, grad_fn=<MseLossBackward>)\n",
      "77 tensor(107.7611, grad_fn=<MseLossBackward>)\n",
      "78 tensor(104.6917, grad_fn=<MseLossBackward>)\n",
      "79 tensor(101.6827, grad_fn=<MseLossBackward>)\n",
      "80 tensor(98.7449, grad_fn=<MseLossBackward>)\n",
      "81 tensor(95.8711, grad_fn=<MseLossBackward>)\n",
      "82 tensor(93.0605, grad_fn=<MseLossBackward>)\n",
      "83 tensor(90.3065, grad_fn=<MseLossBackward>)\n",
      "84 tensor(87.6183, grad_fn=<MseLossBackward>)\n",
      "85 tensor(84.9942, grad_fn=<MseLossBackward>)\n",
      "86 tensor(82.4337, grad_fn=<MseLossBackward>)\n",
      "87 tensor(79.9313, grad_fn=<MseLossBackward>)\n",
      "88 tensor(77.4854, grad_fn=<MseLossBackward>)\n",
      "89 tensor(75.0964, grad_fn=<MseLossBackward>)\n",
      "90 tensor(72.7662, grad_fn=<MseLossBackward>)\n",
      "91 tensor(70.4909, grad_fn=<MseLossBackward>)\n",
      "92 tensor(68.2680, grad_fn=<MseLossBackward>)\n",
      "93 tensor(66.1018, grad_fn=<MseLossBackward>)\n",
      "94 tensor(63.9894, grad_fn=<MseLossBackward>)\n",
      "95 tensor(61.9343, grad_fn=<MseLossBackward>)\n",
      "96 tensor(59.9278, grad_fn=<MseLossBackward>)\n",
      "97 tensor(57.9712, grad_fn=<MseLossBackward>)\n",
      "98 tensor(56.0689, grad_fn=<MseLossBackward>)\n",
      "99 tensor(54.2140, grad_fn=<MseLossBackward>)\n",
      "100 tensor(52.4089, grad_fn=<MseLossBackward>)\n",
      "101 tensor(50.6505, grad_fn=<MseLossBackward>)\n",
      "102 tensor(48.9396, grad_fn=<MseLossBackward>)\n",
      "103 tensor(47.2743, grad_fn=<MseLossBackward>)\n",
      "104 tensor(45.6533, grad_fn=<MseLossBackward>)\n",
      "105 tensor(44.0788, grad_fn=<MseLossBackward>)\n",
      "106 tensor(42.5472, grad_fn=<MseLossBackward>)\n",
      "107 tensor(41.0592, grad_fn=<MseLossBackward>)\n",
      "108 tensor(39.6132, grad_fn=<MseLossBackward>)\n",
      "109 tensor(38.2091, grad_fn=<MseLossBackward>)\n",
      "110 tensor(36.8456, grad_fn=<MseLossBackward>)\n",
      "111 tensor(35.5233, grad_fn=<MseLossBackward>)\n",
      "112 tensor(34.2383, grad_fn=<MseLossBackward>)\n",
      "113 tensor(32.9931, grad_fn=<MseLossBackward>)\n",
      "114 tensor(31.7859, grad_fn=<MseLossBackward>)\n",
      "115 tensor(30.6158, grad_fn=<MseLossBackward>)\n",
      "116 tensor(29.4835, grad_fn=<MseLossBackward>)\n",
      "117 tensor(28.3856, grad_fn=<MseLossBackward>)\n",
      "118 tensor(27.3219, grad_fn=<MseLossBackward>)\n",
      "119 tensor(26.2920, grad_fn=<MseLossBackward>)\n",
      "120 tensor(25.2953, grad_fn=<MseLossBackward>)\n",
      "121 tensor(24.3321, grad_fn=<MseLossBackward>)\n",
      "122 tensor(23.3984, grad_fn=<MseLossBackward>)\n",
      "123 tensor(22.4953, grad_fn=<MseLossBackward>)\n",
      "124 tensor(21.6227, grad_fn=<MseLossBackward>)\n",
      "125 tensor(20.7790, grad_fn=<MseLossBackward>)\n",
      "126 tensor(19.9644, grad_fn=<MseLossBackward>)\n",
      "127 tensor(19.1770, grad_fn=<MseLossBackward>)\n",
      "128 tensor(18.4173, grad_fn=<MseLossBackward>)\n",
      "129 tensor(17.6842, grad_fn=<MseLossBackward>)\n",
      "130 tensor(16.9763, grad_fn=<MseLossBackward>)\n",
      "131 tensor(16.2936, grad_fn=<MseLossBackward>)\n",
      "132 tensor(15.6356, grad_fn=<MseLossBackward>)\n",
      "133 tensor(15.0013, grad_fn=<MseLossBackward>)\n",
      "134 tensor(14.3902, grad_fn=<MseLossBackward>)\n",
      "135 tensor(13.8015, grad_fn=<MseLossBackward>)\n",
      "136 tensor(13.2340, grad_fn=<MseLossBackward>)\n",
      "137 tensor(12.6865, grad_fn=<MseLossBackward>)\n",
      "138 tensor(12.1605, grad_fn=<MseLossBackward>)\n",
      "139 tensor(11.6532, grad_fn=<MseLossBackward>)\n",
      "140 tensor(11.1652, grad_fn=<MseLossBackward>)\n",
      "141 tensor(10.6950, grad_fn=<MseLossBackward>)\n",
      "142 tensor(10.2422, grad_fn=<MseLossBackward>)\n",
      "143 tensor(9.8065, grad_fn=<MseLossBackward>)\n",
      "144 tensor(9.3875, grad_fn=<MseLossBackward>)\n",
      "145 tensor(8.9851, grad_fn=<MseLossBackward>)\n",
      "146 tensor(8.5985, grad_fn=<MseLossBackward>)\n",
      "147 tensor(8.2262, grad_fn=<MseLossBackward>)\n",
      "148 tensor(7.8686, grad_fn=<MseLossBackward>)\n",
      "149 tensor(7.5251, grad_fn=<MseLossBackward>)\n",
      "150 tensor(7.1952, grad_fn=<MseLossBackward>)\n",
      "151 tensor(6.8785, grad_fn=<MseLossBackward>)\n",
      "152 tensor(6.5739, grad_fn=<MseLossBackward>)\n",
      "153 tensor(6.2820, grad_fn=<MseLossBackward>)\n",
      "154 tensor(6.0018, grad_fn=<MseLossBackward>)\n",
      "155 tensor(5.7331, grad_fn=<MseLossBackward>)\n",
      "156 tensor(5.4754, grad_fn=<MseLossBackward>)\n",
      "157 tensor(5.2283, grad_fn=<MseLossBackward>)\n",
      "158 tensor(4.9913, grad_fn=<MseLossBackward>)\n",
      "159 tensor(4.7646, grad_fn=<MseLossBackward>)\n",
      "160 tensor(4.5473, grad_fn=<MseLossBackward>)\n",
      "161 tensor(4.3389, grad_fn=<MseLossBackward>)\n",
      "162 tensor(4.1395, grad_fn=<MseLossBackward>)\n",
      "163 tensor(3.9489, grad_fn=<MseLossBackward>)\n",
      "164 tensor(3.7662, grad_fn=<MseLossBackward>)\n",
      "165 tensor(3.5918, grad_fn=<MseLossBackward>)\n",
      "166 tensor(3.4248, grad_fn=<MseLossBackward>)\n",
      "167 tensor(3.2652, grad_fn=<MseLossBackward>)\n",
      "168 tensor(3.1123, grad_fn=<MseLossBackward>)\n",
      "169 tensor(2.9662, grad_fn=<MseLossBackward>)\n",
      "170 tensor(2.8266, grad_fn=<MseLossBackward>)\n",
      "171 tensor(2.6933, grad_fn=<MseLossBackward>)\n",
      "172 tensor(2.5657, grad_fn=<MseLossBackward>)\n",
      "173 tensor(2.4440, grad_fn=<MseLossBackward>)\n",
      "174 tensor(2.3280, grad_fn=<MseLossBackward>)\n",
      "175 tensor(2.2172, grad_fn=<MseLossBackward>)\n",
      "176 tensor(2.1114, grad_fn=<MseLossBackward>)\n",
      "177 tensor(2.0105, grad_fn=<MseLossBackward>)\n",
      "178 tensor(1.9141, grad_fn=<MseLossBackward>)\n",
      "179 tensor(1.8219, grad_fn=<MseLossBackward>)\n",
      "180 tensor(1.7340, grad_fn=<MseLossBackward>)\n",
      "181 tensor(1.6502, grad_fn=<MseLossBackward>)\n",
      "182 tensor(1.5702, grad_fn=<MseLossBackward>)\n",
      "183 tensor(1.4939, grad_fn=<MseLossBackward>)\n",
      "184 tensor(1.4211, grad_fn=<MseLossBackward>)\n",
      "185 tensor(1.3516, grad_fn=<MseLossBackward>)\n",
      "186 tensor(1.2854, grad_fn=<MseLossBackward>)\n",
      "187 tensor(1.2223, grad_fn=<MseLossBackward>)\n",
      "188 tensor(1.1622, grad_fn=<MseLossBackward>)\n",
      "189 tensor(1.1049, grad_fn=<MseLossBackward>)\n",
      "190 tensor(1.0503, grad_fn=<MseLossBackward>)\n",
      "191 tensor(0.9982, grad_fn=<MseLossBackward>)\n",
      "192 tensor(0.9487, grad_fn=<MseLossBackward>)\n",
      "193 tensor(0.9015, grad_fn=<MseLossBackward>)\n",
      "194 tensor(0.8566, grad_fn=<MseLossBackward>)\n",
      "195 tensor(0.8138, grad_fn=<MseLossBackward>)\n",
      "196 tensor(0.7731, grad_fn=<MseLossBackward>)\n",
      "197 tensor(0.7344, grad_fn=<MseLossBackward>)\n",
      "198 tensor(0.6975, grad_fn=<MseLossBackward>)\n",
      "199 tensor(0.6624, grad_fn=<MseLossBackward>)\n",
      "200 tensor(0.6291, grad_fn=<MseLossBackward>)\n",
      "201 tensor(0.5974, grad_fn=<MseLossBackward>)\n",
      "202 tensor(0.5672, grad_fn=<MseLossBackward>)\n",
      "203 tensor(0.5385, grad_fn=<MseLossBackward>)\n",
      "204 tensor(0.5112, grad_fn=<MseLossBackward>)\n",
      "205 tensor(0.4853, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206 tensor(0.4607, grad_fn=<MseLossBackward>)\n",
      "207 tensor(0.4373, grad_fn=<MseLossBackward>)\n",
      "208 tensor(0.4151, grad_fn=<MseLossBackward>)\n",
      "209 tensor(0.3940, grad_fn=<MseLossBackward>)\n",
      "210 tensor(0.3739, grad_fn=<MseLossBackward>)\n",
      "211 tensor(0.3549, grad_fn=<MseLossBackward>)\n",
      "212 tensor(0.3368, grad_fn=<MseLossBackward>)\n",
      "213 tensor(0.3196, grad_fn=<MseLossBackward>)\n",
      "214 tensor(0.3033, grad_fn=<MseLossBackward>)\n",
      "215 tensor(0.2879, grad_fn=<MseLossBackward>)\n",
      "216 tensor(0.2731, grad_fn=<MseLossBackward>)\n",
      "217 tensor(0.2592, grad_fn=<MseLossBackward>)\n",
      "218 tensor(0.2459, grad_fn=<MseLossBackward>)\n",
      "219 tensor(0.2334, grad_fn=<MseLossBackward>)\n",
      "220 tensor(0.2214, grad_fn=<MseLossBackward>)\n",
      "221 tensor(0.2101, grad_fn=<MseLossBackward>)\n",
      "222 tensor(0.1993, grad_fn=<MseLossBackward>)\n",
      "223 tensor(0.1891, grad_fn=<MseLossBackward>)\n",
      "224 tensor(0.1794, grad_fn=<MseLossBackward>)\n",
      "225 tensor(0.1702, grad_fn=<MseLossBackward>)\n",
      "226 tensor(0.1615, grad_fn=<MseLossBackward>)\n",
      "227 tensor(0.1532, grad_fn=<MseLossBackward>)\n",
      "228 tensor(0.1454, grad_fn=<MseLossBackward>)\n",
      "229 tensor(0.1379, grad_fn=<MseLossBackward>)\n",
      "230 tensor(0.1309, grad_fn=<MseLossBackward>)\n",
      "231 tensor(0.1242, grad_fn=<MseLossBackward>)\n",
      "232 tensor(0.1178, grad_fn=<MseLossBackward>)\n",
      "233 tensor(0.1118, grad_fn=<MseLossBackward>)\n",
      "234 tensor(0.1060, grad_fn=<MseLossBackward>)\n",
      "235 tensor(0.1006, grad_fn=<MseLossBackward>)\n",
      "236 tensor(0.0954, grad_fn=<MseLossBackward>)\n",
      "237 tensor(0.0906, grad_fn=<MseLossBackward>)\n",
      "238 tensor(0.0859, grad_fn=<MseLossBackward>)\n",
      "239 tensor(0.0815, grad_fn=<MseLossBackward>)\n",
      "240 tensor(0.0774, grad_fn=<MseLossBackward>)\n",
      "241 tensor(0.0734, grad_fn=<MseLossBackward>)\n",
      "242 tensor(0.0696, grad_fn=<MseLossBackward>)\n",
      "243 tensor(0.0661, grad_fn=<MseLossBackward>)\n",
      "244 tensor(0.0627, grad_fn=<MseLossBackward>)\n",
      "245 tensor(0.0595, grad_fn=<MseLossBackward>)\n",
      "246 tensor(0.0565, grad_fn=<MseLossBackward>)\n",
      "247 tensor(0.0536, grad_fn=<MseLossBackward>)\n",
      "248 tensor(0.0509, grad_fn=<MseLossBackward>)\n",
      "249 tensor(0.0483, grad_fn=<MseLossBackward>)\n",
      "250 tensor(0.0458, grad_fn=<MseLossBackward>)\n",
      "251 tensor(0.0435, grad_fn=<MseLossBackward>)\n",
      "252 tensor(0.0413, grad_fn=<MseLossBackward>)\n",
      "253 tensor(0.0392, grad_fn=<MseLossBackward>)\n",
      "254 tensor(0.0372, grad_fn=<MseLossBackward>)\n",
      "255 tensor(0.0353, grad_fn=<MseLossBackward>)\n",
      "256 tensor(0.0335, grad_fn=<MseLossBackward>)\n",
      "257 tensor(0.0318, grad_fn=<MseLossBackward>)\n",
      "258 tensor(0.0302, grad_fn=<MseLossBackward>)\n",
      "259 tensor(0.0286, grad_fn=<MseLossBackward>)\n",
      "260 tensor(0.0272, grad_fn=<MseLossBackward>)\n",
      "261 tensor(0.0258, grad_fn=<MseLossBackward>)\n",
      "262 tensor(0.0245, grad_fn=<MseLossBackward>)\n",
      "263 tensor(0.0233, grad_fn=<MseLossBackward>)\n",
      "264 tensor(0.0221, grad_fn=<MseLossBackward>)\n",
      "265 tensor(0.0210, grad_fn=<MseLossBackward>)\n",
      "266 tensor(0.0199, grad_fn=<MseLossBackward>)\n",
      "267 tensor(0.0189, grad_fn=<MseLossBackward>)\n",
      "268 tensor(0.0180, grad_fn=<MseLossBackward>)\n",
      "269 tensor(0.0171, grad_fn=<MseLossBackward>)\n",
      "270 tensor(0.0163, grad_fn=<MseLossBackward>)\n",
      "271 tensor(0.0155, grad_fn=<MseLossBackward>)\n",
      "272 tensor(0.0147, grad_fn=<MseLossBackward>)\n",
      "273 tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "274 tensor(0.0133, grad_fn=<MseLossBackward>)\n",
      "275 tensor(0.0127, grad_fn=<MseLossBackward>)\n",
      "276 tensor(0.0121, grad_fn=<MseLossBackward>)\n",
      "277 tensor(0.0115, grad_fn=<MseLossBackward>)\n",
      "278 tensor(0.0110, grad_fn=<MseLossBackward>)\n",
      "279 tensor(0.0104, grad_fn=<MseLossBackward>)\n",
      "280 tensor(0.0099, grad_fn=<MseLossBackward>)\n",
      "281 tensor(0.0095, grad_fn=<MseLossBackward>)\n",
      "282 tensor(0.0090, grad_fn=<MseLossBackward>)\n",
      "283 tensor(0.0086, grad_fn=<MseLossBackward>)\n",
      "284 tensor(0.0082, grad_fn=<MseLossBackward>)\n",
      "285 tensor(0.0078, grad_fn=<MseLossBackward>)\n",
      "286 tensor(0.0074, grad_fn=<MseLossBackward>)\n",
      "287 tensor(0.0071, grad_fn=<MseLossBackward>)\n",
      "288 tensor(0.0067, grad_fn=<MseLossBackward>)\n",
      "289 tensor(0.0064, grad_fn=<MseLossBackward>)\n",
      "290 tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "291 tensor(0.0058, grad_fn=<MseLossBackward>)\n",
      "292 tensor(0.0056, grad_fn=<MseLossBackward>)\n",
      "293 tensor(0.0053, grad_fn=<MseLossBackward>)\n",
      "294 tensor(0.0051, grad_fn=<MseLossBackward>)\n",
      "295 tensor(0.0048, grad_fn=<MseLossBackward>)\n",
      "296 tensor(0.0046, grad_fn=<MseLossBackward>)\n",
      "297 tensor(0.0044, grad_fn=<MseLossBackward>)\n",
      "298 tensor(0.0042, grad_fn=<MseLossBackward>)\n",
      "299 tensor(0.0040, grad_fn=<MseLossBackward>)\n",
      "300 tensor(0.0038, grad_fn=<MseLossBackward>)\n",
      "301 tensor(0.0036, grad_fn=<MseLossBackward>)\n",
      "302 tensor(0.0035, grad_fn=<MseLossBackward>)\n",
      "303 tensor(0.0033, grad_fn=<MseLossBackward>)\n",
      "304 tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "305 tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "306 tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "307 tensor(0.0027, grad_fn=<MseLossBackward>)\n",
      "308 tensor(0.0026, grad_fn=<MseLossBackward>)\n",
      "309 tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "310 tensor(0.0024, grad_fn=<MseLossBackward>)\n",
      "311 tensor(0.0023, grad_fn=<MseLossBackward>)\n",
      "312 tensor(0.0021, grad_fn=<MseLossBackward>)\n",
      "313 tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "314 tensor(0.0020, grad_fn=<MseLossBackward>)\n",
      "315 tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "316 tensor(0.0018, grad_fn=<MseLossBackward>)\n",
      "317 tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "318 tensor(0.0016, grad_fn=<MseLossBackward>)\n",
      "319 tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "320 tensor(0.0015, grad_fn=<MseLossBackward>)\n",
      "321 tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "322 tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "323 tensor(0.0013, grad_fn=<MseLossBackward>)\n",
      "324 tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "325 tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "326 tensor(0.0011, grad_fn=<MseLossBackward>)\n",
      "327 tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "328 tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "329 tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "330 tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "331 tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "332 tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "333 tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "334 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "335 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "336 tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "337 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "338 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "339 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "340 tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "341 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "342 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "343 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "344 tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "345 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "346 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "347 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "348 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "349 tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "350 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "351 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "352 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "353 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "354 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "355 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "356 tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "357 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "358 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "359 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "360 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "361 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "362 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "363 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "364 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "365 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "366 tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "367 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "368 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "369 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "370 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "371 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "372 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "373 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "374 tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "375 tensor(9.6891e-05, grad_fn=<MseLossBackward>)\n",
      "376 tensor(9.2048e-05, grad_fn=<MseLossBackward>)\n",
      "377 tensor(8.7439e-05, grad_fn=<MseLossBackward>)\n",
      "378 tensor(8.3052e-05, grad_fn=<MseLossBackward>)\n",
      "379 tensor(7.8880e-05, grad_fn=<MseLossBackward>)\n",
      "380 tensor(7.4916e-05, grad_fn=<MseLossBackward>)\n",
      "381 tensor(7.1142e-05, grad_fn=<MseLossBackward>)\n",
      "382 tensor(6.7547e-05, grad_fn=<MseLossBackward>)\n",
      "383 tensor(6.4135e-05, grad_fn=<MseLossBackward>)\n",
      "384 tensor(6.0890e-05, grad_fn=<MseLossBackward>)\n",
      "385 tensor(5.7805e-05, grad_fn=<MseLossBackward>)\n",
      "386 tensor(5.4867e-05, grad_fn=<MseLossBackward>)\n",
      "387 tensor(5.2075e-05, grad_fn=<MseLossBackward>)\n",
      "388 tensor(4.9426e-05, grad_fn=<MseLossBackward>)\n",
      "389 tensor(4.6903e-05, grad_fn=<MseLossBackward>)\n",
      "390 tensor(4.4506e-05, grad_fn=<MseLossBackward>)\n",
      "391 tensor(4.2229e-05, grad_fn=<MseLossBackward>)\n",
      "392 tensor(4.0062e-05, grad_fn=<MseLossBackward>)\n",
      "393 tensor(3.8006e-05, grad_fn=<MseLossBackward>)\n",
      "394 tensor(3.6051e-05, grad_fn=<MseLossBackward>)\n",
      "395 tensor(3.4195e-05, grad_fn=<MseLossBackward>)\n",
      "396 tensor(3.2431e-05, grad_fn=<MseLossBackward>)\n",
      "397 tensor(3.0754e-05, grad_fn=<MseLossBackward>)\n",
      "398 tensor(2.9162e-05, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399 tensor(2.7653e-05, grad_fn=<MseLossBackward>)\n",
      "400 tensor(2.6215e-05, grad_fn=<MseLossBackward>)\n",
      "401 tensor(2.4851e-05, grad_fn=<MseLossBackward>)\n",
      "402 tensor(2.3557e-05, grad_fn=<MseLossBackward>)\n",
      "403 tensor(2.2327e-05, grad_fn=<MseLossBackward>)\n",
      "404 tensor(2.1158e-05, grad_fn=<MseLossBackward>)\n",
      "405 tensor(2.0050e-05, grad_fn=<MseLossBackward>)\n",
      "406 tensor(1.9000e-05, grad_fn=<MseLossBackward>)\n",
      "407 tensor(1.8001e-05, grad_fn=<MseLossBackward>)\n",
      "408 tensor(1.7054e-05, grad_fn=<MseLossBackward>)\n",
      "409 tensor(1.6154e-05, grad_fn=<MseLossBackward>)\n",
      "410 tensor(1.5300e-05, grad_fn=<MseLossBackward>)\n",
      "411 tensor(1.4490e-05, grad_fn=<MseLossBackward>)\n",
      "412 tensor(1.3722e-05, grad_fn=<MseLossBackward>)\n",
      "413 tensor(1.2995e-05, grad_fn=<MseLossBackward>)\n",
      "414 tensor(1.2303e-05, grad_fn=<MseLossBackward>)\n",
      "415 tensor(1.1648e-05, grad_fn=<MseLossBackward>)\n",
      "416 tensor(1.1026e-05, grad_fn=<MseLossBackward>)\n",
      "417 tensor(1.0437e-05, grad_fn=<MseLossBackward>)\n",
      "418 tensor(9.8784e-06, grad_fn=<MseLossBackward>)\n",
      "419 tensor(9.3486e-06, grad_fn=<MseLossBackward>)\n",
      "420 tensor(8.8462e-06, grad_fn=<MseLossBackward>)\n",
      "421 tensor(8.3696e-06, grad_fn=<MseLossBackward>)\n",
      "422 tensor(7.9196e-06, grad_fn=<MseLossBackward>)\n",
      "423 tensor(7.4917e-06, grad_fn=<MseLossBackward>)\n",
      "424 tensor(7.0872e-06, grad_fn=<MseLossBackward>)\n",
      "425 tensor(6.7013e-06, grad_fn=<MseLossBackward>)\n",
      "426 tensor(6.3384e-06, grad_fn=<MseLossBackward>)\n",
      "427 tensor(5.9930e-06, grad_fn=<MseLossBackward>)\n",
      "428 tensor(5.6680e-06, grad_fn=<MseLossBackward>)\n",
      "429 tensor(5.3581e-06, grad_fn=<MseLossBackward>)\n",
      "430 tensor(5.0649e-06, grad_fn=<MseLossBackward>)\n",
      "431 tensor(4.7875e-06, grad_fn=<MseLossBackward>)\n",
      "432 tensor(4.5247e-06, grad_fn=<MseLossBackward>)\n",
      "433 tensor(4.2765e-06, grad_fn=<MseLossBackward>)\n",
      "434 tensor(4.0419e-06, grad_fn=<MseLossBackward>)\n",
      "435 tensor(3.8187e-06, grad_fn=<MseLossBackward>)\n",
      "436 tensor(3.6080e-06, grad_fn=<MseLossBackward>)\n",
      "437 tensor(3.4079e-06, grad_fn=<MseLossBackward>)\n",
      "438 tensor(3.2196e-06, grad_fn=<MseLossBackward>)\n",
      "439 tensor(3.0403e-06, grad_fn=<MseLossBackward>)\n",
      "440 tensor(2.8721e-06, grad_fn=<MseLossBackward>)\n",
      "441 tensor(2.7123e-06, grad_fn=<MseLossBackward>)\n",
      "442 tensor(2.5609e-06, grad_fn=<MseLossBackward>)\n",
      "443 tensor(2.4177e-06, grad_fn=<MseLossBackward>)\n",
      "444 tensor(2.2823e-06, grad_fn=<MseLossBackward>)\n",
      "445 tensor(2.1547e-06, grad_fn=<MseLossBackward>)\n",
      "446 tensor(2.0334e-06, grad_fn=<MseLossBackward>)\n",
      "447 tensor(1.9186e-06, grad_fn=<MseLossBackward>)\n",
      "448 tensor(1.8108e-06, grad_fn=<MseLossBackward>)\n",
      "449 tensor(1.7085e-06, grad_fn=<MseLossBackward>)\n",
      "450 tensor(1.6118e-06, grad_fn=<MseLossBackward>)\n",
      "451 tensor(1.5209e-06, grad_fn=<MseLossBackward>)\n",
      "452 tensor(1.4342e-06, grad_fn=<MseLossBackward>)\n",
      "453 tensor(1.3530e-06, grad_fn=<MseLossBackward>)\n",
      "454 tensor(1.2757e-06, grad_fn=<MseLossBackward>)\n",
      "455 tensor(1.2034e-06, grad_fn=<MseLossBackward>)\n",
      "456 tensor(1.1346e-06, grad_fn=<MseLossBackward>)\n",
      "457 tensor(1.0694e-06, grad_fn=<MseLossBackward>)\n",
      "458 tensor(1.0083e-06, grad_fn=<MseLossBackward>)\n",
      "459 tensor(9.5058e-07, grad_fn=<MseLossBackward>)\n",
      "460 tensor(8.9558e-07, grad_fn=<MseLossBackward>)\n",
      "461 tensor(8.4394e-07, grad_fn=<MseLossBackward>)\n",
      "462 tensor(7.9506e-07, grad_fn=<MseLossBackward>)\n",
      "463 tensor(7.4937e-07, grad_fn=<MseLossBackward>)\n",
      "464 tensor(7.0595e-07, grad_fn=<MseLossBackward>)\n",
      "465 tensor(6.6463e-07, grad_fn=<MseLossBackward>)\n",
      "466 tensor(6.2615e-07, grad_fn=<MseLossBackward>)\n",
      "467 tensor(5.8981e-07, grad_fn=<MseLossBackward>)\n",
      "468 tensor(5.5571e-07, grad_fn=<MseLossBackward>)\n",
      "469 tensor(5.2327e-07, grad_fn=<MseLossBackward>)\n",
      "470 tensor(4.9255e-07, grad_fn=<MseLossBackward>)\n",
      "471 tensor(4.6375e-07, grad_fn=<MseLossBackward>)\n",
      "472 tensor(4.3666e-07, grad_fn=<MseLossBackward>)\n",
      "473 tensor(4.1092e-07, grad_fn=<MseLossBackward>)\n",
      "474 tensor(3.8641e-07, grad_fn=<MseLossBackward>)\n",
      "475 tensor(3.6373e-07, grad_fn=<MseLossBackward>)\n",
      "476 tensor(3.4252e-07, grad_fn=<MseLossBackward>)\n",
      "477 tensor(3.2201e-07, grad_fn=<MseLossBackward>)\n",
      "478 tensor(3.0318e-07, grad_fn=<MseLossBackward>)\n",
      "479 tensor(2.8505e-07, grad_fn=<MseLossBackward>)\n",
      "480 tensor(2.6818e-07, grad_fn=<MseLossBackward>)\n",
      "481 tensor(2.5214e-07, grad_fn=<MseLossBackward>)\n",
      "482 tensor(2.3718e-07, grad_fn=<MseLossBackward>)\n",
      "483 tensor(2.2291e-07, grad_fn=<MseLossBackward>)\n",
      "484 tensor(2.0986e-07, grad_fn=<MseLossBackward>)\n",
      "485 tensor(1.9713e-07, grad_fn=<MseLossBackward>)\n",
      "486 tensor(1.8544e-07, grad_fn=<MseLossBackward>)\n",
      "487 tensor(1.7427e-07, grad_fn=<MseLossBackward>)\n",
      "488 tensor(1.6361e-07, grad_fn=<MseLossBackward>)\n",
      "489 tensor(1.5380e-07, grad_fn=<MseLossBackward>)\n",
      "490 tensor(1.4436e-07, grad_fn=<MseLossBackward>)\n",
      "491 tensor(1.3580e-07, grad_fn=<MseLossBackward>)\n",
      "492 tensor(1.2748e-07, grad_fn=<MseLossBackward>)\n",
      "493 tensor(1.1985e-07, grad_fn=<MseLossBackward>)\n",
      "494 tensor(1.1265e-07, grad_fn=<MseLossBackward>)\n",
      "495 tensor(1.0551e-07, grad_fn=<MseLossBackward>)\n",
      "496 tensor(9.9193e-08, grad_fn=<MseLossBackward>)\n",
      "497 tensor(9.3313e-08, grad_fn=<MseLossBackward>)\n",
      "498 tensor(8.7532e-08, grad_fn=<MseLossBackward>)\n",
      "499 tensor(8.2271e-08, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机初始化训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        # define model architecture\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear2(self.linear1(x).clamp(min=0))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for iter in range(500):\n",
    "    y_pred = model(x)  # 相当于forward pass\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(iter, loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf9f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
